{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "sys.path.append(\".\")\n",
    "# os.environ[\"MAX_JOBS\"] = \"100\"\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCLUDE: ['/home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include', '/home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include/torch/csrc/api/include', '/home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include/TH', '/home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include/THC', '/home/max/miniconda3/envs/xlstmpt21cu121/include']\n",
      "/home/max/myrepos/vlstm_cuda/src\n",
      "/usr/local/cuda-12.3/lib64:\n",
      "/home/max/miniconda3/envs/xlstmpt21cu121/lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/max/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/max/.cache/torch_extensions/py311_cu121/vlstm_v1/build.ninja...\n",
      "Building extension module vlstm_v1...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] /home/max/miniconda3/envs/xlstmpt21cu121/bin/x86_64-conda-linux-gnu-c++ -MMD -MF interface.o.d -DTORCH_EXTENSION_NAME=vlstm_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include -isystem /home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include/TH -isystem /home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include/THC -isystem /home/max/miniconda3/envs/xlstmpt21cu121/include -isystem /home/max/miniconda3/envs/xlstmpt21cu121/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/max/myrepos/vlstm_cuda/src/vlstm_v1/interface.cc -o interface.o \n",
      "[2/3] /home/max/miniconda3/envs/xlstmpt21cu121/bin/nvcc  -ccbin /home/max/miniconda3/envs/xlstmpt21cu121/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=vlstm_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include -isystem /home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include/TH -isystem /home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include/THC -isystem /home/max/miniconda3/envs/xlstmpt21cu121/include -isystem /home/max/miniconda3/envs/xlstmpt21cu121/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -gencode arch=compute_80,code=compute_80 -res-usage --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=13407807929942597099574024998205846127479365820592393377723561443721764030073546976801874298166903427690031858186486050853753882811946569946433649006084096 -std=c++17 -c /home/max/myrepos/vlstm_cuda/src/vlstm_v1/kernels.cu -o kernels.cuda.o \n",
      "\u001b[31mFAILED: \u001b[0mkernels.cuda.o \n",
      "/home/max/miniconda3/envs/xlstmpt21cu121/bin/nvcc  -ccbin /home/max/miniconda3/envs/xlstmpt21cu121/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=vlstm_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include -isystem /home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include/TH -isystem /home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include/THC -isystem /home/max/miniconda3/envs/xlstmpt21cu121/include -isystem /home/max/miniconda3/envs/xlstmpt21cu121/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -gencode arch=compute_80,code=compute_80 -res-usage --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=13407807929942597099574024998205846127479365820592393377723561443721764030073546976801874298166903427690031858186486050853753882811946569946433649006084096 -std=c++17 -c /home/max/myrepos/vlstm_cuda/src/vlstm_v1/kernels.cu -o kernels.cuda.o \n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops.cuh(236): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops.cuh:236 CUDART_VERSION: 12010, arch: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops.cuh\" \":\" \"236\" \" CUDART_VERSION: \" \"12010\" \", arch: \" \"890\")\n",
      "                                                                                                                                            ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops.cuh(241): remark #20200-D: #pragma message: \"INCLUDING FP16\"\n",
      "  #pragma message(\"INCLUDING FP16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                               ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_2fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_2fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_2fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                                ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops.cuh(253): remark #20200-D: #pragma message: \"INCLUDING BF16\"\n",
      "  #pragma message(\"INCLUDING BF16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_bf16.cuh:29 CUDART_VERSION with BF16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with BF16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                               ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_2bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_2bf16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_2bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                                ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/kernels.cu(95): warning #177-D: variable \"tIdx\" was declared but never referenced\n",
      "    int tIdx = threadIdx.x + blockDim.x * threadIdx.y;\n",
      "        ^\n",
      "          detected during instantiation of \"void vlstm::kernel_dispatchers::mmkernelv1_dispatch(scalar_t *, scalar_t *, scalar_t *, int, int, int) [with scalar_t=__nv_bfloat16]\" at line 205\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/kernels.cu(228): warning #177-D: variable \"tIdx\" was declared but never referenced\n",
      "    int tIdx = threadIdx.x + blockDim.x * threadIdx.y;\n",
      "        ^\n",
      "          detected during instantiation of \"void vlstm::kernel_dispatchers::mmkernelv2_dispatch(scalar_t *, scalar_t *, scalar_t *, int, int, int) [with scalar_t=__nv_bfloat16]\" at line 337\n",
      "\n",
      "ptxas info    : 32 bytes gmem\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels10mmkernelv2I6__halfLi4EEEvPT_S4_S4_iii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels10mmkernelv2I6__halfLi4EEEvPT_S4_S4_iii\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 64 bytes smem, 388 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels10mmkernelv2I13__nv_bfloat16Li4EEEvPT_S4_S4_iii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels10mmkernelv2I13__nv_bfloat16Li4EEEvPT_S4_S4_iii\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 64 bytes smem, 388 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels10mmkernelv1I6__halfLi4EEEvPT_S4_S4_iii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels10mmkernelv1I6__halfLi4EEEvPT_S4_S4_iii\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 64 bytes smem, 388 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels10mmkernelv1I13__nv_bfloat16Li4EEEvPT_S4_S4_iii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels10mmkernelv1I13__nv_bfloat16Li4EEEvPT_S4_S4_iii\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 64 bytes smem, 388 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels10copykernelI6__halfEEvPT_S4_ii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels10copykernelI6__halfEEvPT_S4_ii\n",
      "    16 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 24 registers, 376 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels10copykernelI13__nv_bfloat16EEvPT_S4_ii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels10copykernelI13__nv_bfloat16EEvPT_S4_ii\n",
      "    16 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 24 registers, 376 bytes cmem[0]\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops.cuh(236): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops.cuh:236 CUDART_VERSION: 12010, arch: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops.cuh\" \":\" \"236\" \" CUDART_VERSION: \" \"12010\" \", arch: \" \"800\")\n",
      "                                                                                                                                            ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops.cuh(241): remark #20200-D: #pragma message: \"INCLUDING FP16\"\n",
      "  #pragma message(\"INCLUDING FP16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                               ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_2fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_2fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_2fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                                ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops.cuh(253): remark #20200-D: #pragma message: \"INCLUDING BF16\"\n",
      "  #pragma message(\"INCLUDING BF16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_bf16.cuh:29 CUDART_VERSION with BF16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with BF16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                               ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_2bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_2bf16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops_2bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                                ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/kernels.cu(95): warning #177-D: variable \"tIdx\" was declared but never referenced\n",
      "    int tIdx = threadIdx.x + blockDim.x * threadIdx.y;\n",
      "        ^\n",
      "          detected during instantiation of \"void vlstm::kernel_dispatchers::mmkernelv1_dispatch(scalar_t *, scalar_t *, scalar_t *, int, int, int) [with scalar_t=__nv_bfloat16]\" at line 205\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/kernels.cu(228): warning #177-D: variable \"tIdx\" was declared but never referenced\n",
      "    int tIdx = threadIdx.x + blockDim.x * threadIdx.y;\n",
      "        ^\n",
      "          detected during instantiation of \"void vlstm::kernel_dispatchers::mmkernelv2_dispatch(scalar_t *, scalar_t *, scalar_t *, int, int, int) [with scalar_t=__nv_bfloat16]\" at line 337\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops.cuh:236:133: note: '#pragma message: /home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops.cuh:236 CUDART_VERSION: 12010, arch: __CUDA_ARCH__'\n",
      "  236 | #pragma message(AT \" CUDART_VERSION: \" TOSTRING(CUDART_VERSION) \", arch: \"     \\\n",
      "      |                                                                                                                                     ^\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops.cuh:248:90: note: '#pragma message: SKIPPING FP16, because of CUDART_VERSION: 12010, arch: __CUDA_ARCH__'\n",
      "  248 | #pragma message(\"SKIPPING FP16, because of CUDART_VERSION: \" TOSTRING(CUDART_VERSION) \", arch: \" TOSTRING(__CUDA_ARCH__))\n",
      "      |                                                                                          ^\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_v1/../util/inline_ops.cuh:260:90: note: '#pragma message: SKIPPING BF16, because of CUDART_VERSION: 12010, arch: __CUDA_ARCH__'\n",
      "  260 | #pragma message(\"SKIPPING BF16, because of CUDART_VERSION: \" TOSTRING(CUDART_VERSION) \", arch: \" TOSTRING(__CUDA_ARCH__))\n",
      "      |                                                                                          ^\n",
      "/home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include/pybind11/cast.h: In function 'typename pybind11::detail::type_caster<typename pybind11::detail::intrinsic_type<T>::type>::cast_op_type<T> pybind11::detail::cast_op(make_caster<T>&)':\n",
      "/home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include/pybind11/cast.h:45:120: error: expected template-name before '<' token\n",
      "   45 |     return caster.operator typename make_caster<T>::template cast_op_type<T>();\n",
      "      |                                                                                                                        ^\n",
      "/home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include/pybind11/cast.h:45:120: error: expected identifier before '<' token\n",
      "/home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include/pybind11/cast.h:45:123: error: expected primary-expression before '>' token\n",
      "   45 |     return caster.operator typename make_caster<T>::template cast_op_type<T>();\n",
      "      |                                                                                                                           ^\n",
      "/home/max/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/include/pybind11/cast.h:45:126: error: expected primary-expression before ')' token\n",
      "   45 |     return caster.operator typename make_caster<T>::template cast_op_type<T>();\n",
      "      |                                                                                                                              ^\n",
      "ninja: build stopped: subcommand failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error building extension 'vlstm_v1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2100\u001b[0m, in \u001b[0;36m_run_ninja_build\u001b[0;34m(build_directory, verbose, error_prefix)\u001b[0m\n\u001b[1;32m   2099\u001b[0m     stdout_fileno \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 2100\u001b[0m     subprocess\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m   2101\u001b[0m         command,\n\u001b[1;32m   2102\u001b[0m         stdout\u001b[39m=\u001b[39;49mstdout_fileno \u001b[39mif\u001b[39;49;00m verbose \u001b[39melse\u001b[39;49;00m subprocess\u001b[39m.\u001b[39;49mPIPE,\n\u001b[1;32m   2103\u001b[0m         stderr\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mSTDOUT,\n\u001b[1;32m   2104\u001b[0m         cwd\u001b[39m=\u001b[39;49mbuild_directory,\n\u001b[1;32m   2105\u001b[0m         check\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2106\u001b[0m         env\u001b[39m=\u001b[39;49menv)\n\u001b[1;32m   2107\u001b[0m \u001b[39mexcept\u001b[39;00m subprocess\u001b[39m.\u001b[39mCalledProcessError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   2108\u001b[0m     \u001b[39m# Python 2 and 3 compatible way of getting the error object.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/xlstmpt21cu121/lib/python3.11/subprocess.py:571\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[39mif\u001b[39;00m check \u001b[39mand\u001b[39;00m retcode:\n\u001b[0;32m--> 571\u001b[0m         \u001b[39mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[39m.\u001b[39margs,\n\u001b[1;32m    572\u001b[0m                                  output\u001b[39m=\u001b[39mstdout, stderr\u001b[39m=\u001b[39mstderr)\n\u001b[1;32m    573\u001b[0m \u001b[39mreturn\u001b[39;00m CompletedProcess(process\u001b[39m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['ninja', '-v']' returned non-zero exit status 1.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/max/myrepos/vlstm_cuda/vlstm_v1.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/max/myrepos/vlstm_cuda/vlstm_v1.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvlstm_v1\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterface\u001b[39;00m \u001b[39mimport\u001b[39;00m testkernel, copykernel, mmkernelv1, mmkernelv2\n",
      "File \u001b[0;32m~/myrepos/vlstm_cuda/src/vlstm_v1/interface.py:21\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodule \u001b[39m=\u001b[39m load(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvlstm_v1\u001b[39m\u001b[39m\"\u001b[39m, sources\u001b[39m=\u001b[39m[\u001b[39mstr\u001b[39m(filedir \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minterface.cc\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39mstr\u001b[39m(filedir \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mkernels.cu\u001b[39m\u001b[39m\"\u001b[39m)])\n\u001b[1;32m     18\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodule\n\u001b[0;32m---> 21\u001b[0m cppmodule \u001b[39m=\u001b[39m CppModule\u001b[39m.\u001b[39;49minstance()\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtestkernel\u001b[39m(mat_A: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     25\u001b[0m     out \u001b[39m=\u001b[39m cppmodule\u001b[39m.\u001b[39mtestkernel(mat_A)\n",
      "File \u001b[0;32m~/myrepos/vlstm_cuda/src/vlstm_v1/interface.py:17\u001b[0m, in \u001b[0;36mCppModule.instance\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minstance\u001b[39m(\u001b[39mcls\u001b[39m):\n\u001b[1;32m     16\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodule \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodule \u001b[39m=\u001b[39m load(name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mvlstm_v1\u001b[39;49m\u001b[39m\"\u001b[39;49m, sources\u001b[39m=\u001b[39;49m[\u001b[39mstr\u001b[39;49m(filedir \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39minterface.cc\u001b[39;49m\u001b[39m\"\u001b[39;49m), \u001b[39mstr\u001b[39;49m(filedir \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mkernels.cu\u001b[39;49m\u001b[39m\"\u001b[39;49m)])\n\u001b[1;32m     18\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodule\n",
      "File \u001b[0;32m~/myrepos/vlstm_cuda/src/cuda_init.py:53\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, sources, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m myargs \u001b[39m=\u001b[39m {\n\u001b[1;32m     36\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mverbose\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwith_cuda\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     ],\n\u001b[1;32m     51\u001b[0m }\n\u001b[1;32m     52\u001b[0m myargs\u001b[39m.\u001b[39mupdate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 53\u001b[0m \u001b[39mreturn\u001b[39;00m _load(name, sources, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmyargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1308\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(name,\n\u001b[1;32m   1217\u001b[0m          sources: Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m]],\n\u001b[1;32m   1218\u001b[0m          extra_cflags\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m          is_standalone\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1227\u001b[0m          keep_intermediates\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1228\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m \u001b[39m    Loads a PyTorch C++ extension just-in-time (JIT).\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[39m        ...     verbose=True)\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m-> 1308\u001b[0m     \u001b[39mreturn\u001b[39;00m _jit_compile(\n\u001b[1;32m   1309\u001b[0m         name,\n\u001b[1;32m   1310\u001b[0m         [sources] \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(sources, \u001b[39mstr\u001b[39;49m) \u001b[39melse\u001b[39;49;00m sources,\n\u001b[1;32m   1311\u001b[0m         extra_cflags,\n\u001b[1;32m   1312\u001b[0m         extra_cuda_cflags,\n\u001b[1;32m   1313\u001b[0m         extra_ldflags,\n\u001b[1;32m   1314\u001b[0m         extra_include_paths,\n\u001b[1;32m   1315\u001b[0m         build_directory \u001b[39mor\u001b[39;49;00m _get_build_directory(name, verbose),\n\u001b[1;32m   1316\u001b[0m         verbose,\n\u001b[1;32m   1317\u001b[0m         with_cuda,\n\u001b[1;32m   1318\u001b[0m         is_python_module,\n\u001b[1;32m   1319\u001b[0m         is_standalone,\n\u001b[1;32m   1320\u001b[0m         keep_intermediates\u001b[39m=\u001b[39;49mkeep_intermediates)\n",
      "File \u001b[0;32m~/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1710\u001b[0m, in \u001b[0;36m_jit_compile\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1706\u001b[0m                 hipified_sources\u001b[39m.\u001b[39madd(hipify_result[s_abs]\u001b[39m.\u001b[39mhipified_path \u001b[39mif\u001b[39;00m s_abs \u001b[39min\u001b[39;00m hipify_result \u001b[39melse\u001b[39;00m s_abs)\n\u001b[1;32m   1708\u001b[0m             sources \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(hipified_sources)\n\u001b[0;32m-> 1710\u001b[0m         _write_ninja_file_and_build_library(\n\u001b[1;32m   1711\u001b[0m             name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1712\u001b[0m             sources\u001b[39m=\u001b[39;49msources,\n\u001b[1;32m   1713\u001b[0m             extra_cflags\u001b[39m=\u001b[39;49mextra_cflags \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m   1714\u001b[0m             extra_cuda_cflags\u001b[39m=\u001b[39;49mextra_cuda_cflags \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m   1715\u001b[0m             extra_ldflags\u001b[39m=\u001b[39;49mextra_ldflags \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m   1716\u001b[0m             extra_include_paths\u001b[39m=\u001b[39;49mextra_include_paths \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m   1717\u001b[0m             build_directory\u001b[39m=\u001b[39;49mbuild_directory,\n\u001b[1;32m   1718\u001b[0m             verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1719\u001b[0m             with_cuda\u001b[39m=\u001b[39;49mwith_cuda,\n\u001b[1;32m   1720\u001b[0m             is_standalone\u001b[39m=\u001b[39;49mis_standalone)\n\u001b[1;32m   1721\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1722\u001b[0m     baton\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1823\u001b[0m, in \u001b[0;36m_write_ninja_file_and_build_library\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_standalone)\u001b[0m\n\u001b[1;32m   1821\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m   1822\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBuilding extension module \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m'\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstderr)\n\u001b[0;32m-> 1823\u001b[0m _run_ninja_build(\n\u001b[1;32m   1824\u001b[0m     build_directory,\n\u001b[1;32m   1825\u001b[0m     verbose,\n\u001b[1;32m   1826\u001b[0m     error_prefix\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mError building extension \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/xlstmpt21cu121/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2116\u001b[0m, in \u001b[0;36m_run_ninja_build\u001b[0;34m(build_directory, verbose, error_prefix)\u001b[0m\n\u001b[1;32m   2114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(error, \u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m error\u001b[39m.\u001b[39moutput:  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   2115\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00merror\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mdecode(\u001b[39m*\u001b[39mSUBPROCESS_DECODE_ARGS)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m-> 2116\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(message) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error building extension 'vlstm_v1'"
     ]
    }
   ],
   "source": [
    "from src.vlstm_v1.interface import testkernel, copykernel, mmkernelv1, mmkernelv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = torch.bfloat16\n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_A = torch.arange(4).reshape(2, 2).to(dtype=DTYPE, device=DEVICE)\n",
    "mat_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test kernel!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = testkernel(mat_A)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - float16!\n",
      "rows: 2, cols: 2\n",
      "blocksxy: 1-1, threads: 32-32\n",
      "cidx: 0, ridx: 0, val: 0.000000\n",
      "cidx: 1, ridx: 0, val: 2.000000\n",
      "cidx: 0, ridx: 1, val: 1.000000\n",
      "cidx: 1, ridx: 1, val: 3.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = copykernel(mat_A.to(dtype=torch.float16))\n",
    "out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[64., 64., 64.,  ..., 64., 64., 64.],\n",
       "         [64., 64., 64.,  ..., 64., 64., 64.],\n",
       "         [64., 64., 64.,  ..., 64., 64., 64.],\n",
       "         ...,\n",
       "         [64., 64., 64.,  ..., 64., 64., 64.],\n",
       "         [64., 64., 64.,  ..., 64., 64., 64.],\n",
       "         [64., 64., 64.,  ..., 64., 64., 64.]], device='cuda:0',\n",
       "        dtype=torch.bfloat16),\n",
       " torch.Size([32, 32]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warp_size = 32\n",
    "am = torch.ones((1*1*warp_size, 1*1*warp_size), device=DEVICE, dtype=DTYPE)\n",
    "bm = 2 * torch.ones((1*1*warp_size, 1*1*warp_size), device=DEVICE, dtype=DTYPE)\n",
    "cm = am @ bm\n",
    "cm, cm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - bfloat16!\n",
      "m: 32, n: 32, k: 32\n",
      "blocksxy: 8-8, threads: 4-4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[64., 64., 64.,  ..., 64., 64., 64.],\n",
       "        [64., 64., 64.,  ..., 64., 64., 64.],\n",
       "        [64., 64., 64.,  ..., 64., 64., 64.],\n",
       "        ...,\n",
       "        [64., 64., 64.,  ..., 64., 64., 64.],\n",
       "        [64., 64., 64.,  ..., 64., 64., 64.],\n",
       "        [64., 64., 64.,  ..., 64., 64., 64.]], device='cuda:0',\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = torch.bfloat16\n",
    "am = am.to(dtype=dt)\n",
    "bm = bm.to(dtype=dt)\n",
    "mmkernelv1(mat_A=am, mat_B=bm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 8 # sequence length\n",
    "DH = 3 # hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 3]), torch.Size([3, 8]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matA = torch.ones((S, DH), device=DEVICE, dtype=DTYPE)\n",
    "matB = torch.ones((DH, S), device=DEVICE, dtype=DTYPE)\n",
    "matA.shape, matB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3., 3., 3., 3.]], device='cuda:0',\n",
       "        dtype=torch.bfloat16),\n",
       " torch.Size([8, 8]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch\n",
    "pt_out = matA @ matB\n",
    "pt_out, pt_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matA.is_contiguous(), matB.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - bfloat16!\n",
      "m: 8, n: 8, k: 3\n",
      "blocksxy: 2-2, threads: 4-4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3., 67.,  3., 67.,  3., 67.,  3., 67.],\n",
       "         [ 3., 67.,  3., 67.,  3., 67.,  3., 67.],\n",
       "         [ 3., 67.,  3., 67.,  3., 67.,  3., 67.],\n",
       "         [ 3., 67.,  3., 67.,  3., 67.,  3., 67.],\n",
       "         [ 3., 67.,  3., 67.,  3., 67.,  3., 67.],\n",
       "         [ 3., 67.,  3., 67.,  3., 67.,  3., 67.],\n",
       "         [ 3., 67.,  3., 67.,  3., 67.,  3., 67.],\n",
       "         [ 3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.]], device='cuda:0',\n",
       "        dtype=torch.bfloat16),\n",
       " torch.Size([8, 8]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cu_out = mmkernelv1(mat_A=matA, mat_B=matB)\n",
    "cu_out, cu_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - bfloat16!\n",
      "m: 8, n: 8, k: 3\n",
      "blocksxy: 2-2, threads: 4-4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3., 67.,  3., 67.,  3., 67.,  3., 67.],\n",
       "         [ 3., 67.,  3., 67.,  3., 67.,  3., 67.],\n",
       "         [ 3., 67.,  3., 67.,  3., 67.,  3., 67.],\n",
       "         [ 3., 67.,  3., 67.,  3., 67.,  3., 67.],\n",
       "         [ 3., 67.,  3., 67.,  3., 67.,  3., 67.],\n",
       "         [ 3., 67.,  3., 67.,  3., 67.,  3., 67.],\n",
       "         [ 3., 67.,  3., 67.,  3., 67.,  3., 67.],\n",
       "         [ 3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.]], device='cuda:0',\n",
       "        dtype=torch.bfloat16),\n",
       " torch.Size([8, 8]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cu_out = mmkernelv2(mat_A=matA, mat_B=matB)\n",
    "cu_out, cu_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat @ mat.T @ mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 6 # hidden size\n",
    "S = 5 # seq len\n",
    "B = 1 # batch size\n",
    "NH = 2 # num heads\n",
    "DH = H // NH # dim per head\n",
    "DTYPE = torch.bfloat16\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "assert H % NH == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create qkv, inputgates, forgetgates \n",
    "torch.manual_seed(0)\n",
    "qs = torch.randn((B, NH, S, DH), device=DEVICE, dtype=DTYPE)\n",
    "ks = torch.randn((B, NH, S, DH), device=DEVICE, dtype=DTYPE)\n",
    "vs = torch.randn((B, NH, S, DH), device=DEVICE, dtype=DTYPE)\n",
    "ds = torch.rand((B, NH, S, S), device=DEVICE, dtype=DTYPE)\n",
    "\n",
    "max_log_D, _ = torch.max(ds.view(B, NH, -1), dim=-1, keepdim=True)  # (B, NH, 1)\n",
    "log_D_matrix_stabilized = ds - max_log_D.unsqueeze(-1)  # (B, NH, S, S) = (B, NH, S, S) - (B, NH, 1, 1)\n",
    "D_matrix = torch.exp(log_D_matrix_stabilized)  # (B, NH, S, S)\n",
    "mval = torch.exp(-max_log_D.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstmpt21cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
