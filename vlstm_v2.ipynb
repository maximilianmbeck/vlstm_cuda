{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "sys.path.append(\".\")\n",
    "# os.environ[\"MAX_JOBS\"] = \"100\"\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCLUDE: ['/scratch/maximilian.beck/.conda/envs/xlstmdev1/lib/python3.11/site-packages/torch/include', '/scratch/maximilian.beck/.conda/envs/xlstmdev1/lib/python3.11/site-packages/torch/include/torch/csrc/api/include', '/scratch/maximilian.beck/.conda/envs/xlstmdev1/lib/python3.11/site-packages/torch/include/TH', '/scratch/maximilian.beck/.conda/envs/xlstmdev1/lib/python3.11/site-packages/torch/include/THC', '/scratch/maximilian.beck/.conda/envs/xlstmdev1/include']\n",
      "/iarai/home/maximilian.beck/repos/vlstm_cuda/src\n",
      "\n",
      "/scratch/maximilian.beck/.conda/envs/xlstmdev1/lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /scratch/maximilian.beck/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /scratch/maximilian.beck/.cache/torch_extensions/py311_cu118/vlstm_v2/build.ninja...\n",
      "Building extension module vlstm_v2...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] /scratch/maximilian.beck/.conda/envs/xlstmdev1/bin/nvcc  -ccbin /scratch/maximilian.beck/.conda/envs/xlstmdev1/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=vlstm_v2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /scratch/maximilian.beck/.conda/envs/xlstmdev1/lib/python3.11/site-packages/torch/include -isystem /scratch/maximilian.beck/.conda/envs/xlstmdev1/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /scratch/maximilian.beck/.conda/envs/xlstmdev1/lib/python3.11/site-packages/torch/include/TH -isystem /scratch/maximilian.beck/.conda/envs/xlstmdev1/lib/python3.11/site-packages/torch/include/THC -isystem /scratch/maximilian.beck/.conda/envs/xlstmdev1/include -isystem /scratch/maximilian.beck/.conda/envs/xlstmdev1/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -gencode arch=compute_80,code=compute_80 -res-usage --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=13407807929942597099574024998205846127479365820592393377723561443721764030073546976801874298166903427690031858186486050853753882811946569946433649006084096 -std=c++17 -c /iarai/home/maximilian.beck/repos/vlstm_cuda/src/vlstm_v2/kernels.cu -o kernels.cuda.o \n",
      "/iarai/home/maximilian.beck/repos/vlstm_cuda/src/vlstm_v2/../util/inline_ops.cuh(236): remark #20200-D: #pragma message: \"/iarai/home/maximilian.beck/repos/vlstm_cuda/src/vlstm_v2/../util/inline_ops.cuh:236 CUDART_VERSION: 11080, arch: 800\"\n",
      "\n",
      "/iarai/home/maximilian.beck/repos/vlstm_cuda/src/vlstm_v2/../util/inline_ops.cuh(241): remark #20200-D: #pragma message: \"INCLUDING FP16\"\n",
      "\n",
      "/iarai/home/maximilian.beck/repos/vlstm_cuda/src/vlstm_v2/../util/inline_ops_fp16.cuh(29): remark #20200-D: #pragma message: \"/iarai/home/maximilian.beck/repos/vlstm_cuda/src/vlstm_v2/../util/inline_ops_fp16.cuh:29 CUDART_VERSION with FP16: 11080, CUDA_ARCH: 800\"\n",
      "\n",
      "/iarai/home/maximilian.beck/repos/vlstm_cuda/src/vlstm_v2/../util/inline_ops_2fp16.cuh(29): remark #20200-D: #pragma message: \"/iarai/home/maximilian.beck/repos/vlstm_cuda/src/vlstm_v2/../util/inline_ops_2fp16.cuh:29 CUDART_VERSION with FP16: 11080, CUDA_ARCH: 800\"\n",
      "\n",
      "/iarai/home/maximilian.beck/repos/vlstm_cuda/src/vlstm_v2/../util/inline_ops.cuh(253): remark #20200-D: #pragma message: \"INCLUDING BF16\"\n",
      "\n",
      "/iarai/home/maximilian.beck/repos/vlstm_cuda/src/vlstm_v2/../util/inline_ops_bf16.cuh(29): remark #20200-D: #pragma message: \"/iarai/home/maximilian.beck/repos/vlstm_cuda/src/vlstm_v2/../util/inline_ops_bf16.cuh:29 CUDART_VERSION with BF16: 11080, CUDA_ARCH: 800\"\n",
      "\n",
      "/iarai/home/maximilian.beck/repos/vlstm_cuda/src/vlstm_v2/../util/inline_ops_2bf16.cuh(29): remark #20200-D: #pragma message: \"/iarai/home/maximilian.beck/repos/vlstm_cuda/src/vlstm_v2/../util/inline_ops_2bf16.cuh:29 CUDART_VERSION with FP16: 11080, CUDA_ARCH: 800\"\n",
      "\n",
      "ptxas info    : 123 bytes gmem\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels10mmkernelv1I6__halfLi8EEEvPT_S4_S4_iii' for 'sm_80'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels10mmkernelv1I6__halfLi8EEEvPT_S4_S4_iii\n",
      "    16 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 32 registers, 256 bytes smem, 388 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels10mmkernelv1I13__nv_bfloat16Li8EEEvPT_S4_S4_iii' for 'sm_80'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels10mmkernelv1I13__nv_bfloat16Li8EEEvPT_S4_S4_iii\n",
      "    16 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 32 registers, 256 bytes smem, 388 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels10copykernelI6__halfEEvPT_S4_ii' for 'sm_80'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels10copykernelI6__halfEEvPT_S4_ii\n",
      "    16 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 24 registers, 376 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels10copykernelI13__nv_bfloat16EEvPT_S4_ii' for 'sm_80'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels10copykernelI13__nv_bfloat16EEvPT_S4_ii\n",
      "    16 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 24 registers, 376 bytes cmem[0]\n",
      "/iarai/home/maximilian.beck/repos/vlstm_cuda/src/vlstm_v2/../util/inline_ops.cuh:236:149: note: '#pragma message: /iarai/home/maximilian.beck/repos/vlstm_cuda/src/vlstm_v2/../util/inline_ops.cuh:236 CUDART_VERSION: 11080, arch: __CUDA_ARCH__'\n",
      "  236 | #pragma message(AT \" CUDART_VERSION: \" TOSTRING(CUDART_VERSION) \", arch: \"     \\\n",
      "      |                                                                                                                                                     ^\n",
      "/iarai/home/maximilian.beck/repos/vlstm_cuda/src/vlstm_v2/../util/inline_ops.cuh:248:90: note: '#pragma message: SKIPPING FP16, because of CUDART_VERSION: 11080, arch: __CUDA_ARCH__'\n",
      "  248 | #pragma message(\"SKIPPING FP16, because of CUDART_VERSION: \" TOSTRING(CUDART_VERSION) \", arch: \" TOSTRING(__CUDA_ARCH__))\n",
      "      |                                                                                          ^\n",
      "/iarai/home/maximilian.beck/repos/vlstm_cuda/src/vlstm_v2/../util/inline_ops.cuh:260:90: note: '#pragma message: SKIPPING BF16, because of CUDART_VERSION: 11080, arch: __CUDA_ARCH__'\n",
      "  260 | #pragma message(\"SKIPPING BF16, because of CUDART_VERSION: \" TOSTRING(CUDART_VERSION) \", arch: \" TOSTRING(__CUDA_ARCH__))\n",
      "      |                                                                                          ^\n",
      "[2/2] /scratch/maximilian.beck/.conda/envs/xlstmdev1/bin/x86_64-conda-linux-gnu-c++ interface.o kernels.cuda.o -shared -L/scratch/maximilian.beck/.conda/envs/xlstmdev1/lib -lcublas -L/scratch/maximilian.beck/.conda/envs/xlstmdev1/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/scratch/maximilian.beck/.conda/envs/xlstmdev1/lib -lcudart -o vlstm_v2.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module vlstm_v2...\n"
     ]
    }
   ],
   "source": [
    "from src.vlstm_v2.interface import testkernel, copykernel, mmkernelv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "DTYPE = torch.bfloat16\n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_A = torch.arange(4).reshape(2, 2).to(dtype=DTYPE, device=DEVICE)\n",
    "mat_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test kernel!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = testkernel(mat_A)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - float16!\n",
      "rows: 2, cols: 2\n",
      "blocksxy: 1-1, threads: 32-32\n",
      "cidx: 0, ridx: 0, val: 0.000000\n",
      "cidx: 1, ridx: 0, val: 2.000000\n",
      "cidx: 0, ridx: 1, val: 1.000000\n",
      "cidx: 1, ridx: 1, val: 3.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = copykernel(mat_A.to(dtype=torch.float16))\n",
    "out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[128., 128., 128.,  ..., 128., 128., 128.],\n",
       "         [128., 128., 128.,  ..., 128., 128., 128.],\n",
       "         [128., 128., 128.,  ..., 128., 128., 128.],\n",
       "         ...,\n",
       "         [128., 128., 128.,  ..., 128., 128., 128.],\n",
       "         [128., 128., 128.,  ..., 128., 128., 128.],\n",
       "         [128., 128., 128.,  ..., 128., 128., 128.]], device='cuda:0',\n",
       "        dtype=torch.bfloat16),\n",
       " torch.Size([32, 32]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warp_size = 32\n",
    "am = torch.ones((1*1*warp_size, 2*1*warp_size), device=DEVICE, dtype=DTYPE)\n",
    "bm = 2 * torch.ones((2*1*warp_size, 1*1*warp_size), device=DEVICE, dtype=DTYPE)\n",
    "cm = am @ bm\n",
    "cm, cm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - bfloat16!\n",
      "m: 32, n: 32, k: 64\n",
      "blocksxy: 4-4, threads: 8-8\n",
      "In Kernel: m: 32, n: 32, k: 64\n",
      "In Kernel: gdim.x: 4, gdim.y: 4, bdim.x: 8, bdim.y: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[128., 128., 128.,  ..., 128., 128., 128.],\n",
       "        [128., 128., 128.,  ..., 128., 128., 128.],\n",
       "        [128., 128., 128.,  ..., 128., 128., 128.],\n",
       "        ...,\n",
       "        [128., 128., 128.,  ..., 128., 128., 128.],\n",
       "        [128., 128., 128.,  ..., 128., 128., 128.],\n",
       "        [128., 128., 128.,  ..., 128., 128., 128.]], device='cuda:0',\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = torch.bfloat16\n",
    "am = am.to(dtype=dt)\n",
    "bm = bm.to(dtype=dt)\n",
    "mmkernelv1(mat_A=am, mat_B=bm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "S = 8 # sequence length\n",
    "DH = 8 # hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 8]), torch.Size([8, 8]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matA = torch.arange((2*S * DH), device=DEVICE, dtype=DTYPE).reshape((2*S, DH))\n",
    "matB = torch.ones((DH, S), device=DEVICE, dtype=DTYPE)\n",
    "matA.shape, matB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 28.,  28.,  28.,  28.,  28.,  28.,  28.,  28.],\n",
       "         [ 92.,  92.,  92.,  92.,  92.,  92.,  92.,  92.],\n",
       "         [156., 156., 156., 156., 156., 156., 156., 156.],\n",
       "         [220., 220., 220., 220., 220., 220., 220., 220.],\n",
       "         [284., 284., 284., 284., 284., 284., 284., 284.],\n",
       "         [348., 348., 348., 348., 348., 348., 348., 348.],\n",
       "         [412., 412., 412., 412., 412., 412., 412., 412.],\n",
       "         [476., 476., 476., 476., 476., 476., 476., 476.],\n",
       "         [540., 540., 540., 540., 540., 540., 540., 540.],\n",
       "         [604., 604., 604., 604., 604., 604., 604., 604.],\n",
       "         [668., 668., 668., 668., 668., 668., 668., 668.],\n",
       "         [732., 732., 732., 732., 732., 732., 732., 732.],\n",
       "         [796., 796., 796., 796., 796., 796., 796., 796.],\n",
       "         [860., 860., 860., 860., 860., 860., 860., 860.],\n",
       "         [924., 924., 924., 924., 924., 924., 924., 924.],\n",
       "         [988., 988., 988., 988., 988., 988., 988., 988.]], device='cuda:0',\n",
       "        dtype=torch.bfloat16),\n",
       " torch.Size([16, 8]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch\n",
    "pt_out = matA @ matB\n",
    "pt_out, pt_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matA.is_contiguous(), matB.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - bfloat16!\n",
      "m: 16, n: 8, k: 8\n",
      "blocksxy: 1-2, threads: 8-8\n",
      "In Kernel: m: 16, n: 8, k: 8\n",
      "In Kernel: gdim.x: 1, gdim.y: 2, bdim.x: 8, bdim.y: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 28.,  28.,  28.,  28.,  28.,  28.,  28.,  28.],\n",
       "         [ 92.,  92.,  92.,  92.,  92.,  92.,  92.,  92.],\n",
       "         [156., 156., 156., 156., 156., 156., 156., 156.],\n",
       "         [220., 220., 220., 220., 220., 220., 220., 220.],\n",
       "         [284., 284., 284., 284., 284., 284., 284., 284.],\n",
       "         [348., 348., 348., 348., 348., 348., 348., 348.],\n",
       "         [412., 412., 412., 412., 412., 412., 412., 412.],\n",
       "         [476., 476., 476., 476., 476., 476., 476., 476.],\n",
       "         [540., 540., 540., 540., 540., 540., 540., 540.],\n",
       "         [608., 608., 608., 608., 608., 608., 608., 608.],\n",
       "         [672., 672., 672., 672., 672., 672., 672., 672.],\n",
       "         [736., 736., 736., 736., 736., 736., 736., 736.],\n",
       "         [800., 800., 800., 800., 800., 800., 800., 800.],\n",
       "         [864., 864., 864., 864., 864., 864., 864., 864.],\n",
       "         [928., 928., 928., 928., 928., 928., 928., 928.],\n",
       "         [992., 992., 992., 992., 992., 992., 992., 992.]], device='cuda:0',\n",
       "        dtype=torch.bfloat16),\n",
       " torch.Size([16, 8]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cu_out = mmkernelv1(mat_A=matA, mat_B=matB)\n",
    "cu_out, cu_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# cu_out = mmkernelv2(mat_A=matA, mat_B=matB)\n",
    "# cu_out, cu_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat @ mat.T @ mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 6 # hidden size\n",
    "S = 5 # seq len\n",
    "B = 1 # batch size\n",
    "NH = 2 # num heads\n",
    "DH = H // NH # dim per head\n",
    "DTYPE = torch.bfloat16\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "assert H % NH == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create qkv, inputgates, forgetgates \n",
    "torch.manual_seed(0)\n",
    "qs = torch.randn((B, NH, S, DH), device=DEVICE, dtype=DTYPE)\n",
    "ks = torch.randn((B, NH, S, DH), device=DEVICE, dtype=DTYPE)\n",
    "vs = torch.randn((B, NH, S, DH), device=DEVICE, dtype=DTYPE)\n",
    "ds = torch.rand((B, NH, S, S), device=DEVICE, dtype=DTYPE)\n",
    "\n",
    "max_log_D, _ = torch.max(ds.view(B, NH, -1), dim=-1, keepdim=True)  # (B, NH, 1)\n",
    "log_D_matrix_stabilized = ds - max_log_D.unsqueeze(-1)  # (B, NH, S, S) = (B, NH, S, S) - (B, NH, 1, 1)\n",
    "D_matrix = torch.exp(log_D_matrix_stabilized)  # (B, NH, S, S)\n",
    "mval = torch.exp(-max_log_D.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstmpt21cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
