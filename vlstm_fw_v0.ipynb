{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "sys.path.append(\".\")\n",
    "# os.environ[\"MAX_JOBS\"] = \"100\"\n",
    "\n",
    "import torch\n",
    "torch.set_printoptions(linewidth=200)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCLUDE: ['/home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include', '/home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include/torch/csrc/api/include', '/home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include/TH', '/home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include/THC', '/home/max/miniconda3/envs/xlstmpt21cu118/include']\n",
      "/home/max/myrepos/vlstm_cuda/src\n",
      "/home/max/cpplibs/libtorch/lib:/usr/local/cuda-12.3/lib64:\n",
      "/home/max/miniconda3/envs/xlstmpt21cu118/lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/max/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/max/.cache/torch_extensions/py311_cu118/vlstm_fw_v0/build.ninja...\n",
      "Building extension module vlstm_fw_v0...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module vlstm_fw_v0...\n"
     ]
    }
   ],
   "source": [
    "from src.vlstm_fw_v0.interface import vlstm_fw_torch\n",
    "from src.vlstm_fw_v0.interface import vlstm_fw_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA vLSTM forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "S = 16 # seq len\n",
    "B = 1 # batch size\n",
    "NH = 1 # num heads\n",
    "DH = 4 # dim per head\n",
    "DTYPE = torch.float32\n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0000, 0.1000, 0.2000, 0.3000],\n",
       "           [0.4000, 0.5000, 0.6000, 0.7000],\n",
       "           [0.8000, 0.9000, 1.0000, 1.1000],\n",
       "           [1.2000, 1.3000, 1.4000, 1.5000],\n",
       "           [1.6000, 1.7000, 1.8000, 1.9000],\n",
       "           [2.0000, 2.1000, 2.2000, 2.3000],\n",
       "           [2.4000, 2.5000, 2.6000, 2.7000],\n",
       "           [2.8000, 2.9000, 3.0000, 3.1000],\n",
       "           [3.2000, 3.3000, 3.4000, 3.5000],\n",
       "           [3.6000, 3.7000, 3.8000, 3.9000],\n",
       "           [4.0000, 4.1000, 4.2000, 4.3000],\n",
       "           [4.4000, 4.5000, 4.6000, 4.7000],\n",
       "           [4.8000, 4.9000, 5.0000, 5.1000],\n",
       "           [5.2000, 5.3000, 5.4000, 5.5000],\n",
       "           [5.6000, 5.7000, 5.8000, 5.9000],\n",
       "           [6.0000, 6.1000, 6.2000, 6.3000]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 16, 4]),\n",
       " 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create qkv, inputgates, forgetgates \n",
    "torch.manual_seed(0)\n",
    "qs = torch.arange((B*NH*S*DH), device=DEVICE, dtype=DTYPE).reshape((B, NH, S, DH)) / 10.\n",
    "ks = torch.ones((B, NH, S, DH), device=DEVICE, dtype=DTYPE) / 100.\n",
    "vs = torch.ones((B, NH, S, DH), device=DEVICE, dtype=DTYPE) / 100.\n",
    "# igs = (1. + torch.arange((B * NH * S), device=DEVICE, dtype=DTYPE)).reshape(B, NH, S, 1) / 100.\n",
    "igs = torch.ones((B, NH, S, 1), device=DEVICE, dtype=DTYPE) #/ 10.\n",
    "fgs = 0.01 * torch.ones((B, NH, S, 1), device=DEVICE, dtype=DTYPE)\n",
    "qs, qs.shape, len(qs.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[8.1548e-05, 8.1548e-05, 8.1548e-05, 8.1548e-05],\n",
       "           [4.4926e-04, 4.4926e-04, 4.4926e-04, 4.4926e-04],\n",
       "           [9.0641e-04, 9.0641e-04, 9.0641e-04, 9.0641e-04],\n",
       "           [1.3812e-03, 1.3812e-03, 1.3812e-03, 1.3812e-03],\n",
       "           [1.8511e-03, 1.8511e-03, 1.8511e-03, 1.8511e-03],\n",
       "           [2.3116e-03, 2.3116e-03, 2.3116e-03, 2.3116e-03],\n",
       "           [2.7640e-03, 2.7640e-03, 2.7640e-03, 2.7640e-03],\n",
       "           [3.2106e-03, 3.2106e-03, 3.2106e-03, 3.2106e-03],\n",
       "           [3.6533e-03, 3.6533e-03, 3.6533e-03, 3.6533e-03],\n",
       "           [4.0937e-03, 4.0937e-03, 4.0937e-03, 4.0937e-03],\n",
       "           [4.5327e-03, 4.5327e-03, 4.5327e-03, 4.5327e-03],\n",
       "           [4.9708e-03, 4.9708e-03, 4.9708e-03, 4.9708e-03],\n",
       "           [5.4085e-03, 5.4085e-03, 5.4085e-03, 5.4085e-03],\n",
       "           [5.8460e-03, 5.8460e-03, 5.8460e-03, 5.8460e-03],\n",
       "           [6.2833e-03, 6.2833e-03, 6.2833e-03, 6.2833e-03],\n",
       "           [6.7205e-03, 6.7205e-03, 6.7205e-03, 6.7205e-03]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 16, 4]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch version\n",
    "rs = vlstm_fw_torch(\n",
    "    queries=qs,\n",
    "    keys=ks,\n",
    "    values=vs,\n",
    "    igate_preact=igs,\n",
    "    fgate_preact=fgs,\n",
    ")\n",
    "rs, rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0010, 0.0010, 0.0010, 0.0010],\n",
       "           [0.0035, 0.0035, 0.0035, 0.0035],\n",
       "           [0.0061, 0.0061, 0.0061, 0.0061],\n",
       "           [0.0086, 0.0086, 0.0086, 0.0086],\n",
       "           [0.0112, 0.0112, 0.0112, 0.0112],\n",
       "           [0.0138, 0.0138, 0.0138, 0.0138],\n",
       "           [0.0163, 0.0163, 0.0163, 0.0163],\n",
       "           [0.0189, 0.0189, 0.0189, 0.0189],\n",
       "           [0.0214, 0.0214, 0.0214, 0.0214],\n",
       "           [0.0240, 0.0240, 0.0240, 0.0240],\n",
       "           [0.0266, 0.0266, 0.0266, 0.0266],\n",
       "           [0.0291, 0.0291, 0.0291, 0.0291],\n",
       "           [0.0317, 0.0317, 0.0317, 0.0317],\n",
       "           [0.0342, 0.0342, 0.0342, 0.0342],\n",
       "           [0.0368, 0.0368, 0.0368, 0.0368],\n",
       "           [0.0394, 0.0394, 0.0394, 0.0394]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 16, 4]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch version QKV product (To test if this is still correct after changes to the code.)\n",
    "# at some point we have to compare to the vlstm_fw_torch version.\n",
    "rs = qs @ ks.transpose(-1, -2) @ vs\n",
    "rs, rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - float32!\n",
      "B: 1, NH: 1, S: 16, DH: 4\n",
      "blocksxy: 1-2, threads: 4-4, shared_mem in bytes: 2560\n",
      "In Kernel: gdim.x: 1, gdim.y: 2, gdim.z: 1, bdim.x: 4, bdim.y: 4\n",
      "In Kernel: QtileDim: 8, KVtileDim: 8, TblockDim:4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0010, 0.0010, 0.0010, 0.0010],\n",
       "           [0.0035, 0.0035, 0.0035, 0.0035],\n",
       "           [0.0061, 0.0061, 0.0061, 0.0061],\n",
       "           [0.0086, 0.0086, 0.0086, 0.0086],\n",
       "           [0.0112, 0.0112, 0.0112, 0.0112],\n",
       "           [0.0138, 0.0138, 0.0138, 0.0138],\n",
       "           [0.0163, 0.0163, 0.0163, 0.0163],\n",
       "           [0.0189, 0.0189, 0.0189, 0.0189],\n",
       "           [0.0214, 0.0214, 0.0214, 0.0214],\n",
       "           [0.0240, 0.0240, 0.0240, 0.0240],\n",
       "           [0.0266, 0.0266, 0.0266, 0.0266],\n",
       "           [0.0291, 0.0291, 0.0291, 0.0291],\n",
       "           [0.0317, 0.0317, 0.0317, 0.0317],\n",
       "           [0.0342, 0.0342, 0.0342, 0.0342],\n",
       "           [0.0368, 0.0368, 0.0368, 0.0368],\n",
       "           [0.0394, 0.0394, 0.0394, 0.0394]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 16, 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cuda kernel\n",
    "rs = vlstm_fw_cuda(mat_Q=qs, mat_K=ks, mat_V=vs)\n",
    "rs, rs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch vLSTM forward - Normalization\n",
    "\n",
    "Shows that the result is identical, whether we use rowwise stabilization or not. \n",
    "For the Kernel it is easier to implement the rowwise normalization, therefore use the rowwise normalization as default. (It is also somewhat conceptually easier to convey. We treat each timestep independently.)\n",
    "\n",
    "In most cases due to the input (i think forget gate is meant here??) gate the maximum is the last input timestep (i.e. the element on the diagonal). Then the result of the stabilization is that there are almost always ones on the diagonal, and the other timesteps are decayed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vlstm_fw_v0.torch_impl import vlstm_fw_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "S = 8 # seq len\n",
    "B = 1 # batch size\n",
    "NH = 1 # num heads\n",
    "DH = 4 # dim per head\n",
    "DTYPE = torch.float32\n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 8, 4]), torch.Size([1, 1, 8, 1]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create qkv, inputgates, forgetgates \n",
    "torch.manual_seed(0)\n",
    "qs = torch.randn((B, NH, S, DH), device=DEVICE, dtype=DTYPE)\n",
    "ks = torch.randn((B, NH, S, DH), device=DEVICE, dtype=DTYPE)\n",
    "vs = torch.randn((B, NH, S, DH), device=DEVICE, dtype=DTYPE)\n",
    "igs = torch.rand((B, NH, S, 1), device=DEVICE, dtype=DTYPE)\n",
    "igs2 = (1. + torch.arange((B * NH * S), device=DEVICE, dtype=DTYPE)).reshape(B, NH, S, 1)\n",
    "fgs = torch.rand((B, NH, S, 1), device=DEVICE, dtype=DTYPE)\n",
    "qs.shape, fgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-2.5441,  0.7163,  0.4934, -0.1267],\n",
       "           [-0.0950,  0.1215, -0.2144, -0.2080],\n",
       "           [-1.1865,  0.5090, -0.2582, -0.4349],\n",
       "           [ 0.6274, -0.0040, -1.7983,  1.1217],\n",
       "           [-0.2278,  0.4122, -2.1547,  1.6022],\n",
       "           [ 0.8848, -0.3820,  0.6012, -0.5168],\n",
       "           [ 1.9075, -1.2873, -0.1571,  0.0968],\n",
       "           [ 1.5725, -1.0859, -0.3932,  1.7004]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 8, 4]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = vlstm_fw_torch(\n",
    "    queries=qs,\n",
    "    keys=ks,\n",
    "    values=vs,\n",
    "    igate_preact=igs,\n",
    "    fgate_preact=fgs,\n",
    "    stabilize_rowwise=True,\n",
    ")\n",
    "rs, rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-2.5441,  0.7163,  0.4934, -0.1267],\n",
       "           [-0.0950,  0.1215, -0.2144, -0.2080],\n",
       "           [-1.1865,  0.5090, -0.2582, -0.4349],\n",
       "           [ 0.6274, -0.0040, -1.7983,  1.1217],\n",
       "           [-0.2278,  0.4122, -2.1547,  1.6022],\n",
       "           [ 0.8848, -0.3820,  0.6012, -0.5168],\n",
       "           [ 1.9075, -1.2873, -0.1571,  0.0968],\n",
       "           [ 1.5725, -1.0859, -0.3932,  1.7004]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 8, 4]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = vlstm_fw_torch(\n",
    "    queries=qs,\n",
    "    keys=ks,\n",
    "    values=vs,\n",
    "    igate_preact=igs,\n",
    "    fgate_preact=fgs,\n",
    "    stabilize_rowwise=False,\n",
    ")\n",
    "rs, rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCLUDE: ['/home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include', '/home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include/torch/csrc/api/include', '/home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include/TH', '/home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include/THC', '/home/max/miniconda3/envs/xlstmpt21cu118/include']\n",
      "/home/max/myrepos/vlstm_cuda/src\n",
      "/home/max/cpplibs/libtorch/lib:/usr/local/cuda-12.3/lib64:\n",
      "/home/max/miniconda3/envs/xlstmpt21cu118/lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/max/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/max/.cache/torch_extensions/py311_cu118/vlstm_fw_v0/build.ninja...\n",
      "Building extension module vlstm_fw_v0...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    }
   ],
   "source": [
    "from src.vlstm_fw_v0.interface import vlstm_fw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstmpt21cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
