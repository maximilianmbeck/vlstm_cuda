{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch \n",
    "torch.set_printoptions(linewidth=200, threshold=100000)\n",
    "\n",
    "from ln import MultiHeadLayerNorm\n",
    "from mlstm_parallel import mlstm_torch_autograd\n",
    "from mlstm_chunkwise._torch_fw_legacy import mlstm_chunkwise_parallel_legacy\n",
    "from mlstm_chunkwise.torch_fw import mlstm_chunkwise_parallel_fw_looped, mlstm_chunkwise_parallel_fw_parallel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match vLSTM chunkwise parallel to parallel (forward and backward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "S = 12 # seq len\n",
    "B = 1 # batch size\n",
    "NH = 1 # num heads\n",
    "DH = 6 # dim per head\n",
    "\n",
    "DTYPE = torch.float64\n",
    "PT_P_AG_DTYPE = torch.float64\n",
    "PT_CPL_AG_DTYPE = torch.float64\n",
    "PT_CPP_AG_DTYPE = torch.float64\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "EPS = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "matQ = torch.randn((B, NH, S, DH), dtype=DTYPE, device=DEVICE)\n",
    "matK = torch.randn((B, NH, S, DH), dtype=DTYPE, device=DEVICE)\n",
    "matV = torch.randn((B, NH, S, DH), dtype=DTYPE, device=DEVICE)\n",
    "vecI = torch.randn((B, NH, S), dtype=DTYPE, device=DEVICE)\n",
    "vecF = torch.randn((B, NH, S), dtype=DTYPE, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 3.* torch.randn((B, NH, S, DH), device=DEVICE, dtype=DTYPE) # offset for scaled version to have a larger gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float64, requires_grad=True),\n",
       " None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mh_layernorm = MultiHeadLayerNorm(NH*DH, eps=1e-6).to(device=DEVICE, dtype=DTYPE)\n",
    "mh_layernorm.weight, mh_layernorm.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parallel baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "matQ_pt_p_ag = matQ.clone().to(PT_P_AG_DTYPE).detach().requires_grad_(True)\n",
    "matK_pt_p_ag = matK.clone().to(PT_P_AG_DTYPE).detach().requires_grad_(True)\n",
    "matV_pt_p_ag = matV.clone().to(PT_P_AG_DTYPE).detach().requires_grad_(True)\n",
    "vecI_pt_p_ag = vecI.clone().to(PT_P_AG_DTYPE).detach().requires_grad_(True)\n",
    "vecF_pt_p_ag = vecF.clone().to(PT_P_AG_DTYPE).detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "matQ_pt_p_ag.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "matH_pt_p_ag = mlstm_torch_autograd(matQ_pt_p_ag, matK_pt_p_ag, matV_pt_p_ag, vecI_pt_p_ag, vecF_pt_p_ag, EPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matH_pt_p_ag.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parallel baseline. With GroupNorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matQ_pt_p_gn_ag = matQ.clone().to(PT_P_AG_DTYPE).detach().requires_grad_(True)\n",
    "matK_pt_p_gn_ag = matK.clone().to(PT_P_AG_DTYPE).detach().requires_grad_(True)\n",
    "matV_pt_p_gn_ag = matV.clone().to(PT_P_AG_DTYPE).detach().requires_grad_(True)\n",
    "vecI_pt_p_gn_ag = vecI.clone().to(PT_P_AG_DTYPE).detach().requires_grad_(True)\n",
    "vecF_pt_p_gn_ag = vecF.clone().to(PT_P_AG_DTYPE).detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "matQ_pt_p_gn_ag.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "matH_pt_p_gn_ag = mlstm_torch_autograd(matQ_pt_p_gn_ag, matK_pt_p_gn_ag, matV_pt_p_gn_ag, vecI_pt_p_gn_ag, vecF_pt_p_gn_ag, EPS)\n",
    "matH_pt_p_gn_ag_scaled = mh_layernorm(matH_pt_p_gn_ag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "((matH_pt_p_gn_ag_scaled + offset) ** 2).sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunkwise legacy version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.4967e+00, -6.5781e-01, -2.8072e-01,  9.9590e-01, -7.2010e-01,  8.2376e-03],\n",
       "          [ 1.1961e+00, -1.5677e-01,  1.4917e-01,  6.2761e-01, -9.5811e-01,  3.4698e-01],\n",
       "          [ 3.5374e-01, -1.1173e+00, -1.7381e+00, -3.8857e-01, -1.5804e-01, -5.8152e-01],\n",
       "          [ 3.5401e-02,  1.6806e-01,  6.9934e-01, -8.9639e-01,  3.4695e-01,  1.4512e-01],\n",
       "          [-1.0049e+00,  9.2938e-01,  8.7770e-02,  4.1880e-01, -1.1683e+00,  4.4696e-01],\n",
       "          [ 7.9167e-01,  1.6583e+00,  1.5999e+00,  1.2255e-01,  7.5304e-02, -1.3655e+00],\n",
       "          [-7.1761e-01,  2.2927e+00,  1.3356e+00,  1.7731e+00,  1.0226e-01, -4.0546e+00],\n",
       "          [-1.2917e+00, -3.1530e+00, -2.5687e+00,  2.9751e-01, -9.5043e-01,  1.6586e+00],\n",
       "          [ 2.8938e-03,  1.0294e+00,  1.7216e+00,  3.1265e-01, -1.0807e-01, -1.8735e+00],\n",
       "          [-3.6011e-02, -2.2485e-01, -2.7282e-01,  6.0402e-02,  2.2168e-02,  1.8846e-01],\n",
       "          [ 3.6046e-01,  6.9569e-01,  8.1353e-01,  4.3021e-01, -1.8089e-01,  1.6464e-01],\n",
       "          [ 3.5779e-01,  2.6625e-01,  1.1654e+00,  5.6096e-01, -6.2637e-02,  4.8294e-02]]]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matH_cpl = mlstm_chunkwise_parallel_legacy(matQ, matK, matV, vecI.unsqueeze(-1), vecF.unsqueeze(-1), chunk_size=4)\n",
    "matH_cpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 2.2204e-16, -5.5511e-17,  0.0000e+00,  1.1102e-16, -2.2204e-16,  0.0000e+00],\n",
       "          [ 5.5511e-17,  0.0000e+00,  2.2204e-16,  0.0000e+00, -5.5511e-17,  0.0000e+00],\n",
       "          [ 2.0817e-17, -2.7756e-17,  1.1102e-16, -2.2204e-16,  5.5511e-17,  2.7756e-17],\n",
       "          [ 2.2204e-16,  0.0000e+00,  1.3878e-17,  0.0000e+00,  0.0000e+00, -5.5511e-17],\n",
       "          [-2.2204e-16, -2.2204e-16, -2.2204e-16,  0.0000e+00, -6.9389e-17,  2.2204e-16],\n",
       "          [ 2.2204e-16,  8.8818e-16,  0.0000e+00,  4.4409e-16,  1.3878e-16,  0.0000e+00],\n",
       "          [ 2.2204e-16,  8.8818e-16,  4.4409e-16,  0.0000e+00,  2.2204e-16, -2.2204e-16],\n",
       "          [ 1.1926e-16,  1.1102e-15,  4.4409e-16,  4.4409e-16,  8.3267e-17, -1.3323e-15],\n",
       "          [-4.8572e-17, -3.3307e-16, -1.6653e-16,  3.4694e-17, -2.0817e-16,  1.3878e-16],\n",
       "          [ 1.1102e-16,  2.2204e-16,  1.1102e-16, -1.6653e-16,  1.6653e-16, -8.3267e-17],\n",
       "          [-1.6653e-16, -3.3307e-16, -2.2204e-16, -3.3307e-16,  2.2204e-16, -1.7347e-16]]]], device='cuda:0', dtype=torch.float64, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matH_pt_p_ag - matH_cpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3323e-15, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(matH_pt_p_ag - matH_cpl).abs().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunkwise looped version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "matQ_pt_cpl_ag = matQ.clone().to(PT_CPL_AG_DTYPE).detach().requires_grad_(True)\n",
    "matK_pt_cpl_ag = matK.clone().to(PT_CPL_AG_DTYPE).detach().requires_grad_(True)\n",
    "matV_pt_cpl_ag = matV.clone().to(PT_CPL_AG_DTYPE).detach().requires_grad_(True)\n",
    "vecI_pt_cpl_ag = vecI.clone().to(PT_CPL_AG_DTYPE).detach().requires_grad_(True)\n",
    "vecF_pt_cpl_ag = vecF.clone().to(PT_CPL_AG_DTYPE).detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "matH_cplo = mlstm_chunkwise_parallel_fw_looped(matQ_pt_cpl_ag, matK_pt_cpl_ag, matV_pt_cpl_ag, vecI_pt_cpl_ag, vecF_pt_cpl_ag, seq_chunk_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "matH_cplo.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 2.2204e-16, -5.5511e-17,  0.0000e+00,  1.1102e-16, -2.2204e-16,  0.0000e+00],\n",
       "          [ 5.5511e-17,  0.0000e+00,  2.2204e-16,  0.0000e+00, -5.5511e-17,  0.0000e+00],\n",
       "          [ 2.0817e-17, -2.7756e-17,  1.1102e-16, -2.2204e-16,  5.5511e-17,  2.7756e-17],\n",
       "          [ 2.2204e-16,  0.0000e+00,  1.3878e-17,  0.0000e+00,  0.0000e+00, -5.5511e-17],\n",
       "          [-2.2204e-16, -2.2204e-16, -2.2204e-16,  0.0000e+00, -6.9389e-17,  2.2204e-16],\n",
       "          [ 2.2204e-16,  8.8818e-16,  0.0000e+00,  4.4409e-16,  1.3878e-16,  0.0000e+00],\n",
       "          [ 2.2204e-16,  8.8818e-16,  4.4409e-16,  0.0000e+00,  2.2204e-16, -2.2204e-16],\n",
       "          [ 1.1926e-16,  1.1102e-15,  4.4409e-16,  4.4409e-16,  8.3267e-17, -1.3323e-15],\n",
       "          [-4.8572e-17, -3.3307e-16, -1.6653e-16,  3.4694e-17, -2.0817e-16,  1.3878e-16],\n",
       "          [ 1.1102e-16,  2.2204e-16,  1.1102e-16, -1.6653e-16,  1.6653e-16, -8.3267e-17],\n",
       "          [-1.6653e-16, -3.3307e-16, -2.2204e-16, -3.3307e-16,  2.2204e-16, -1.7347e-16]]]], device='cuda:0', dtype=torch.float64, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matH_pt_p_ag - matH_cplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3323e-15, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(matH_pt_p_ag - matH_cplo).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q grad diff: 1.9984014443252818e-15\n",
      "k grad diff: 3.552713678800501e-15\n",
      "v grad diff: 8.881784197001252e-16\n",
      "i grad diff: 1.7763568394002505e-15\n",
      "f grad diff: 1.1102230246251565e-15\n"
     ]
    }
   ],
   "source": [
    "print(f\"q grad diff: {(matQ_pt_p_ag.grad - matQ_pt_cpl_ag.grad).abs().max()}\")\n",
    "print(f\"k grad diff: {(matK_pt_p_ag.grad - matK_pt_cpl_ag.grad).abs().max()}\")\n",
    "print(f\"v grad diff: {(matV_pt_p_ag.grad - matV_pt_cpl_ag.grad).abs().max()}\")\n",
    "print(f\"i grad diff: {(vecI_pt_p_ag.grad - vecI_pt_cpl_ag.grad).abs().max()}\")\n",
    "print(f\"f grad diff: {(vecF_pt_p_ag.grad - vecF_pt_cpl_ag.grad).abs().max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunkwise parallel version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "matQ_pt_cpp_ag = matQ.clone().to(PT_CPP_AG_DTYPE).detach().requires_grad_(True)\n",
    "matK_pt_cpp_ag = matK.clone().to(PT_CPP_AG_DTYPE).detach().requires_grad_(True)\n",
    "matV_pt_cpp_ag = matV.clone().to(PT_CPP_AG_DTYPE).detach().requires_grad_(True)\n",
    "vecI_pt_cpp_ag = vecI.clone().to(PT_CPP_AG_DTYPE).detach().requires_grad_(True)\n",
    "vecF_pt_cpp_ag = vecF.clone().to(PT_CPP_AG_DTYPE).detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "matH_cppa = mlstm_chunkwise_parallel_fw_parallel(matQ_pt_cpp_ag, matK_pt_cpp_ag, matV_pt_cpp_ag, vecI_pt_cpp_ag, vecF_pt_cpp_ag, seq_chunk_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "matH_cppa.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 4.4409e-16, -1.3878e-16,  0.0000e+00,  2.2204e-16, -2.2204e-16,  0.0000e+00],\n",
       "          [ 5.5511e-17,  0.0000e+00,  2.2204e-16,  0.0000e+00, -2.7756e-17,  1.1102e-16],\n",
       "          [ 2.0817e-17, -2.7756e-17,  1.1102e-16, -2.2204e-16,  5.5511e-17,  2.7756e-17],\n",
       "          [ 2.2204e-16,  0.0000e+00,  1.3878e-17,  0.0000e+00,  0.0000e+00, -5.5511e-17],\n",
       "          [-2.2204e-16, -2.2204e-16, -2.2204e-16,  0.0000e+00, -6.9389e-17,  2.2204e-16],\n",
       "          [ 1.1102e-16,  8.8818e-16,  0.0000e+00,  4.4409e-16,  9.7145e-17,  0.0000e+00],\n",
       "          [ 4.4409e-16,  1.7764e-15,  4.4409e-16, -1.6653e-16,  4.4409e-16, -4.4409e-16],\n",
       "          [ 7.1991e-17,  8.8818e-16,  4.4409e-16,  4.9960e-16, -1.3878e-17, -1.3323e-15],\n",
       "          [-4.8572e-17, -3.0531e-16, -1.6653e-16,  3.4694e-17, -2.3592e-16,  1.3878e-16],\n",
       "          [ 1.6653e-16,  3.3307e-16,  2.2204e-16,  0.0000e+00,  1.1102e-16, -2.7756e-17],\n",
       "          [-1.6653e-16, -2.2204e-16, -2.2204e-16, -3.3307e-16,  2.0817e-16, -1.7347e-16]]]], device='cuda:0', dtype=torch.float64, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matH_pt_p_ag - matH_cppa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7764e-15, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(matH_pt_p_ag - matH_cppa).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q grad diff: 5.329070518200751e-15\n",
      "k grad diff: 7.105427357601002e-15\n",
      "v grad diff: 8.881784197001252e-16\n",
      "i grad diff: 5.329070518200751e-15\n",
      "f grad diff: 8.881784197001252e-16\n"
     ]
    }
   ],
   "source": [
    "print(f\"q grad diff: {(matQ_pt_p_ag.grad - matQ_pt_cpp_ag.grad).abs().max()}\")\n",
    "print(f\"k grad diff: {(matK_pt_p_ag.grad - matK_pt_cpp_ag.grad).abs().max()}\")\n",
    "print(f\"v grad diff: {(matV_pt_p_ag.grad - matV_pt_cpp_ag.grad).abs().max()}\")\n",
    "print(f\"i grad diff: {(vecI_pt_p_ag.grad - vecI_pt_cpp_ag.grad).abs().max()}\")\n",
    "print(f\"f grad diff: {(vecF_pt_p_ag.grad - vecF_pt_cpp_ag.grad).abs().max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunkwise parallel version. With GroupNorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "matQ_pt_cpp_gn_ag = matQ.clone().to(PT_CPP_AG_DTYPE).detach().requires_grad_(True)\n",
    "matK_pt_cpp_gn_ag = matK.clone().to(PT_CPP_AG_DTYPE).detach().requires_grad_(True)\n",
    "matV_pt_cpp_gn_ag = matV.clone().to(PT_CPP_AG_DTYPE).detach().requires_grad_(True)\n",
    "vecI_pt_cpp_gn_ag = vecI.clone().to(PT_CPP_AG_DTYPE).detach().requires_grad_(True)\n",
    "vecF_pt_cpp_gn_ag = vecF.clone().to(PT_CPP_AG_DTYPE).detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "matH_cppa_gn = mlstm_chunkwise_parallel_fw_parallel(matQ_pt_cpp_gn_ag, matK_pt_cpp_gn_ag, matV_pt_cpp_gn_ag, vecI_pt_cpp_gn_ag, vecF_pt_cpp_gn_ag, seq_chunk_size=4, detach_denominator=False)\n",
    "matH_cppa_gn_scaled = mh_layernorm(matH_cppa_gn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "((matH_cppa_gn_scaled + offset) ** 2).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 4.4409e-16, -1.3878e-16,  0.0000e+00,  2.2204e-16, -2.2204e-16,  0.0000e+00],\n",
       "          [ 5.5511e-17,  0.0000e+00,  2.2204e-16,  0.0000e+00, -2.7756e-17,  1.1102e-16],\n",
       "          [ 2.0817e-17, -2.7756e-17,  1.1102e-16, -2.2204e-16,  5.5511e-17,  2.7756e-17],\n",
       "          [ 2.2204e-16,  0.0000e+00,  1.3878e-17,  0.0000e+00,  0.0000e+00, -5.5511e-17],\n",
       "          [-2.2204e-16, -2.2204e-16, -2.2204e-16,  0.0000e+00, -6.9389e-17,  2.2204e-16],\n",
       "          [ 1.1102e-16,  8.8818e-16,  0.0000e+00,  4.4409e-16,  9.7145e-17,  0.0000e+00],\n",
       "          [ 4.4409e-16,  1.7764e-15,  4.4409e-16, -1.6653e-16,  4.4409e-16, -4.4409e-16],\n",
       "          [ 7.1991e-17,  8.8818e-16,  4.4409e-16,  4.9960e-16, -1.3878e-17, -1.3323e-15],\n",
       "          [-4.8572e-17, -3.0531e-16, -1.6653e-16,  3.4694e-17, -2.3592e-16,  1.3878e-16],\n",
       "          [ 1.6653e-16,  3.3307e-16,  2.2204e-16,  0.0000e+00,  1.1102e-16, -2.7756e-17],\n",
       "          [-1.6653e-16, -2.2204e-16, -2.2204e-16, -3.3307e-16,  2.0817e-16, -1.7347e-16]]]], device='cuda:0', dtype=torch.float64, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matH_pt_p_gn_ag - matH_cppa_gn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7764e-15, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(matH_pt_p_gn_ag - matH_cppa_gn).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q grad diff: 1.0658141036401503e-14\n",
      "k grad diff: 8.881784197001252e-15\n",
      "v grad diff: 8.881784197001252e-15\n",
      "i grad diff: 1.2212453270876722e-14\n",
      "f grad diff: 4.884981308350689e-15\n"
     ]
    }
   ],
   "source": [
    "print(f\"q grad diff: {(matQ_pt_p_gn_ag.grad - matQ_pt_cpp_gn_ag.grad).abs().max()}\")\n",
    "print(f\"k grad diff: {(matK_pt_p_gn_ag.grad - matK_pt_cpp_gn_ag.grad).abs().max()}\")\n",
    "print(f\"v grad diff: {(matV_pt_p_gn_ag.grad - matV_pt_cpp_gn_ag.grad).abs().max()}\")\n",
    "print(f\"i grad diff: {(vecI_pt_p_gn_ag.grad - vecI_pt_cpp_gn_ag.grad).abs().max()}\")\n",
    "print(f\"f grad diff: {(vecF_pt_p_gn_ag.grad - vecF_pt_cpp_gn_ag.grad).abs().max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunkwise parallel version. With GroupNorm. Normalizer detached.\n",
    "\n",
    "Should still match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "matQ_pt_cpp_gn_ag = matQ.clone().to(PT_CPP_AG_DTYPE).detach().requires_grad_(True)\n",
    "matK_pt_cpp_gn_ag = matK.clone().to(PT_CPP_AG_DTYPE).detach().requires_grad_(True)\n",
    "matV_pt_cpp_gn_ag = matV.clone().to(PT_CPP_AG_DTYPE).detach().requires_grad_(True)\n",
    "vecI_pt_cpp_gn_ag = vecI.clone().to(PT_CPP_AG_DTYPE).detach().requires_grad_(True)\n",
    "vecF_pt_cpp_gn_ag = vecF.clone().to(PT_CPP_AG_DTYPE).detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "matH_cppa_gn = mlstm_chunkwise_parallel_fw_parallel(matQ_pt_cpp_gn_ag, matK_pt_cpp_gn_ag, matV_pt_cpp_gn_ag, vecI_pt_cpp_gn_ag, vecF_pt_cpp_gn_ag, seq_chunk_size=4, detach_denominator=True)\n",
    "matH_cppa_gn_scaled = mh_layernorm(matH_cppa_gn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "((matH_cppa_gn_scaled + offset) ** 2).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 4.4409e-16, -1.3878e-16,  0.0000e+00,  2.2204e-16, -2.2204e-16,  0.0000e+00],\n",
       "          [ 5.5511e-17,  0.0000e+00,  2.2204e-16,  0.0000e+00, -2.7756e-17,  1.1102e-16],\n",
       "          [ 2.0817e-17, -2.7756e-17,  1.1102e-16, -2.2204e-16,  5.5511e-17,  2.7756e-17],\n",
       "          [ 2.2204e-16,  0.0000e+00,  1.3878e-17,  0.0000e+00,  0.0000e+00, -5.5511e-17],\n",
       "          [-2.2204e-16, -2.2204e-16, -2.2204e-16,  0.0000e+00, -6.9389e-17,  2.2204e-16],\n",
       "          [ 1.1102e-16,  8.8818e-16,  0.0000e+00,  4.4409e-16,  9.7145e-17,  0.0000e+00],\n",
       "          [ 4.4409e-16,  1.7764e-15,  4.4409e-16, -1.6653e-16,  4.4409e-16, -4.4409e-16],\n",
       "          [ 7.1991e-17,  8.8818e-16,  4.4409e-16,  4.9960e-16, -1.3878e-17, -1.3323e-15],\n",
       "          [-4.8572e-17, -3.0531e-16, -1.6653e-16,  3.4694e-17, -2.3592e-16,  1.3878e-16],\n",
       "          [ 1.6653e-16,  3.3307e-16,  2.2204e-16,  0.0000e+00,  1.1102e-16, -2.7756e-17],\n",
       "          [-1.6653e-16, -2.2204e-16, -2.2204e-16, -3.3307e-16,  2.0817e-16, -1.7347e-16]]]], device='cuda:0', dtype=torch.float64, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matH_pt_p_gn_ag - matH_cppa_gn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7764e-15, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(matH_pt_p_gn_ag - matH_cppa_gn).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q grad diff: 8.372333267328833e-05\n",
      "k grad diff: 9.059612933826067e-05\n",
      "v grad diff: 8.881784197001252e-15\n",
      "i grad diff: 0.001453663991701326\n",
      "f grad diff: 1.2969772461013385e-06\n"
     ]
    }
   ],
   "source": [
    "# when we detach the denominator in the parallel version too the difference is everywhere < 1e-14 (for torch.float64)\n",
    "print(f\"q grad diff: {(matQ_pt_p_gn_ag.grad - matQ_pt_cpp_gn_ag.grad).abs().max()}\")\n",
    "print(f\"k grad diff: {(matK_pt_p_gn_ag.grad - matK_pt_cpp_gn_ag.grad).abs().max()}\")\n",
    "print(f\"v grad diff: {(matV_pt_p_gn_ag.grad - matV_pt_cpp_gn_ag.grad).abs().max()}\")\n",
    "print(f\"i grad diff: {(vecI_pt_p_gn_ag.grad - vecI_pt_cpp_gn_ag.grad).abs().max()}\")\n",
    "print(f\"f grad diff: {(vecF_pt_p_gn_ag.grad - vecF_pt_cpp_gn_ag.grad).abs().max()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstmpt220cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
