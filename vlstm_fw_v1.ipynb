{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "sys.path.append(\".\")\n",
    "# os.environ[\"MAX_JOBS\"] = \"100\"\n",
    "\n",
    "import torch\n",
    "torch.set_printoptions(linewidth=200)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCLUDE: ['/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include', '/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/torch/csrc/api/include', '/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/TH', '/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/THC', '/home/max/miniconda3/envs/xlstmpt220cu121/include']\n",
      "/home/max/myrepos/vlstm_cuda/src\n",
      "/home/max/cpplibs/libtorch/lib:/usr/local/cuda-12.3/lib64:\n",
      "/home/max/miniconda3/envs/xlstmpt220cu121/lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/max/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/max/.cache/torch_extensions/py311_cu121/vlstm_fw_v1/build.ninja...\n",
      "Building extension module vlstm_fw_v1...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] /home/max/miniconda3/envs/xlstmpt220cu121/bin/nvcc --generate-dependencies-with-compile --dependency-output kernels.cuda.o.d -ccbin /home/max/miniconda3/envs/xlstmpt220cu121/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=vlstm_fw_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include -isystem /home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/TH -isystem /home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/THC -isystem /home/max/miniconda3/envs/xlstmpt220cu121/include -isystem /home/max/miniconda3/envs/xlstmpt220cu121/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -gencode arch=compute_80,code=compute_80 -res-usage --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=13407807929942597099574024998205846127479365820592393377723561443721764030073546976801874298166903427690031858186486050853753882811946569946433649006084096 -std=c++17 -c /home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/kernels.cu -o kernels.cuda.o \n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops.cuh(260): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops.cuh:260 CUDART_VERSION: 12010, arch: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops.cuh\" \":\" \"260\" \" CUDART_VERSION: \" \"12010\" \", arch: \" \"890\")\n",
      "                                                                                                                                               ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops.cuh(265): remark #20200-D: #pragma message: \"INCLUDING FP16\"\n",
      "  #pragma message(\"INCLUDING FP16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                                  ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_2fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_2fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_2fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops.cuh(278): remark #20200-D: #pragma message: \"INCLUDING BF16\"\n",
      "  #pragma message(\"INCLUDING BF16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_bf16.cuh:29 CUDART_VERSION with BF16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with BF16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                                  ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_2bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_2bf16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_2bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                                   ^\n",
      "\n",
      "ptxas info    : 279 bytes gmem\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels8vlstm_fwIfLi4ELi8ELi8EEEvPT_S3_S3_S3_S3_S3_S3_iiii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels8vlstm_fwIfLi4ELi8ELi8EEEvPT_S3_S3_S3_S3_S3_S3_iiii\n",
      "    56 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 114 registers, 424 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels8vlstm_fwI6__halfLi4ELi8ELi8EEEvPT_S4_S4_S4_S4_S4_S4_iiii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels8vlstm_fwI6__halfLi4ELi8ELi8EEEvPT_S4_S4_S4_S4_S4_S4_iiii\n",
      "    56 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 113 registers, 424 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels8vlstm_fwI13__nv_bfloat16Li4ELi8ELi8EEEvPT_S4_S4_S4_S4_S4_S4_iiii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels8vlstm_fwI13__nv_bfloat16Li4ELi8ELi8EEEvPT_S4_S4_S4_S4_S4_S4_iiii\n",
      "    56 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 112 registers, 424 bytes cmem[0]\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops.cuh(260): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops.cuh:260 CUDART_VERSION: 12010, arch: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops.cuh\" \":\" \"260\" \" CUDART_VERSION: \" \"12010\" \", arch: \" \"800\")\n",
      "                                                                                                                                               ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops.cuh(265): remark #20200-D: #pragma message: \"INCLUDING FP16\"\n",
      "  #pragma message(\"INCLUDING FP16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                                  ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_2fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_2fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_2fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops.cuh(278): remark #20200-D: #pragma message: \"INCLUDING BF16\"\n",
      "  #pragma message(\"INCLUDING BF16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_bf16.cuh:29 CUDART_VERSION with BF16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with BF16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                                  ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_2bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_2bf16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops_2bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops.cuh:260:136: note: '#pragma message: /home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops.cuh:260 CUDART_VERSION: 12010, arch: __CUDA_ARCH__'\n",
      "  260 | #pragma message(AT \" CUDART_VERSION: \" TOSTRING(                               \\\n",
      "      |                                                                                                                                        ^\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops.cuh:272:90: note: '#pragma message: SKIPPING FP16, because of CUDART_VERSION: 12010, arch: __CUDA_ARCH__'\n",
      "  272 | #pragma message(\"SKIPPING FP16, because of CUDART_VERSION: \" TOSTRING(         \\\n",
      "      |                                                                                          ^\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fw_v1/../util/inline_ops.cuh:285:90: note: '#pragma message: SKIPPING BF16, because of CUDART_VERSION: 12010, arch: __CUDA_ARCH__'\n",
      "  285 | #pragma message(\"SKIPPING BF16, because of CUDART_VERSION: \" TOSTRING(         \\\n",
      "      |                                                                                          ^\n",
      "[2/2] /home/max/miniconda3/envs/xlstmpt220cu121/bin/x86_64-conda-linux-gnu-c++ interface.o kernels.cuda.o -shared -L/home/max/miniconda3/envs/xlstmpt220cu121/lib -lcublas -L/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/home/max/miniconda3/envs/xlstmpt220cu121/lib -lcudart -o vlstm_fw_v1.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module vlstm_fw_v1...\n"
     ]
    }
   ],
   "source": [
    "from src.vlstm_fw_v1.interface import vlstm_fw_torch\n",
    "from src.vlstm_fw_v1.interface import vlstm_fw_cuda\n",
    "from vlstm_formulations.vlstm_parallel import vlstm_parallel_fw_torch\n",
    "from vlstm_formulations.vlstm_parallel_tiled import vlstm_parallel_tiled\n",
    "\n",
    "# Commit: 59a91237f7e6733247aa5d2b3da8a66970e7bf54\n",
    "# ptxas info    : 343 bytes gmem\n",
    "# ptxas info    : Compiling entry function '_ZN5vlstm7kernels8vlstm_fwIfLi4ELi8ELi8EEEvPT_S3_S3_S3_iiii' for 'sm_89'\n",
    "# ptxas info    : Function properties for _ZN5vlstm7kernels8vlstm_fwIfLi4ELi8ELi8EEEvPT_S3_S3_S3_iiii\n",
    "#     24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
    "# ptxas info    : Used 66 registers, 400 bytes cmem[0]\n",
    "# ptxas info    : Compiling entry function '_ZN5vlstm7kernels8vlstm_fwI6__halfLi4ELi8ELi8EEEvPT_S4_S4_S4_iiii' for 'sm_89'\n",
    "# ptxas info    : Function properties for _ZN5vlstm7kernels8vlstm_fwI6__halfLi4ELi8ELi8EEEvPT_S4_S4_S4_iiii\n",
    "#     24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
    "# ptxas info    : Used 72 registers, 400 bytes cmem[0]\n",
    "# ptxas info    : Compiling entry function '_ZN5vlstm7kernels8vlstm_fwI13__nv_bfloat16Li4ELi8ELi8EEEvPT_S4_S4_S4_iiii' for 'sm_89'\n",
    "# ptxas info    : Function properties for _ZN5vlstm7kernels8vlstm_fwI13__nv_bfloat16Li4ELi8ELi8EEEvPT_S4_S4_S4_iiii\n",
    "#     24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
    "# ptxas info    : Used 63 registers, 400 bytes cmem[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA vLSTM forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "S = 8 #16 #16 #8 # seq len\n",
    "B = 1 # batch size\n",
    "NH = 1 # num heads\n",
    "DH = 8 # dim per head\n",
    "DTYPE = torch.float32\n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000],\n",
       "           [0.8000, 0.9000, 1.0000, 1.1000, 1.2000, 1.3000, 1.4000, 1.5000],\n",
       "           [1.6000, 1.7000, 1.8000, 1.9000, 2.0000, 2.1000, 2.2000, 2.3000],\n",
       "           [2.4000, 2.5000, 2.6000, 2.7000, 2.8000, 2.9000, 3.0000, 3.1000],\n",
       "           [3.2000, 3.3000, 3.4000, 3.5000, 3.6000, 3.7000, 3.8000, 3.9000],\n",
       "           [4.0000, 4.1000, 4.2000, 4.3000, 4.4000, 4.5000, 4.6000, 4.7000],\n",
       "           [4.8000, 4.9000, 5.0000, 5.1000, 5.2000, 5.3000, 5.4000, 5.5000],\n",
       "           [5.6000, 5.7000, 5.8000, 5.9000, 6.0000, 6.1000, 6.2000, 6.3000]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 8, 8]),\n",
       " 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create qkv, inputgates, forgetgates \n",
    "torch.manual_seed(0)\n",
    "qs = torch.arange((B*NH*S*DH), device=DEVICE, dtype=DTYPE).reshape((B, NH, S, DH)) / 10.\n",
    "ks = torch.ones((B, NH, S, DH), device=DEVICE, dtype=DTYPE) / 100.\n",
    "vs = torch.ones((B, NH, S, DH), device=DEVICE, dtype=DTYPE) / 100.\n",
    "igs = (1. + torch.arange((B * NH * S), device=DEVICE, dtype=DTYPE)).reshape(B, NH, S, 1) / 10.\n",
    "# igs = torch.zeros((B, NH, S, 1), device=DEVICE, dtype=DTYPE) #/ 10.\n",
    "fgs = torch.ones((B, NH, S, 1), device=DEVICE, dtype=DTYPE)\n",
    "qs, qs.shape, len(qs.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_gt = vlstm_parallel_fw_torch(queries=qs, keys=ks, values=vs, igate_preact=igs, fgate_preact=fgs)\n",
    "# result_gt, result_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch version QKV product (To test if this is still correct after changes to the code.)\n",
    "# at some point we have to compare to the vlstm_fw_torch version.\n",
    "# rs = qs @ ks.transpose(-1, -2) @ vs\n",
    "# rs, rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs_torch = (qs @ ks.transpose(-1, -2) * torch.tril(torch.ones((B, NH, S, S))).to(device=DEVICE, dtype=DTYPE)) @ vs\n",
    "# rs_torch, rs_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_tiles: 1, torch.Size([1, 1, 8, 8])\n",
      "kv_tiles: 1, torch.Size([1, 1, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
       "           [0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 8, 8]),\n",
       " tensor([[[[1.],\n",
       "           [2.],\n",
       "           [3.],\n",
       "           [4.],\n",
       "           [5.],\n",
       "           [6.],\n",
       "           [7.],\n",
       "           [8.]]]], device='cuda:0'),\n",
       " tensor([[[[0.0099],\n",
       "           [0.0413],\n",
       "           [0.0740],\n",
       "           [0.1058],\n",
       "           [0.1372],\n",
       "           [0.1682],\n",
       "           [0.1992],\n",
       "           [0.2302]]]], device='cuda:0'),\n",
       " tensor([[[[0.3679],\n",
       "           [0.1353],\n",
       "           [0.0740],\n",
       "           [0.1058],\n",
       "           [0.1372],\n",
       "           [0.1682],\n",
       "           [0.1992],\n",
       "           [0.2302]]]], device='cuda:0'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_gt, m, l, n = vlstm_parallel_tiled(queries=qs, keys=ks, values=vs, igate_preact=igs, fgate_preact=fgs)\n",
    "result_gt, result_gt.shape, m, l, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
       "           [0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 8, 8]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch version\n",
    "rs, log_fgm, igm, matD = vlstm_fw_torch(\n",
    "    queries=qs,\n",
    "    keys=ks,\n",
    "    values=vs,\n",
    "    igate_preact=igs,\n",
    "    fgate_preact=fgs,\n",
    ")\n",
    "rs, rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qTileIdx=0, kvTileIdx=0, cTileBlockXIdx=0, cTileBlockYIdx=0, hWTileSMIdxY=0, tbIdxXY=(0,0): m_val=1.000000, m_prev_val=-inf, n_val=0.367879, n_prev_val=0.000000\n",
    "# qTileIdx=0, kvTileIdx=0, cTileBlockXIdx=0, cTileBlockYIdx=0, hWTileSMIdxY=1, tbIdxXY=(0,1): m_val=2.000000, m_prev_val=-inf, n_val=0.135335, n_prev_val=0.000000\n",
    "# qTileIdx=0, kvTileIdx=0, cTileBlockXIdx=0, cTileBlockYIdx=0, hWTileSMIdxY=2, tbIdxXY=(0,2): m_val=3.000000, m_prev_val=-inf, n_val=0.209238, n_prev_val=0.000000\n",
    "# qTileIdx=0, kvTileIdx=0, cTileBlockXIdx=0, cTileBlockYIdx=0, hWTileSMIdxY=3, tbIdxXY=(0,3): m_val=4.000000, m_prev_val=-inf, n_val=0.299359, n_prev_val=0.000000\n",
    "# qTileIdx=0, kvTileIdx=0, cTileBlockXIdx=0, cTileBlockYIdx=0, hWTileSMIdxY=4, tbIdxXY=(0,0): m_val=5.000000, m_prev_val=-inf, n_val=0.387931, n_prev_val=0.000000\n",
    "# qTileIdx=0, kvTileIdx=0, cTileBlockXIdx=0, cTileBlockYIdx=0, hWTileSMIdxY=5, tbIdxXY=(0,1): m_val=6.000000, m_prev_val=-inf, n_val=0.475842, n_prev_val=0.000000\n",
    "# qTileIdx=0, kvTileIdx=0, cTileBlockXIdx=0, cTileBlockYIdx=0, hWTileSMIdxY=6, tbIdxXY=(0,2): m_val=7.000000, m_prev_val=-inf, n_val=0.563509, n_prev_val=0.000000\n",
    "# qTileIdx=0, kvTileIdx=0, cTileBlockXIdx=0, cTileBlockYIdx=0, hWTileSMIdxY=7, tbIdxXY=(0,3): m_val=8.000000, m_prev_val=-inf, n_val=0.651093, n_prev_val=0.000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - float32!\n",
      "B: 1, NH: 1, S: 8, DH: 8\n",
      "blocksxy: 1-1, threadsxy: 4-4, shared_mem in bytes: 5700\n",
      "In Kernel: gdim.x: 1, gdim.y: 1, gdim.z: 1, bdim.x: 4, bdim.y: 4\n",
      "In Kernel: QtileDim: 8, KVtileDim: 8, TblockDim:4\n",
      "qTileIdx=0, kvTileIdx=0, cTileBlockXIdx=0, cTileBlockYIdx=0, lThreadSMIdx=0, tbIdxXY=(0,0): l_val=0.028000, exp(-m_val)=0.367879, n_val=0.367879\n",
      "qTileIdx=0, kvTileIdx=0, cTileBlockXIdx=0, cTileBlockYIdx=0, lThreadSMIdx=1, tbIdxXY=(1,0): l_val=0.116743, exp(-m_val)=0.135335, n_val=0.135335\n",
      "qTileIdx=0, kvTileIdx=0, cTileBlockXIdx=0, cTileBlockYIdx=0, lThreadSMIdx=2, tbIdxXY=(2,0): l_val=0.209238, exp(-m_val)=0.049787, n_val=0.209238\n",
      "qTileIdx=0, kvTileIdx=0, cTileBlockXIdx=0, cTileBlockYIdx=0, lThreadSMIdx=3, tbIdxXY=(3,0): l_val=0.299359, exp(-m_val)=0.018316, n_val=0.299359\n",
      "qTileIdx=0, kvTileIdx=0, cTileBlockXIdx=0, cTileBlockYIdx=0, lThreadSMIdx=4, tbIdxXY=(0,1): l_val=0.387931, exp(-m_val)=0.006738, n_val=0.387931\n",
      "qTileIdx=0, kvTileIdx=0, cTileBlockXIdx=0, cTileBlockYIdx=0, lThreadSMIdx=5, tbIdxXY=(1,1): l_val=0.475842, exp(-m_val)=0.002479, n_val=0.475842\n",
      "qTileIdx=0, kvTileIdx=0, cTileBlockXIdx=0, cTileBlockYIdx=0, lThreadSMIdx=6, tbIdxXY=(2,1): l_val=0.563509, exp(-m_val)=0.000912, n_val=0.563509\n",
      "qTileIdx=0, kvTileIdx=0, cTileBlockXIdx=0, cTileBlockYIdx=0, lThreadSMIdx=7, tbIdxXY=(3,1): l_val=0.651093, exp(-m_val)=0.000335, n_val=0.651093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008],\n",
       "           [0.0086, 0.0086, 0.0086, 0.0086, 0.0086, 0.0086, 0.0086, 0.0086],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 8, 8]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cuda kernel\n",
    "rs_cuda, matC = vlstm_fw_cuda(mat_Q=qs, mat_K=ks, mat_V=vs, igate_preact=igs.squeeze(-1), fgate_preact=fgs.squeeze(-1))\n",
    "rs_cuda, rs_cuda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-4.9202e-04, -4.9202e-04, -4.9202e-04, -4.9202e-04, -4.9202e-04, -4.9202e-04, -4.9202e-04, -4.9202e-04],\n",
       "          [-5.5764e-03, -5.5764e-03, -5.5764e-03, -5.5764e-03, -5.5764e-03, -5.5764e-03, -5.5764e-03, -5.5764e-03],\n",
       "          [-1.3411e-07, -1.3411e-07, -1.3411e-07, -1.3411e-07, -1.3411e-07, -1.3411e-07, -1.3411e-07, -1.3411e-07],\n",
       "          [-9.4064e-08, -9.4064e-08, -9.4064e-08, -9.4064e-08, -9.4064e-08, -9.4064e-08, -9.4064e-08, -9.4064e-08],\n",
       "          [-7.3574e-08, -7.3574e-08, -7.3574e-08, -7.3574e-08, -7.3574e-08, -7.3574e-08, -7.3574e-08, -7.3574e-08],\n",
       "          [-5.9605e-08, -5.9605e-08, -5.9605e-08, -5.9605e-08, -5.9605e-08, -5.9605e-08, -5.9605e-08, -5.9605e-08],\n",
       "          [-5.0291e-08, -5.0291e-08, -5.0291e-08, -5.0291e-08, -5.0291e-08, -5.0291e-08, -5.0291e-08, -5.0291e-08],\n",
       "          [-4.3772e-08, -4.3772e-08, -4.3772e-08, -4.3772e-08, -4.3772e-08, -4.3772e-08, -4.3772e-08, -4.3772e-08]]]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs - rs_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.6867,  2.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.3735,  1.6867,  3.0000,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.0602,  1.3735,  2.6867,  4.0000,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.2530,  1.0602,  2.3735,  3.6867,  5.0000,    -inf,    -inf,    -inf],\n",
       "          [-0.5663,  0.7470,  2.0602,  3.3735,  4.6867,  6.0000,    -inf,    -inf],\n",
       "          [-0.8796,  0.4337,  1.7470,  3.0602,  4.3735,  5.6867,  7.0000,    -inf],\n",
       "          [-1.1928,  0.1204,  1.4337,  2.7470,  4.0602,  5.3735,  6.6867,  8.0000]]]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.6867,  2.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.3735,  1.6867,  3.0000,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.0602,  1.3735,  2.6867,  4.0000,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.2530,  1.0602,  2.3735,  3.6867,  5.0000,    -inf,    -inf,    -inf],\n",
       "          [-0.5663,  0.7470,  2.0602,  3.3735,  4.6867,  6.0000,    -inf,    -inf],\n",
       "          [-0.8796,  0.4337,  1.7470,  3.0602,  4.3735,  5.6867,  7.0000,    -inf],\n",
       "          [-1.1928,  0.1204,  1.4337,  2.7470,  4.0602,  5.3735,  6.6867,  8.0000]]]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000e+00,        nan,        nan,        nan,        nan,        nan,        nan,        nan],\n",
       "          [5.9605e-08, 0.0000e+00,        nan,        nan,        nan,        nan,        nan,        nan],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00,        nan,        nan,        nan,        nan,        nan],\n",
       "          [5.9605e-08, 1.1921e-07, 0.0000e+00, 0.0000e+00,        nan,        nan,        nan,        nan],\n",
       "          [1.1921e-07, 2.3842e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00,        nan,        nan,        nan],\n",
       "          [1.1921e-07, 2.3842e-07, 2.3842e-07, 0.0000e+00, 4.7684e-07, 0.0000e+00,        nan,        nan],\n",
       "          [0.0000e+00, 0.0000e+00, 1.1921e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,        nan],\n",
       "          [0.0000e+00, 0.0000e+00, 1.1921e-07, 2.3842e-07, 0.0000e+00, 0.0000e+00, 4.7684e-07, 0.0000e+00]]]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matC - matD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstmpt21cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
