{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "sys.path.append(\".\")\n",
    "# os.environ[\"MAX_JOBS\"] = \"100\"\n",
    "\n",
    "import torch\n",
    "torch.set_printoptions(linewidth=200)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCLUDE: ['/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include', '/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/torch/csrc/api/include', '/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/TH', '/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/THC', '/home/max/miniconda3/envs/xlstmpt220cu121/include']\n",
      "/home/max/myrepos/vlstm_cuda/src\n",
      "/home/max/cpplibs/libtorch/lib:/usr/local/cuda-12.3/lib64:\n",
      "/home/max/miniconda3/envs/xlstmpt220cu121/lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/max/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/max/.cache/torch_extensions/py311_cu121/vlstm_fw_v1/build.ninja...\n",
      "Building extension module vlstm_fw_v1...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module vlstm_fw_v1...\n"
     ]
    }
   ],
   "source": [
    "from src.vlstm_fw_v1.interface import vlstm_fw_torch\n",
    "from src.vlstm_fw_v1.interface import vlstm_fw_cuda\n",
    "\n",
    "# Commit: 59a91237f7e6733247aa5d2b3da8a66970e7bf54\n",
    "# ptxas info    : 343 bytes gmem\n",
    "# ptxas info    : Compiling entry function '_ZN5vlstm7kernels8vlstm_fwIfLi4ELi8ELi8EEEvPT_S3_S3_S3_iiii' for 'sm_89'\n",
    "# ptxas info    : Function properties for _ZN5vlstm7kernels8vlstm_fwIfLi4ELi8ELi8EEEvPT_S3_S3_S3_iiii\n",
    "#     24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
    "# ptxas info    : Used 66 registers, 400 bytes cmem[0]\n",
    "# ptxas info    : Compiling entry function '_ZN5vlstm7kernels8vlstm_fwI6__halfLi4ELi8ELi8EEEvPT_S4_S4_S4_iiii' for 'sm_89'\n",
    "# ptxas info    : Function properties for _ZN5vlstm7kernels8vlstm_fwI6__halfLi4ELi8ELi8EEEvPT_S4_S4_S4_iiii\n",
    "#     24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
    "# ptxas info    : Used 72 registers, 400 bytes cmem[0]\n",
    "# ptxas info    : Compiling entry function '_ZN5vlstm7kernels8vlstm_fwI13__nv_bfloat16Li4ELi8ELi8EEEvPT_S4_S4_S4_iiii' for 'sm_89'\n",
    "# ptxas info    : Function properties for _ZN5vlstm7kernels8vlstm_fwI13__nv_bfloat16Li4ELi8ELi8EEEvPT_S4_S4_S4_iiii\n",
    "#     24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
    "# ptxas info    : Used 63 registers, 400 bytes cmem[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA vLSTM forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "S = 16 #16 #8 # seq len\n",
    "B = 1 # batch size\n",
    "NH = 1 # num heads\n",
    "DH = 8 # dim per head\n",
    "DTYPE = torch.float32\n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.0000,  0.1000,  0.2000,  0.3000,  0.4000,  0.5000,  0.6000,  0.7000],\n",
       "           [ 0.8000,  0.9000,  1.0000,  1.1000,  1.2000,  1.3000,  1.4000,  1.5000],\n",
       "           [ 1.6000,  1.7000,  1.8000,  1.9000,  2.0000,  2.1000,  2.2000,  2.3000],\n",
       "           [ 2.4000,  2.5000,  2.6000,  2.7000,  2.8000,  2.9000,  3.0000,  3.1000],\n",
       "           [ 3.2000,  3.3000,  3.4000,  3.5000,  3.6000,  3.7000,  3.8000,  3.9000],\n",
       "           [ 4.0000,  4.1000,  4.2000,  4.3000,  4.4000,  4.5000,  4.6000,  4.7000],\n",
       "           [ 4.8000,  4.9000,  5.0000,  5.1000,  5.2000,  5.3000,  5.4000,  5.5000],\n",
       "           [ 5.6000,  5.7000,  5.8000,  5.9000,  6.0000,  6.1000,  6.2000,  6.3000],\n",
       "           [ 6.4000,  6.5000,  6.6000,  6.7000,  6.8000,  6.9000,  7.0000,  7.1000],\n",
       "           [ 7.2000,  7.3000,  7.4000,  7.5000,  7.6000,  7.7000,  7.8000,  7.9000],\n",
       "           [ 8.0000,  8.1000,  8.2000,  8.3000,  8.4000,  8.5000,  8.6000,  8.7000],\n",
       "           [ 8.8000,  8.9000,  9.0000,  9.1000,  9.2000,  9.3000,  9.4000,  9.5000],\n",
       "           [ 9.6000,  9.7000,  9.8000,  9.9000, 10.0000, 10.1000, 10.2000, 10.3000],\n",
       "           [10.4000, 10.5000, 10.6000, 10.7000, 10.8000, 10.9000, 11.0000, 11.1000],\n",
       "           [11.2000, 11.3000, 11.4000, 11.5000, 11.6000, 11.7000, 11.8000, 11.9000],\n",
       "           [12.0000, 12.1000, 12.2000, 12.3000, 12.4000, 12.5000, 12.6000, 12.7000]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 16, 8]),\n",
       " 128)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create qkv, inputgates, forgetgates \n",
    "torch.manual_seed(0)\n",
    "qs = torch.arange((B*NH*S*DH), device=DEVICE, dtype=DTYPE).reshape((B, NH, S, DH)) / 10.\n",
    "ks = torch.ones((B, NH, S, DH), device=DEVICE, dtype=DTYPE) / 100.\n",
    "vs = torch.ones((B, NH, S, DH), device=DEVICE, dtype=DTYPE) / 100.\n",
    "igs = (1. + torch.arange((B * NH * S), device=DEVICE, dtype=DTYPE)).reshape(B, NH, S, 1)# / 100.\n",
    "# igs = torch.zeros((B, NH, S, 1), device=DEVICE, dtype=DTYPE) #/ 10.\n",
    "fgs = torch.ones((B, NH, S, 1), device=DEVICE, dtype=DTYPE)\n",
    "qs, qs.shape, len(qs.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
       "           [0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "           [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 16, 8]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch version\n",
    "rs, log_fgm, igm, matD = vlstm_fw_torch(\n",
    "    queries=qs,\n",
    "    keys=ks,\n",
    "    values=vs,\n",
    "    igate_preact=igs,\n",
    "    fgate_preact=fgs,\n",
    ")\n",
    "rs, rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0045, 0.0045, 0.0045, 0.0045, 0.0045, 0.0045, 0.0045, 0.0045],\n",
       "           [0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147],\n",
       "           [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
       "           [0.0352, 0.0352, 0.0352, 0.0352, 0.0352, 0.0352, 0.0352, 0.0352],\n",
       "           [0.0454, 0.0454, 0.0454, 0.0454, 0.0454, 0.0454, 0.0454, 0.0454],\n",
       "           [0.0557, 0.0557, 0.0557, 0.0557, 0.0557, 0.0557, 0.0557, 0.0557],\n",
       "           [0.0659, 0.0659, 0.0659, 0.0659, 0.0659, 0.0659, 0.0659, 0.0659],\n",
       "           [0.0762, 0.0762, 0.0762, 0.0762, 0.0762, 0.0762, 0.0762, 0.0762],\n",
       "           [0.0864, 0.0864, 0.0864, 0.0864, 0.0864, 0.0864, 0.0864, 0.0864],\n",
       "           [0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966],\n",
       "           [0.1069, 0.1069, 0.1069, 0.1069, 0.1069, 0.1069, 0.1069, 0.1069],\n",
       "           [0.1171, 0.1171, 0.1171, 0.1171, 0.1171, 0.1171, 0.1171, 0.1171],\n",
       "           [0.1274, 0.1274, 0.1274, 0.1274, 0.1274, 0.1274, 0.1274, 0.1274],\n",
       "           [0.1376, 0.1376, 0.1376, 0.1376, 0.1376, 0.1376, 0.1376, 0.1376],\n",
       "           [0.1478, 0.1478, 0.1478, 0.1478, 0.1478, 0.1478, 0.1478, 0.1478],\n",
       "           [0.1581, 0.1581, 0.1581, 0.1581, 0.1581, 0.1581, 0.1581, 0.1581]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 16, 8]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch version QKV product (To test if this is still correct after changes to the code.)\n",
    "# at some point we have to compare to the vlstm_fw_torch version.\n",
    "rs = qs @ ks.transpose(-1, -2) @ vs\n",
    "rs, rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
       "           [0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018],\n",
       "           [0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047],\n",
       "           [0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088],\n",
       "           [0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142],\n",
       "           [0.0209, 0.0209, 0.0209, 0.0209, 0.0209, 0.0209, 0.0209, 0.0209],\n",
       "           [0.0288, 0.0288, 0.0288, 0.0288, 0.0288, 0.0288, 0.0288, 0.0288],\n",
       "           [0.0381, 0.0381, 0.0381, 0.0381, 0.0381, 0.0381, 0.0381, 0.0381],\n",
       "           [0.0486, 0.0486, 0.0486, 0.0486, 0.0486, 0.0486, 0.0486, 0.0486],\n",
       "           [0.0604, 0.0604, 0.0604, 0.0604, 0.0604, 0.0604, 0.0604, 0.0604],\n",
       "           [0.0735, 0.0735, 0.0735, 0.0735, 0.0735, 0.0735, 0.0735, 0.0735],\n",
       "           [0.0878, 0.0878, 0.0878, 0.0878, 0.0878, 0.0878, 0.0878, 0.0878],\n",
       "           [0.1035, 0.1035, 0.1035, 0.1035, 0.1035, 0.1035, 0.1035, 0.1035],\n",
       "           [0.1204, 0.1204, 0.1204, 0.1204, 0.1204, 0.1204, 0.1204, 0.1204],\n",
       "           [0.1386, 0.1386, 0.1386, 0.1386, 0.1386, 0.1386, 0.1386, 0.1386],\n",
       "           [0.1581, 0.1581, 0.1581, 0.1581, 0.1581, 0.1581, 0.1581, 0.1581]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 16, 8]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_torch = (qs @ ks.transpose(-1, -2) * torch.tril(torch.ones((B, NH, S, S))).to(device=DEVICE, dtype=DTYPE)) @ vs\n",
    "rs_torch, rs_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 16, 1]), torch.Size([1, 1, 16]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "igs.shape, igs.squeeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - float32!\n",
      "B: 1, NH: 1, S: 16, DH: 8\n",
      "blocksxy: 1-2, threadsxy: 4-4, shared_mem in bytes: 5700\n",
      "In Kernel: gdim.x: 1, gdim.y: 2, gdim.z: 1, bdim.x: 4, bdim.y: 4\n",
      "In Kernel: QtileDim: 8, KVtileDim: 8, TblockDim:4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
       "           [0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018],\n",
       "           [0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047, 0.0047],\n",
       "           [0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088],\n",
       "           [0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142],\n",
       "           [0.0209, 0.0209, 0.0209, 0.0209, 0.0209, 0.0209, 0.0209, 0.0209],\n",
       "           [0.0288, 0.0288, 0.0288, 0.0288, 0.0288, 0.0288, 0.0288, 0.0288],\n",
       "           [0.0381, 0.0381, 0.0381, 0.0381, 0.0381, 0.0381, 0.0381, 0.0381],\n",
       "           [0.0486, 0.0486, 0.0486, 0.0486, 0.0486, 0.0486, 0.0486, 0.0486],\n",
       "           [0.0604, 0.0604, 0.0604, 0.0604, 0.0604, 0.0604, 0.0604, 0.0604],\n",
       "           [0.0735, 0.0735, 0.0735, 0.0735, 0.0735, 0.0735, 0.0735, 0.0735],\n",
       "           [0.0878, 0.0878, 0.0878, 0.0878, 0.0878, 0.0878, 0.0878, 0.0878],\n",
       "           [0.1035, 0.1035, 0.1035, 0.1035, 0.1035, 0.1035, 0.1035, 0.1035],\n",
       "           [0.1204, 0.1204, 0.1204, 0.1204, 0.1204, 0.1204, 0.1204, 0.1204],\n",
       "           [0.1386, 0.1386, 0.1386, 0.1386, 0.1386, 0.1386, 0.1386, 0.1386],\n",
       "           [0.1581, 0.1581, 0.1581, 0.1581, 0.1581, 0.1581, 0.1581, 0.1581]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 16, 8]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cuda kernel\n",
    "rs_cuda, matC = vlstm_fw_cuda(mat_Q=qs, mat_K=ks, mat_V=vs, igate_preact=igs.squeeze(-1), fgate_preact=fgs.squeeze(-1))\n",
    "rs_cuda, rs_cuda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 1.1176e-08,  1.1176e-08,  1.1176e-08,  1.1176e-08,  1.1176e-08,  1.1176e-08,  1.1176e-08,  1.1176e-08],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09],\n",
       "          [-7.4506e-09, -7.4506e-09, -7.4506e-09, -7.4506e-09, -7.4506e-09, -7.4506e-09, -7.4506e-09, -7.4506e-09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-1.4901e-08, -1.4901e-08, -1.4901e-08, -1.4901e-08, -1.4901e-08, -1.4901e-08, -1.4901e-08, -1.4901e-08]]]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_torch - rs_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 16, 16])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.6867,  2.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.3735,  1.6867,  3.0000,    -inf,    -inf,    -inf,    -inf,    -inf,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0602,  1.3735,  2.6867,  4.0000,    -inf,    -inf,    -inf,    -inf,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.2530,  1.0602,  2.3735,  3.6867,  5.0000,    -inf,    -inf,    -inf,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.5663,  0.7470,  2.0602,  3.3735,  4.6867,  6.0000,    -inf,    -inf,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.8796,  0.4337,  1.7470,  3.0602,  4.3735,  5.6867,  7.0000,    -inf,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.1928,  0.1204,  1.4337,  2.7470,  4.0602,  5.3735,  6.6867,  8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.5061, -0.1928,  1.1204,  2.4337,  3.7470,  5.0602,  6.3735,  7.6867,  9.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-1.8194, -0.5061,  0.8072,  2.1204,  3.4337,  4.7470,  6.0602,  7.3735,  8.6867, 10.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-2.1326, -0.8194,  0.4939,  1.8072,  3.1204,  4.4337,  5.7470,  7.0602,  8.3735,  9.6867, 11.0000,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-2.4459, -1.1326,  0.1806,  1.4939,  2.8072,  4.1204,  5.4337,  6.7470,  8.0602,  9.3735, 10.6867, 12.0000,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-2.7591, -1.4459, -0.1326,  1.1806,  2.4939,  3.8072,  5.1204,  6.4337,  7.7470,  9.0602, 10.3735, 11.6867, 13.0000,    -inf,    -inf,    -inf],\n",
       "          [-3.0724, -1.7591, -0.4459,  0.8674,  2.1806,  3.4939,  4.8072,  6.1204,  7.4337,  8.7470, 10.0602, 11.3735, 12.6867, 14.0000,    -inf,    -inf],\n",
       "          [-3.3857, -2.0724, -0.7591,  0.5541,  1.8674,  3.1806,  4.4939,  5.8072,  7.1204,  8.4337,  9.7470, 11.0602, 12.3735, 13.6867, 15.0000,    -inf],\n",
       "          [-3.6989, -2.3857, -1.0724,  0.2409,  1.5541,  2.8674,  4.1806,  5.4939,  6.8072,  8.1204,  9.4337, 10.7470, 12.0602, 13.3735, 14.6867, 16.0000]]]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.6867,  2.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.3735,  1.6867,  3.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.0602,  1.3735,  2.6867,  4.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.2530,  1.0602,  2.3735,  3.6867,  5.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.5663,  0.7470,  2.0602,  3.3735,  4.6867,  6.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.8796,  0.4337,  1.7470,  3.0602,  4.3735,  5.6867,  7.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-1.1928,  0.1204,  1.4337,  2.7470,  4.0602,  5.3735,  6.6867,  8.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-1.5061, -0.1928,  1.1204,  2.4337,  3.7470,  5.0602,  6.3735,  7.6867,  9.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-1.8194, -0.5061,  0.8072,  2.1204,  3.4337,  4.7470,  6.0602,  7.3735,  8.6867, 10.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-2.1326, -0.8194,  0.4939,  1.8072,  3.1204,  4.4337,  5.7470,  7.0602,  8.3735,  9.6867, 11.0000,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-2.4459, -1.1326,  0.1806,  1.4939,  2.8072,  4.1204,  5.4337,  6.7470,  8.0602,  9.3735, 10.6867, 12.0000,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-2.7591, -1.4459, -0.1326,  1.1806,  2.4939,  3.8072,  5.1204,  6.4337,  7.7470,  9.0602, 10.3735, 11.6867, 13.0000,    -inf,    -inf,    -inf],\n",
       "          [-3.0724, -1.7591, -0.4459,  0.8674,  2.1806,  3.4939,  4.8072,  6.1204,  7.4337,  8.7470, 10.0602, 11.3735, 12.6867, 14.0000,    -inf,    -inf],\n",
       "          [-3.3857, -2.0724, -0.7591,  0.5541,  1.8674,  3.1806,  4.4939,  5.8072,  7.1204,  8.4337,  9.7470, 11.0602, 12.3735, 13.6867, 15.0000,    -inf],\n",
       "          [-3.6989, -2.3857, -1.0724,  0.2409,  1.5541,  2.8674,  4.1806,  5.4939,  6.8072,  8.1204,  9.4337, 10.7470, 12.0602, 13.3735, 14.6867, 16.0000]]]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00,         nan,         nan,         nan,         nan,         nan,         nan,         nan,         inf,         inf,         inf,         inf,         inf,         inf,\n",
       "                   inf,         inf],\n",
       "          [ 5.9605e-08,  0.0000e+00,         nan,         nan,         nan,         nan,         nan,         nan,         inf,         inf,         inf,         inf,         inf,         inf,\n",
       "                   inf,         inf],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,         nan,         nan,         nan,         nan,         nan,         inf,         inf,         inf,         inf,         inf,         inf,\n",
       "                   inf,         inf],\n",
       "          [ 5.9605e-08,  1.1921e-07,  0.0000e+00,  0.0000e+00,         nan,         nan,         nan,         nan,         inf,         inf,         inf,         inf,         inf,         inf,\n",
       "                   inf,         inf],\n",
       "          [ 1.1921e-07,  2.3842e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,         nan,         nan,         nan,         inf,         inf,         inf,         inf,         inf,         inf,\n",
       "                   inf,         inf],\n",
       "          [ 1.1921e-07,  2.3842e-07,  2.3842e-07,  0.0000e+00,  4.7684e-07,  0.0000e+00,         nan,         nan,         inf,         inf,         inf,         inf,         inf,         inf,\n",
       "                   inf,         inf],\n",
       "          [ 0.0000e+00,  0.0000e+00,  1.1921e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,         nan,         inf,         inf,         inf,         inf,         inf,         inf,\n",
       "                   inf,         inf],\n",
       "          [ 0.0000e+00,  0.0000e+00,  1.1921e-07,  2.3842e-07,  0.0000e+00,  0.0000e+00,  4.7684e-07,  0.0000e+00,         inf,         inf,         inf,         inf,         inf,         inf,\n",
       "                   inf,         inf],\n",
       "          [ 0.0000e+00,  0.0000e+00,  1.1921e-07,  0.0000e+00,  2.3842e-07,  0.0000e+00,  0.0000e+00,  4.7684e-07,  0.0000e+00,         nan,         nan,         nan,         nan,         nan,\n",
       "                   nan,         nan],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  2.3842e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,         nan,         nan,         nan,         nan,\n",
       "                   nan,         nan],\n",
       "          [ 0.0000e+00,  0.0000e+00,  2.3842e-07,  0.0000e+00,  2.3842e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.5367e-07,  0.0000e+00,  0.0000e+00,         nan,         nan,         nan,\n",
       "                   nan,         nan],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  2.3842e-07,  0.0000e+00,  4.7684e-07,  4.7684e-07,  0.0000e+00,  0.0000e+00,  9.5367e-07,  0.0000e+00,  0.0000e+00,         nan,         nan,\n",
       "                   nan,         nan],\n",
       "          [ 0.0000e+00,  0.0000e+00,  2.3842e-07,  0.0000e+00,  2.3842e-07,  0.0000e+00,  4.7684e-07,  4.7684e-07,  0.0000e+00,  0.0000e+00,  9.5367e-07,  0.0000e+00,  0.0000e+00,         nan,\n",
       "                   nan,         nan],\n",
       "          [ 0.0000e+00, -2.3842e-07,  0.0000e+00, -2.3842e-07,  0.0000e+00, -2.3842e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -9.5367e-07,  0.0000e+00,\n",
       "                   nan,         nan],\n",
       "          [ 0.0000e+00, -4.7684e-07, -2.3842e-07, -4.7684e-07, -2.3842e-07, -4.7684e-07, -4.7684e-07,  0.0000e+00,  0.0000e+00, -9.5367e-07, -9.5367e-07,  0.0000e+00,  0.0000e+00, -9.5367e-07,\n",
       "            0.0000e+00,         nan],\n",
       "          [ 0.0000e+00, -4.7684e-07, -4.7684e-07, -7.1526e-07, -4.7684e-07, -7.1526e-07, -4.7684e-07, -9.5367e-07, -4.7684e-07,  0.0000e+00, -9.5367e-07, -9.5367e-07,  0.0000e+00,  0.0000e+00,\n",
       "           -9.5367e-07,  0.0000e+00]]]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matC - matD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstmpt21cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
