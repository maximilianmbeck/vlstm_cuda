{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch \n",
    "\n",
    "from vlstm_Dmat import vlstm_fw_Dtildemat, vlstm_fwbw_Dtildemat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLSTM Dmat forward backward implementation\n",
    "\n",
    "In this notebook we implement the forward and backward pass of the D (decay matrix) construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = torch.float32 \n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "S = 5\n",
    "NH = 4\n",
    "DH = 6\n",
    "EPS = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "igate_preacts = torch.randn((B, NH, S, 1), dtype=DTYPE, device=DEVICE)\n",
    "fgate_preacts = torch.randn((B, NH, S, 1), dtype=DTYPE, device=DEVICE)\n",
    "# fgate_preacts = 0.1*torch.ones((B, NH, S, 1), dtype=DTYPE, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward / Backward impl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 1.6217],\n",
       "           [-1.1963],\n",
       "           [ 1.2651],\n",
       "           [-1.0511],\n",
       "           [ 0.8248]],\n",
       " \n",
       "          [[ 1.7687],\n",
       "           [-0.2273],\n",
       "           [-0.3192],\n",
       "           [-0.1224],\n",
       "           [ 0.5716]],\n",
       " \n",
       "          [[ 1.7480],\n",
       "           [ 1.5525],\n",
       "           [-0.0406],\n",
       "           [ 1.0113],\n",
       "           [ 1.5455]],\n",
       " \n",
       "          [[ 2.1709],\n",
       "           [-0.0638],\n",
       "           [ 0.3106],\n",
       "           [-0.2125],\n",
       "           [ 0.0994]]],\n",
       " \n",
       " \n",
       "         [[[ 0.7163],\n",
       "           [-0.5652],\n",
       "           [ 0.1756],\n",
       "           [-0.3835],\n",
       "           [ 0.2200]],\n",
       " \n",
       "          [[-0.3686],\n",
       "           [-1.3597],\n",
       "           [ 0.2573],\n",
       "           [ 0.0560],\n",
       "           [ 0.3304]],\n",
       " \n",
       "          [[-0.1345],\n",
       "           [ 0.9675],\n",
       "           [ 0.8923],\n",
       "           [ 0.5020],\n",
       "           [ 0.1634]],\n",
       " \n",
       "          [[-0.3290],\n",
       "           [ 0.5576],\n",
       "           [ 0.7773],\n",
       "           [-1.5742],\n",
       "           [ 0.2267]]]], device='cuda:0', requires_grad=True),\n",
       " tensor([[[[ 0.0124],\n",
       "           [-1.0455],\n",
       "           [-0.0735],\n",
       "           [ 0.0832],\n",
       "           [-0.5680]],\n",
       " \n",
       "          [[ 1.9620],\n",
       "           [-0.3731],\n",
       "           [ 0.5843],\n",
       "           [-0.4221],\n",
       "           [ 1.5006]],\n",
       " \n",
       "          [[ 0.5582],\n",
       "           [ 1.2511],\n",
       "           [ 0.6232],\n",
       "           [ 0.7075],\n",
       "           [ 1.3565]],\n",
       " \n",
       "          [[ 0.5134],\n",
       "           [ 1.9529],\n",
       "           [-1.8601],\n",
       "           [-0.8365],\n",
       "           [ 0.6902]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5590],\n",
       "           [ 1.5131],\n",
       "           [ 0.3081],\n",
       "           [ 0.3594],\n",
       "           [ 0.7247]],\n",
       " \n",
       "          [[-0.5370],\n",
       "           [-1.3131],\n",
       "           [-1.0852],\n",
       "           [ 1.7549],\n",
       "           [-0.2841]],\n",
       " \n",
       "          [[ 0.3271],\n",
       "           [-0.4896],\n",
       "           [ 0.5642],\n",
       "           [-0.4275],\n",
       "           [-1.2305]],\n",
       " \n",
       "          [[ 1.1763],\n",
       "           [-0.0798],\n",
       "           [ 1.8283],\n",
       "           [ 0.3819],\n",
       "           [ 0.5638]]]], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "igate_preacts_pt = igate_preacts.clone().detach().requires_grad_(True)\n",
    "fgate_preacts_pt = fgate_preacts.clone().detach().requires_grad_(True)\n",
    "igate_preacts_pt, fgate_preacts_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 1.6217,    -inf,    -inf,    -inf,    -inf],\n",
       "           [ 0.2749, -1.1963,    -inf,    -inf,    -inf],\n",
       "           [-0.4557, -1.9268,  1.2651,    -inf,    -inf],\n",
       "           [-1.1081, -2.5793,  0.6127, -1.0511,    -inf],\n",
       "           [-2.1250, -3.5962, -0.4043, -2.0680,  0.8248]],\n",
       " \n",
       "          [[ 1.7687,    -inf,    -inf,    -inf,    -inf],\n",
       "           [ 0.8717, -0.2273,    -inf,    -inf,    -inf],\n",
       "           [ 0.4286, -0.6704, -0.3192,    -inf,    -inf],\n",
       "           [-0.4977, -1.5967, -1.2455, -0.1224,    -inf],\n",
       "           [-0.6990, -1.7980, -1.4468, -0.3237,  0.5716]],\n",
       " \n",
       "          [[ 1.7480,    -inf,    -inf,    -inf,    -inf],\n",
       "           [ 1.4963,  1.5525,    -inf,    -inf,    -inf],\n",
       "           [ 1.0670,  1.1231, -0.0406,    -inf,    -inf],\n",
       "           [ 0.6663,  0.7224, -0.4413,  1.0113,    -inf],\n",
       "           [ 0.4371,  0.4932, -0.6705,  0.7821,  1.5455]],\n",
       " \n",
       "          [[ 2.1709,    -inf,    -inf,    -inf,    -inf],\n",
       "           [ 2.0383, -0.0638,    -inf,    -inf,    -inf],\n",
       "           [ 0.0335, -2.0685,  0.3106,    -inf,    -inf],\n",
       "           [-1.1629, -3.2650, -0.8858, -0.2125,    -inf],\n",
       "           [-1.5694, -3.6714, -1.2923, -0.6189,  0.0994]]],\n",
       " \n",
       " \n",
       "         [[[ 0.7163,    -inf,    -inf,    -inf,    -inf],\n",
       "           [ 0.5172, -0.5652,    -inf,    -inf,    -inf],\n",
       "           [-0.0337, -1.1161,  0.1756,    -inf,    -inf],\n",
       "           [-0.5632, -1.6456, -0.3539, -0.3835,    -inf],\n",
       "           [-0.9583, -2.0407, -0.7490, -0.7786,  0.2200]],\n",
       " \n",
       "          [[-0.3686,    -inf,    -inf,    -inf,    -inf],\n",
       "           [-1.9199, -1.3597,    -inf,    -inf,    -inf],\n",
       "           [-3.2962, -2.7360,  0.2573,    -inf,    -inf],\n",
       "           [-3.4557, -2.8955,  0.0978,  0.0560,    -inf],\n",
       "           [-4.3009, -3.7407, -0.7475, -0.7892,  0.3304]],\n",
       " \n",
       "          [[-0.1345,    -inf,    -inf,    -inf,    -inf],\n",
       "           [-1.1021,  0.9675,    -inf,    -inf,    -inf],\n",
       "           [-1.5525,  0.5172,  0.8923,    -inf,    -inf],\n",
       "           [-2.4820, -0.4124, -0.0373,  0.5020,    -inf],\n",
       "           [-3.9688, -1.8991, -1.5240, -0.9847,  0.1634]],\n",
       " \n",
       "          [[-0.3290,    -inf,    -inf,    -inf,    -inf],\n",
       "           [-1.0629,  0.5576,    -inf,    -inf,    -inf],\n",
       "           [-1.2119,  0.4086,  0.7773,    -inf,    -inf],\n",
       "           [-1.7322, -0.1117,  0.2569, -1.5742,    -inf],\n",
       "           [-2.1827, -0.5622, -0.1935, -2.0247,  0.2267]]]], device='cuda:0',\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([2, 4, 5, 5]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmat_pt = vlstm_fw_Dtildemat(igate_preacts_pt, fgate_preacts_pt)\n",
    "dmat_pt, dmat_pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-inf, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmat_pt.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_pt.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[5.],\n",
       "           [5.],\n",
       "           [5.],\n",
       "           [5.],\n",
       "           [5.]],\n",
       " \n",
       "          [[5.],\n",
       "           [5.],\n",
       "           [5.],\n",
       "           [5.],\n",
       "           [5.]],\n",
       " \n",
       "          [[5.],\n",
       "           [5.],\n",
       "           [5.],\n",
       "           [5.],\n",
       "           [5.]],\n",
       " \n",
       "          [[5.],\n",
       "           [5.],\n",
       "           [5.],\n",
       "           [5.],\n",
       "           [5.]]],\n",
       " \n",
       " \n",
       "         [[[5.],\n",
       "           [5.],\n",
       "           [5.],\n",
       "           [5.],\n",
       "           [5.]],\n",
       " \n",
       "          [[5.],\n",
       "           [5.],\n",
       "           [5.],\n",
       "           [5.],\n",
       "           [5.]],\n",
       " \n",
       "          [[5.],\n",
       "           [5.],\n",
       "           [5.],\n",
       "           [5.],\n",
       "           [5.]],\n",
       " \n",
       "          [[5.],\n",
       "           [5.],\n",
       "           [5.],\n",
       "           [5.],\n",
       "           [5.]]]], device='cuda:0'),\n",
       " tensor([[[[0.0000],\n",
       "           [2.9597],\n",
       "           [3.1102],\n",
       "           [2.8753],\n",
       "           [2.5532]],\n",
       " \n",
       "          [[0.0000],\n",
       "           [2.3688],\n",
       "           [2.1476],\n",
       "           [3.6239],\n",
       "           [0.7294]],\n",
       " \n",
       "          [[0.0000],\n",
       "           [0.8900],\n",
       "           [2.0944],\n",
       "           [1.9809],\n",
       "           [0.8192]],\n",
       " \n",
       "          [[0.0000],\n",
       "           [0.4970],\n",
       "           [5.1919],\n",
       "           [4.1863],\n",
       "           [1.3360]]],\n",
       " \n",
       " \n",
       "         [[[0.0000],\n",
       "           [0.7219],\n",
       "           [2.5414],\n",
       "           [2.4667],\n",
       "           [1.3054]],\n",
       " \n",
       "          [[0.0000],\n",
       "           [3.1521],\n",
       "           [4.4849],\n",
       "           [0.8846],\n",
       "           [2.2822]],\n",
       " \n",
       "          [[0.0000],\n",
       "           [2.4800],\n",
       "           [2.1754],\n",
       "           [3.6317],\n",
       "           [3.0956]],\n",
       " \n",
       "          [[0.0000],\n",
       "           [2.0798],\n",
       "           [0.8306],\n",
       "           [2.4341],\n",
       "           [1.4506]]]], device='cuda:0'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "igate_preacts_pt.grad, fgate_preacts_pt.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 1.6217],\n",
       "           [-1.1963],\n",
       "           [ 1.2651],\n",
       "           [-1.0511],\n",
       "           [ 0.8248]],\n",
       " \n",
       "          [[ 1.7687],\n",
       "           [-0.2273],\n",
       "           [-0.3192],\n",
       "           [-0.1224],\n",
       "           [ 0.5716]],\n",
       " \n",
       "          [[ 1.7480],\n",
       "           [ 1.5525],\n",
       "           [-0.0406],\n",
       "           [ 1.0113],\n",
       "           [ 1.5455]],\n",
       " \n",
       "          [[ 2.1709],\n",
       "           [-0.0638],\n",
       "           [ 0.3106],\n",
       "           [-0.2125],\n",
       "           [ 0.0994]]],\n",
       " \n",
       " \n",
       "         [[[ 0.7163],\n",
       "           [-0.5652],\n",
       "           [ 0.1756],\n",
       "           [-0.3835],\n",
       "           [ 0.2200]],\n",
       " \n",
       "          [[-0.3686],\n",
       "           [-1.3597],\n",
       "           [ 0.2573],\n",
       "           [ 0.0560],\n",
       "           [ 0.3304]],\n",
       " \n",
       "          [[-0.1345],\n",
       "           [ 0.9675],\n",
       "           [ 0.8923],\n",
       "           [ 0.5020],\n",
       "           [ 0.1634]],\n",
       " \n",
       "          [[-0.3290],\n",
       "           [ 0.5576],\n",
       "           [ 0.7773],\n",
       "           [-1.5742],\n",
       "           [ 0.2267]]]], device='cuda:0', requires_grad=True),\n",
       " torch.Size([2, 4, 5, 1]),\n",
       " tensor([[[[ 0.0124],\n",
       "           [-1.0455],\n",
       "           [-0.0735],\n",
       "           [ 0.0832],\n",
       "           [-0.5680]],\n",
       " \n",
       "          [[ 1.9620],\n",
       "           [-0.3731],\n",
       "           [ 0.5843],\n",
       "           [-0.4221],\n",
       "           [ 1.5006]],\n",
       " \n",
       "          [[ 0.5582],\n",
       "           [ 1.2511],\n",
       "           [ 0.6232],\n",
       "           [ 0.7075],\n",
       "           [ 1.3565]],\n",
       " \n",
       "          [[ 0.5134],\n",
       "           [ 1.9529],\n",
       "           [-1.8601],\n",
       "           [-0.8365],\n",
       "           [ 0.6902]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5590],\n",
       "           [ 1.5131],\n",
       "           [ 0.3081],\n",
       "           [ 0.3594],\n",
       "           [ 0.7247]],\n",
       " \n",
       "          [[-0.5370],\n",
       "           [-1.3131],\n",
       "           [-1.0852],\n",
       "           [ 1.7549],\n",
       "           [-0.2841]],\n",
       " \n",
       "          [[ 0.3271],\n",
       "           [-0.4896],\n",
       "           [ 0.5642],\n",
       "           [-0.4275],\n",
       "           [-1.2305]],\n",
       " \n",
       "          [[ 1.1763],\n",
       "           [-0.0798],\n",
       "           [ 1.8283],\n",
       "           [ 0.3819],\n",
       "           [ 0.5638]]]], device='cuda:0', requires_grad=True),\n",
       " torch.Size([2, 4, 5, 1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "igate_preacts_obw = igate_preacts.clone().detach().requires_grad_(True)\n",
    "fgate_preacts_obw = fgate_preacts.clone().detach().requires_grad_(True)\n",
    "igate_preacts_obw, igate_preacts_obw.shape, fgate_preacts_obw, fgate_preacts_obw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 1.6217,    -inf,    -inf,    -inf,    -inf],\n",
       "           [ 0.2749, -1.1963,    -inf,    -inf,    -inf],\n",
       "           [-0.4557, -1.9268,  1.2651,    -inf,    -inf],\n",
       "           [-1.1081, -2.5793,  0.6127, -1.0511,    -inf],\n",
       "           [-2.1250, -3.5962, -0.4043, -2.0680,  0.8248]],\n",
       " \n",
       "          [[ 1.7687,    -inf,    -inf,    -inf,    -inf],\n",
       "           [ 0.8717, -0.2273,    -inf,    -inf,    -inf],\n",
       "           [ 0.4286, -0.6704, -0.3192,    -inf,    -inf],\n",
       "           [-0.4977, -1.5967, -1.2455, -0.1224,    -inf],\n",
       "           [-0.6990, -1.7980, -1.4468, -0.3237,  0.5716]],\n",
       " \n",
       "          [[ 1.7480,    -inf,    -inf,    -inf,    -inf],\n",
       "           [ 1.4963,  1.5525,    -inf,    -inf,    -inf],\n",
       "           [ 1.0670,  1.1231, -0.0406,    -inf,    -inf],\n",
       "           [ 0.6663,  0.7224, -0.4413,  1.0113,    -inf],\n",
       "           [ 0.4371,  0.4932, -0.6705,  0.7821,  1.5455]],\n",
       " \n",
       "          [[ 2.1709,    -inf,    -inf,    -inf,    -inf],\n",
       "           [ 2.0383, -0.0638,    -inf,    -inf,    -inf],\n",
       "           [ 0.0335, -2.0685,  0.3106,    -inf,    -inf],\n",
       "           [-1.1629, -3.2650, -0.8858, -0.2125,    -inf],\n",
       "           [-1.5694, -3.6714, -1.2923, -0.6189,  0.0994]]],\n",
       " \n",
       " \n",
       "         [[[ 0.7163,    -inf,    -inf,    -inf,    -inf],\n",
       "           [ 0.5172, -0.5652,    -inf,    -inf,    -inf],\n",
       "           [-0.0337, -1.1161,  0.1756,    -inf,    -inf],\n",
       "           [-0.5632, -1.6456, -0.3539, -0.3835,    -inf],\n",
       "           [-0.9583, -2.0407, -0.7490, -0.7786,  0.2200]],\n",
       " \n",
       "          [[-0.3686,    -inf,    -inf,    -inf,    -inf],\n",
       "           [-1.9199, -1.3597,    -inf,    -inf,    -inf],\n",
       "           [-3.2962, -2.7360,  0.2573,    -inf,    -inf],\n",
       "           [-3.4557, -2.8955,  0.0978,  0.0560,    -inf],\n",
       "           [-4.3009, -3.7407, -0.7475, -0.7892,  0.3304]],\n",
       " \n",
       "          [[-0.1345,    -inf,    -inf,    -inf,    -inf],\n",
       "           [-1.1021,  0.9675,    -inf,    -inf,    -inf],\n",
       "           [-1.5525,  0.5172,  0.8923,    -inf,    -inf],\n",
       "           [-2.4820, -0.4124, -0.0373,  0.5020,    -inf],\n",
       "           [-3.9688, -1.8991, -1.5240, -0.9847,  0.1634]],\n",
       " \n",
       "          [[-0.3290,    -inf,    -inf,    -inf,    -inf],\n",
       "           [-1.0629,  0.5576,    -inf,    -inf,    -inf],\n",
       "           [-1.2119,  0.4086,  0.7773,    -inf,    -inf],\n",
       "           [-1.7322, -0.1117,  0.2569, -1.5742,    -inf],\n",
       "           [-2.1827, -0.5622, -0.1935, -2.0247,  0.2267]]]], device='cuda:0',\n",
       "        grad_fn=<vLSTMFwBwDtildematBackward>),\n",
       " torch.Size([2, 4, 5, 5]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmat_obw = vlstm_fwbw_Dtildemat(igate_preacts_obw, fgate_preacts_obw)\n",
    "dmat_obw, dmat_obw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., nan, nan, nan, nan],\n",
       "          [0., 0., nan, nan, nan],\n",
       "          [0., 0., 0., nan, nan],\n",
       "          [0., 0., 0., 0., nan],\n",
       "          [0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., nan, nan, nan, nan],\n",
       "          [0., 0., nan, nan, nan],\n",
       "          [0., 0., 0., nan, nan],\n",
       "          [0., 0., 0., 0., nan],\n",
       "          [0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., nan, nan, nan, nan],\n",
       "          [0., 0., nan, nan, nan],\n",
       "          [0., 0., 0., nan, nan],\n",
       "          [0., 0., 0., 0., nan],\n",
       "          [0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., nan, nan, nan, nan],\n",
       "          [0., 0., nan, nan, nan],\n",
       "          [0., 0., 0., nan, nan],\n",
       "          [0., 0., 0., 0., nan],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., nan, nan, nan, nan],\n",
       "          [0., 0., nan, nan, nan],\n",
       "          [0., 0., 0., nan, nan],\n",
       "          [0., 0., 0., 0., nan],\n",
       "          [0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., nan, nan, nan, nan],\n",
       "          [0., 0., nan, nan, nan],\n",
       "          [0., 0., 0., nan, nan],\n",
       "          [0., 0., 0., 0., nan],\n",
       "          [0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., nan, nan, nan, nan],\n",
       "          [0., 0., nan, nan, nan],\n",
       "          [0., 0., 0., nan, nan],\n",
       "          [0., 0., 0., 0., nan],\n",
       "          [0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., nan, nan, nan, nan],\n",
       "          [0., 0., nan, nan, nan],\n",
       "          [0., 0., 0., nan, nan],\n",
       "          [0., 0., 0., 0., nan],\n",
       "          [0., 0., 0., 0., 0.]]]], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmat_obw - dmat_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmat_obw.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# igate_preacts_obw.grad, fgate_preacts_obw.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward match: True\n",
      "igate preact match: True\n",
      "fgate pract match: True\n"
     ]
    }
   ],
   "source": [
    "atol = 1e-6\n",
    "rtol = 1e-6\n",
    "print(f\"Forward match: {torch.allclose(dmat_obw, dmat_pt, atol=atol, rtol=rtol)}\")\n",
    "print(f\"igate preact match: {torch.allclose(igate_preacts_obw.grad, igate_preacts_pt.grad, atol=atol, rtol=rtol)}\")\n",
    "print(f\"fgate pract match: {torch.allclose(fgate_preacts_obw.grad, fgate_preacts_pt.grad, atol=atol, rtol=rtol)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]],\n",
       " \n",
       "          [[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]],\n",
       " \n",
       "          [[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]],\n",
       " \n",
       "          [[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]]],\n",
       " \n",
       " \n",
       "         [[[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]],\n",
       " \n",
       "          [[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]],\n",
       " \n",
       "          [[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]],\n",
       " \n",
       "          [[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]]]], device='cuda:0'),\n",
       " tensor([[[[ 0.0000e+00],\n",
       "           [ 0.0000e+00],\n",
       "           [ 0.0000e+00],\n",
       "           [-2.3842e-07],\n",
       "           [ 2.3842e-07]],\n",
       " \n",
       "          [[ 0.0000e+00],\n",
       "           [-2.3842e-07],\n",
       "           [ 0.0000e+00],\n",
       "           [ 0.0000e+00],\n",
       "           [ 0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00],\n",
       "           [ 5.9605e-08],\n",
       "           [-2.3842e-07],\n",
       "           [ 0.0000e+00],\n",
       "           [ 1.7881e-07]],\n",
       " \n",
       "          [[ 0.0000e+00],\n",
       "           [ 2.9802e-08],\n",
       "           [ 0.0000e+00],\n",
       "           [ 0.0000e+00],\n",
       "           [-1.1921e-07]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0000e+00],\n",
       "           [ 5.9605e-08],\n",
       "           [ 2.3842e-07],\n",
       "           [ 2.3842e-07],\n",
       "           [ 0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00],\n",
       "           [-2.3842e-07],\n",
       "           [ 0.0000e+00],\n",
       "           [ 1.1921e-07],\n",
       "           [ 0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00],\n",
       "           [ 0.0000e+00],\n",
       "           [ 2.3842e-07],\n",
       "           [ 2.3842e-07],\n",
       "           [ 2.3842e-07]],\n",
       " \n",
       "          [[ 0.0000e+00],\n",
       "           [ 2.3842e-07],\n",
       "           [ 2.3842e-07],\n",
       "           [ 0.0000e+00],\n",
       "           [ 2.3842e-07]]]], device='cuda:0'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "igate_preacts_obw.grad - igate_preacts_pt.grad, fgate_preacts_obw.grad - fgate_preacts_pt.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_grad_dtilde_mat = torch.tril(torch.ones((S, S), dtype=DTYPE, device=DEVICE), diagonal=-1).unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0., 0., 0.],\n",
      "          [1., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0.],\n",
      "          [1., 1., 1., 1., 0.]]]], device='cuda:0') torch.Size([1, 1, 5, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 4, -1]' is invalid for input of size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, S):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k):\n\u001b[0;32m----> 7\u001b[0m         delta_fbar[:, :, k, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmasked_grad_dtilde_mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m      8\u001b[0m delta_fbar\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 4, -1]' is invalid for input of size 5"
     ]
    }
   ],
   "source": [
    "delta_fbar = torch.zeros((B, NH, S, 1), device=DEVICE, dtype=DTYPE)\n",
    "print(masked_grad_dtilde_mat, masked_grad_dtilde_mat.shape)\n",
    "\n",
    "# first forget gate index (k=0) does not get a gradient (since it is not used in the forward pass)\n",
    "for k in range(1, S):\n",
    "    for j in range(k):\n",
    "        delta_fbar[:, :, k, 0] += masked_grad_dtilde_mat[:, :, :, j].view(B, NH, -1).sum()\n",
    "delta_fbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_grad_dtilde_mat[:, :, :, j].view(B, NH, -1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "          [  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
       "          [  1.,   2.,   2.,  ...,   2.,   2.,   2.],\n",
       "          ...,\n",
       "          [  1.,   2.,   3.,  ..., 125., 125., 125.],\n",
       "          [  1.,   2.,   3.,  ..., 126., 126., 126.],\n",
       "          [  1.,   2.,   3.,  ..., 126., 127., 127.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmat_cs = masked_grad_dtilde_mat.cumsum(dim=-1)\n",
    "dmat_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
       "            1.0000e+00, 1.0000e+00],\n",
       "           [2.0000e+00, 3.0000e+00, 3.0000e+00,  ..., 3.0000e+00,\n",
       "            3.0000e+00, 3.0000e+00],\n",
       "           ...,\n",
       "           [1.2500e+02, 2.4900e+02, 3.7200e+02,  ..., 7.8750e+03,\n",
       "            7.8750e+03, 7.8750e+03],\n",
       "           [1.2600e+02, 2.5100e+02, 3.7500e+02,  ..., 8.0010e+03,\n",
       "            8.0010e+03, 8.0010e+03],\n",
       "           [1.2700e+02, 2.5300e+02, 3.7800e+02,  ..., 8.1270e+03,\n",
       "            8.1280e+03, 8.1280e+03]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 128, 128]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = dmat_cs.cumsum(dim=-2)\n",
    "res, res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 127])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:, :, -1, :-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstmpt21cu118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
