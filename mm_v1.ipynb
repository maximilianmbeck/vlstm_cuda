{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "sys.path.append(\".\")\n",
    "# os.environ[\"MAX_JOBS\"] = \"100\"\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCLUDE: ['/home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include', '/home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include/torch/csrc/api/include', '/home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include/TH', '/home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include/THC', '/home/max/miniconda3/envs/xlstmpt21cu118/include']\n",
      "/home/max/myrepos/vlstm_cuda/src\n",
      "/home/max/cpplibs/libtorch/lib:/usr/local/cuda-12.3/lib64:\n",
      "/home/max/miniconda3/envs/xlstmpt21cu118/lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/max/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /home/max/.cache/torch_extensions/py311_cu118/mm_v1...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/max/.cache/torch_extensions/py311_cu118/mm_v1/build.ninja...\n",
      "Building extension module mm_v1...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] /home/max/miniconda3/envs/xlstmpt21cu118/bin/x86_64-conda-linux-gnu-c++ -MMD -MF interface.o.d -DTORCH_EXTENSION_NAME=mm_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include -isystem /home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include/TH -isystem /home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include/THC -isystem /home/max/miniconda3/envs/xlstmpt21cu118/include -isystem /home/max/miniconda3/envs/xlstmpt21cu118/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/max/myrepos/vlstm_cuda/src/mm_v1/interface.cc -o interface.o \n",
      "[2/3] /home/max/miniconda3/envs/xlstmpt21cu118/bin/nvcc  -ccbin /home/max/miniconda3/envs/xlstmpt21cu118/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=mm_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include -isystem /home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include/TH -isystem /home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/include/THC -isystem /home/max/miniconda3/envs/xlstmpt21cu118/include -isystem /home/max/miniconda3/envs/xlstmpt21cu118/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -gencode arch=compute_80,code=compute_80 -res-usage --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=13407807929942597099574024998205846127479365820592393377723561443721764030073546976801874298166903427690031858186486050853753882811946569946433649006084096 -std=c++17 -c /home/max/myrepos/vlstm_cuda/src/mm_v1/kernels.cu -o kernels.cuda.o \n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops.cuh(255): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops.cuh:255 CUDART_VERSION: 11080, arch: 800\"\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops.cuh(260): remark #20200-D: #pragma message: \"INCLUDING FP16\"\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops_fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops_fp16.cuh:29 CUDART_VERSION with FP16: 11080, CUDA_ARCH: 800\"\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops_2fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops_2fp16.cuh:29 CUDART_VERSION with FP16: 11080, CUDA_ARCH: 800\"\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops.cuh(273): remark #20200-D: #pragma message: \"INCLUDING BF16\"\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops_bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops_bf16.cuh:29 CUDART_VERSION with BF16: 11080, CUDA_ARCH: 800\"\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops_2bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops_2bf16.cuh:29 CUDART_VERSION with FP16: 11080, CUDA_ARCH: 800\"\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops.cuh(255): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops.cuh:255 CUDART_VERSION: 11080, arch: 890\"\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops.cuh(260): remark #20200-D: #pragma message: \"INCLUDING FP16\"\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops_fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops_fp16.cuh:29 CUDART_VERSION with FP16: 11080, CUDA_ARCH: 890\"\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops_2fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops_2fp16.cuh:29 CUDART_VERSION with FP16: 11080, CUDA_ARCH: 890\"\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops.cuh(273): remark #20200-D: #pragma message: \"INCLUDING BF16\"\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops_bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops_bf16.cuh:29 CUDART_VERSION with BF16: 11080, CUDA_ARCH: 890\"\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops_2bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops_2bf16.cuh:29 CUDART_VERSION with FP16: 11080, CUDA_ARCH: 890\"\n",
      "\n",
      "ptxas info    : 236 bytes gmem\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels10mmkernelv1I6__halfLi4EEEvPT_S4_S4_iii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels10mmkernelv1I6__halfLi4EEEvPT_S4_S4_iii\n",
      "    48 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 64 bytes smem, 388 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels10mmkernelv1I13__nv_bfloat16Li4EEEvPT_S4_S4_iii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels10mmkernelv1I13__nv_bfloat16Li4EEEvPT_S4_S4_iii\n",
      "    48 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 64 bytes smem, 388 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels10copykernelI6__halfEEvPT_S4_ii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels10copykernelI6__halfEEvPT_S4_ii\n",
      "    16 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 24 registers, 376 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels10copykernelI13__nv_bfloat16EEvPT_S4_ii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels10copykernelI13__nv_bfloat16EEvPT_S4_ii\n",
      "    16 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 24 registers, 376 bytes cmem[0]\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops.cuh:255:130: note: '#pragma message: /home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops.cuh:255 CUDART_VERSION: 11080, arch: __CUDA_ARCH__'\n",
      "  255 | #pragma message(AT \" CUDART_VERSION: \" TOSTRING(                               \\\n",
      "      |                                                                                                                                  ^\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops.cuh:267:90: note: '#pragma message: SKIPPING FP16, because of CUDART_VERSION: 11080, arch: __CUDA_ARCH__'\n",
      "  267 | #pragma message(\"SKIPPING FP16, because of CUDART_VERSION: \" TOSTRING(         \\\n",
      "      |                                                                                          ^\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v1/../util/inline_ops.cuh:280:90: note: '#pragma message: SKIPPING BF16, because of CUDART_VERSION: 11080, arch: __CUDA_ARCH__'\n",
      "  280 | #pragma message(\"SKIPPING BF16, because of CUDART_VERSION: \" TOSTRING(         \\\n",
      "      |                                                                                          ^\n",
      "[3/3] /home/max/miniconda3/envs/xlstmpt21cu118/bin/x86_64-conda-linux-gnu-c++ interface.o kernels.cuda.o -shared -L/home/max/miniconda3/envs/xlstmpt21cu118/lib -lcublas -L/home/max/miniconda3/envs/xlstmpt21cu118/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/home/max/miniconda3/envs/xlstmpt21cu118/lib -lcudart -o mm_v1.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module mm_v1...\n"
     ]
    }
   ],
   "source": [
    "from src.mm_v1.interface import testkernel, copykernel, mmkernelv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "DTYPE = torch.bfloat16\n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "S = 8 # sequence length\n",
    "DH = 8 # hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 8]),\n",
       " torch.Size([8, 8]),\n",
       " tensor([[ 0.0000,  0.1001,  0.2002,  0.3008,  0.4004,  0.5000,  0.6016,  0.6992],\n",
       "         [ 0.8008,  0.8984,  1.0000,  1.1016,  1.2031,  1.2969,  1.3984,  1.5000],\n",
       "         [ 1.6016,  1.7031,  1.7969,  1.8984,  2.0000,  2.0938,  2.2031,  2.2969],\n",
       "         [ 2.4062,  2.5000,  2.5938,  2.7031,  2.7969,  2.9062,  3.0000,  3.0938],\n",
       "         [ 3.2031,  3.2969,  3.4062,  3.5000,  3.5938,  3.7031,  3.7969,  3.9062],\n",
       "         [ 4.0000,  4.0938,  4.1875,  4.3125,  4.4062,  4.5000,  4.5938,  4.6875],\n",
       "         [ 4.8125,  4.9062,  5.0000,  5.0938,  5.1875,  5.3125,  5.4062,  5.5000],\n",
       "         [ 5.5938,  5.6875,  5.8125,  5.9062,  6.0000,  6.0938,  6.1875,  6.3125],\n",
       "         [ 6.4062,  6.5000,  6.5938,  6.6875,  6.8125,  6.9062,  7.0000,  7.0938],\n",
       "         [ 7.1875,  7.3125,  7.4062,  7.5000,  7.5938,  7.6875,  7.8125,  7.9062],\n",
       "         [ 8.0000,  8.1250,  8.1875,  8.3125,  8.3750,  8.5000,  8.6250,  8.6875],\n",
       "         [ 8.8125,  8.8750,  9.0000,  9.1250,  9.1875,  9.3125,  9.3750,  9.5000],\n",
       "         [ 9.6250,  9.6875,  9.8125,  9.8750, 10.0000, 10.1250, 10.1875, 10.3125],\n",
       "         [10.3750, 10.5000, 10.6250, 10.6875, 10.8125, 10.8750, 11.0000, 11.1250],\n",
       "         [11.1875, 11.3125, 11.3750, 11.5000, 11.6250, 11.6875, 11.8125, 11.8750],\n",
       "         [12.0000, 12.1250, 12.1875, 12.3125, 12.3750, 12.5000, 12.6250, 12.6875]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor([ 2.7969,  9.1875, 15.6250, 22.0000, 28.3750, 34.7500, 41.2500, 47.5000,\n",
       "         54.0000, 60.5000, 67.0000, 73.0000, 79.5000, 86.0000, 92.5000, 99.0000],\n",
       "        device='cuda:0', dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matA = torch.arange((2*S * DH), device=DEVICE, dtype=DTYPE).reshape((2*S, DH)) / 10.\n",
    "matB = torch.ones((DH, S), device=DEVICE, dtype=DTYPE)\n",
    "matA.shape, matB.shape, matA, matA.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.7969,  2.7969,  2.7969,  2.7969,  2.7969,  2.7969,  2.7969,  2.7969],\n",
       "         [ 9.1875,  9.1875,  9.1875,  9.1875,  9.1875,  9.1875,  9.1875,  9.1875],\n",
       "         [15.6250, 15.6250, 15.6250, 15.6250, 15.6250, 15.6250, 15.6250, 15.6250],\n",
       "         [22.0000, 22.0000, 22.0000, 22.0000, 22.0000, 22.0000, 22.0000, 22.0000],\n",
       "         [28.3750, 28.3750, 28.3750, 28.3750, 28.3750, 28.3750, 28.3750, 28.3750],\n",
       "         [34.7500, 34.7500, 34.7500, 34.7500, 34.7500, 34.7500, 34.7500, 34.7500],\n",
       "         [41.2500, 41.2500, 41.2500, 41.2500, 41.2500, 41.2500, 41.2500, 41.2500],\n",
       "         [47.5000, 47.5000, 47.5000, 47.5000, 47.5000, 47.5000, 47.5000, 47.5000],\n",
       "         [54.0000, 54.0000, 54.0000, 54.0000, 54.0000, 54.0000, 54.0000, 54.0000],\n",
       "         [60.5000, 60.5000, 60.5000, 60.5000, 60.5000, 60.5000, 60.5000, 60.5000],\n",
       "         [67.0000, 67.0000, 67.0000, 67.0000, 67.0000, 67.0000, 67.0000, 67.0000],\n",
       "         [73.0000, 73.0000, 73.0000, 73.0000, 73.0000, 73.0000, 73.0000, 73.0000],\n",
       "         [79.5000, 79.5000, 79.5000, 79.5000, 79.5000, 79.5000, 79.5000, 79.5000],\n",
       "         [86.0000, 86.0000, 86.0000, 86.0000, 86.0000, 86.0000, 86.0000, 86.0000],\n",
       "         [92.5000, 92.5000, 92.5000, 92.5000, 92.5000, 92.5000, 92.5000, 92.5000],\n",
       "         [99.0000, 99.0000, 99.0000, 99.0000, 99.0000, 99.0000, 99.0000, 99.0000]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " torch.Size([16, 8]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch\n",
    "pt_out = matA @ matB\n",
    "pt_out, pt_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matA.is_contiguous(), matB.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - bfloat16!\n",
      "m: 16, n: 8, k: 8\n",
      "blocksxy: 2-4, threads: 4-4\n",
      "In Kernel: m: 16, n: 8, k: 8\n",
      "<1, 0> - (ty,i) - As: (b) 7.187500\n",
      "<1, 0> - (ty,i) - Bs: (b) 1.000000\n",
      "In Kernel: gdim.x: 2, gdim.y: 4, bdim.x: 4, bdim.y: 4\n",
      "<0, 9> - (cx,cy)-InLoop:Csub: (b) 7.187500\n",
      "<1, 1> - (ty,i) - As: (b) 7.312500\n",
      "<1, 1> - (ty,i) - Bs: (b) 1.000000\n",
      "<0, 9> - (cx,cy)-InLoop:Csub: (b) 14.500000\n",
      "<1, 2> - (ty,i) - As: (b) 7.406250\n",
      "<1, 2> - (ty,i) - Bs: (b) 1.000000\n",
      "<0, 9> - (cx,cy)-InLoop:Csub: (b) 21.875000\n",
      "<1, 3> - (ty,i) - As: (b) 7.500000\n",
      "<1, 3> - (ty,i) - Bs: (b) 1.000000\n",
      "<0, 9> - (cx,cy)-InLoop:Csub: (b) 29.375000\n",
      "<0, 9> - (cx,cy)-AfterLoop:Csub: (b) 29.375000\n",
      "<1, 0> - (ty,i) - As: (b) 7.593750\n",
      "<1, 0> - (ty,i) - Bs: (b) 1.000000\n",
      "<0, 9> - (cx,cy)-InLoop:Csub: (b) 37.000000\n",
      "<1, 1> - (ty,i) - As: (b) 7.687500\n",
      "<1, 1> - (ty,i) - Bs: (b) 1.000000\n",
      "<0, 9> - (cx,cy)-InLoop:Csub: (b) 44.750000\n",
      "<1, 2> - (ty,i) - As: (b) 7.812500\n",
      "<1, 2> - (ty,i) - Bs: (b) 1.000000\n",
      "<0, 9> - (cx,cy)-InLoop:Csub: (b) 52.500000\n",
      "<1, 3> - (ty,i) - As: (b) 7.906250\n",
      "<1, 3> - (ty,i) - Bs: (b) 1.000000\n",
      "<0, 9> - (cx,cy)-InLoop:Csub: (b) 60.500000\n",
      "<0, 9> - (cx,cy)-AfterLoop:Csub: (b) 60.500000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.7969,  2.7969,  2.7969,  2.7969,  2.7969,  2.7969,  2.7969,  2.7969],\n",
       "         [ 9.2500,  9.2500,  9.2500,  9.2500,  9.2500,  9.2500,  9.2500,  9.2500],\n",
       "         [15.6250, 15.6250, 15.6250, 15.6250, 15.6250, 15.6250, 15.6250, 15.6250],\n",
       "         [22.0000, 22.0000, 22.0000, 22.0000, 22.0000, 22.0000, 22.0000, 22.0000],\n",
       "         [28.3750, 28.3750, 28.3750, 28.3750, 28.3750, 28.3750, 28.3750, 28.3750],\n",
       "         [34.7500, 34.7500, 34.7500, 34.7500, 34.7500, 34.7500, 34.7500, 34.7500],\n",
       "         [41.2500, 41.2500, 41.2500, 41.2500, 41.2500, 41.2500, 41.2500, 41.2500],\n",
       "         [47.5000, 47.5000, 47.5000, 47.5000, 47.5000, 47.5000, 47.5000, 47.5000],\n",
       "         [54.0000, 54.0000, 54.0000, 54.0000, 54.0000, 54.0000, 54.0000, 54.0000],\n",
       "         [60.5000, 60.5000, 60.5000, 60.5000, 60.5000, 60.5000, 60.5000, 60.5000],\n",
       "         [66.5000, 66.5000, 66.5000, 66.5000, 66.5000, 66.5000, 66.5000, 66.5000],\n",
       "         [73.5000, 73.5000, 73.5000, 73.5000, 73.5000, 73.5000, 73.5000, 73.5000],\n",
       "         [79.5000, 79.5000, 79.5000, 79.5000, 79.5000, 79.5000, 79.5000, 79.5000],\n",
       "         [86.0000, 86.0000, 86.0000, 86.0000, 86.0000, 86.0000, 86.0000, 86.0000],\n",
       "         [92.5000, 92.5000, 92.5000, 92.5000, 92.5000, 92.5000, 92.5000, 92.5000],\n",
       "         [98.5000, 98.5000, 98.5000, 98.5000, 98.5000, 98.5000, 98.5000, 98.5000]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " torch.Size([16, 8]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cu_out = mmkernelv1(mat_A=matA, mat_B=matB)\n",
    "cu_out, cu_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.1875, 7.3125, 7.4062, 7.5000, 7.5938, 7.6875, 7.8125, 7.9062],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matA[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.1875, 14.5000, 21.8750, 29.3750, 37.0000, 44.7500, 52.5000, 60.5000],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matA[9].cumsum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 72., 145., 219., 294., 370., 448., 528., 608.], device='cuda:0',\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(72,80, dtype=torch.bfloat16, device=torch.device('cuda:0')).cumsum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 72., 145., 219., 294., 370., 448., 524., 604.], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(72,80, dtype=torch.bfloat16, device=torch.device('cpu')).cumsum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 72., 145., 219., 294., 370., 447., 525., 604.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(72,80, dtype=torch.float32, device=torch.device('cpu')).cumsum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 72., 145., 219., 294., 370., 447., 525., 604.], device='cuda:0',\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(72,80, dtype=torch.float16, device=torch.device('cuda:0')).cumsum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# cu_out = mmkernelv2(mat_A=matA, mat_B=matB)\n",
    "# cu_out, cu_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat @ mat.T @ mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 6 # hidden size\n",
    "S = 5 # seq len\n",
    "B = 1 # batch size\n",
    "NH = 2 # num heads\n",
    "DH = H // NH # dim per head\n",
    "DTYPE = torch.bfloat16\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "assert H % NH == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create qkv, inputgates, forgetgates \n",
    "torch.manual_seed(0)\n",
    "qs = torch.randn((B, NH, S, DH), device=DEVICE, dtype=DTYPE)\n",
    "ks = torch.randn((B, NH, S, DH), device=DEVICE, dtype=DTYPE)\n",
    "vs = torch.randn((B, NH, S, DH), device=DEVICE, dtype=DTYPE)\n",
    "ds = torch.rand((B, NH, S, S), device=DEVICE, dtype=DTYPE)\n",
    "\n",
    "max_log_D, _ = torch.max(ds.view(B, NH, -1), dim=-1, keepdim=True)  # (B, NH, 1)\n",
    "log_D_matrix_stabilized = ds - max_log_D.unsqueeze(-1)  # (B, NH, S, S) = (B, NH, S, S) - (B, NH, 1, 1)\n",
    "D_matrix = torch.exp(log_D_matrix_stabilized)  # (B, NH, S, S)\n",
    "mval = torch.exp(-max_log_D.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstmpt21cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
