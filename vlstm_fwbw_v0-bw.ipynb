{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "sys.path.append(\".\")\n",
    "# os.environ[\"MAX_JOBS\"] = \"100\"\n",
    "\n",
    "import torch\n",
    "torch.set_printoptions(linewidth=200)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCLUDE: ['/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include', '/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/torch/csrc/api/include', '/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/TH', '/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/THC', '/home/max/miniconda3/envs/xlstmpt220cu121/include']\n",
      "/home/max/myrepos/vlstm_cuda/src\n",
      "/home/max/cpplibs/libtorch/lib:/usr/local/cuda-12.3/lib64:\n",
      "/home/max/miniconda3/envs/xlstmpt220cu121/lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/max/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/max/.cache/torch_extensions/py311_cu121/vlstm_fwbw_v0/build.ninja...\n",
      "Building extension module vlstm_fwbw_v0...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] /home/max/miniconda3/envs/xlstmpt220cu121/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel_bw.cuda.o.d -ccbin /home/max/miniconda3/envs/xlstmpt220cu121/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=vlstm_fwbw_v0 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include -isystem /home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/TH -isystem /home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/THC -isystem /home/max/miniconda3/envs/xlstmpt220cu121/include -isystem /home/max/miniconda3/envs/xlstmpt220cu121/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -gencode arch=compute_80,code=compute_80 -res-usage --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=13407807929942597099574024998205846127479365820592393377723561443721764030073546976801874298166903427690031858186486050853753882811946569946433649006084096 -std=c++17 -c /home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/kernel_bw.cu -o kernel_bw.cuda.o \n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh(264): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh:264 CUDART_VERSION: 12010, arch: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh\" \":\" \"264\" \" CUDART_VERSION: \" \"12010\" \", arch: \" \"890\")\n",
      "                                                                                                                                                 ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh(269): remark #20200-D: #pragma message: \"INCLUDING FP16\"\n",
      "  #pragma message(\"INCLUDING FP16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                                    ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                                     ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh(282): remark #20200-D: #pragma message: \"INCLUDING BF16\"\n",
      "  #pragma message(\"INCLUDING BF16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_bf16.cuh:29 CUDART_VERSION with BF16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with BF16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                                    ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2bf16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                                     ^\n",
      "\n",
      "ptxas info    : 270 bytes gmem\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels8vlstm_bwIfLi4ELi8ELi8EEEvPT_S3_S3_S3_S3_S3_S3_S3_S3_S3_S3_S3_S3_S3_PfS4_iiii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels8vlstm_bwIfLi4ELi8ELi8EEEvPT_S3_S3_S3_S3_S3_S3_S3_S3_S3_S3_S3_S3_S3_PfS4_iiii\n",
      "    48 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 128 registers, 496 bytes cmem[0], 16 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels8vlstm_bwI6__halfLi4ELi8ELi8EEEvPT_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_PfS5_iiii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels8vlstm_bwI6__halfLi4ELi8ELi8EEEvPT_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_PfS5_iiii\n",
      "    48 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 143 registers, 496 bytes cmem[0], 8 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels8vlstm_bwI13__nv_bfloat16Li4ELi8ELi8EEEvPT_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_PfS5_iiii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels8vlstm_bwI13__nv_bfloat16Li4ELi8ELi8EEEvPT_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_PfS5_iiii\n",
      "    48 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 122 registers, 496 bytes cmem[0], 8 bytes cmem[2]\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh(264): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh:264 CUDART_VERSION: 12010, arch: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh\" \":\" \"264\" \" CUDART_VERSION: \" \"12010\" \", arch: \" \"800\")\n",
      "                                                                                                                                                 ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh(269): remark #20200-D: #pragma message: \"INCLUDING FP16\"\n",
      "  #pragma message(\"INCLUDING FP16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                                    ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                                     ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh(282): remark #20200-D: #pragma message: \"INCLUDING BF16\"\n",
      "  #pragma message(\"INCLUDING BF16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_bf16.cuh:29 CUDART_VERSION with BF16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with BF16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                                    ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2bf16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                                     ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh:264:138: note: '#pragma message: /home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh:264 CUDART_VERSION: 12010, arch: __CUDA_ARCH__'\n",
      "  264 | #pragma message(AT \" CUDART_VERSION: \" TOSTRING(                               \\\n",
      "      |                                                                                                                                          ^\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh:276:90: note: '#pragma message: SKIPPING FP16, because of CUDART_VERSION: 12010, arch: __CUDA_ARCH__'\n",
      "  276 | #pragma message(\"SKIPPING FP16, because of CUDART_VERSION: \" TOSTRING(         \\\n",
      "      |                                                                                          ^\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh:289:90: note: '#pragma message: SKIPPING BF16, because of CUDART_VERSION: 12010, arch: __CUDA_ARCH__'\n",
      "  289 | #pragma message(\"SKIPPING BF16, because of CUDART_VERSION: \" TOSTRING(         \\\n",
      "      |                                                                                          ^\n",
      "[2/2] /home/max/miniconda3/envs/xlstmpt220cu121/bin/x86_64-conda-linux-gnu-c++ interface.o kernel_fw.cuda.o kernel_bw.cuda.o -shared -L/home/max/miniconda3/envs/xlstmpt220cu121/lib -lcublas -L/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/home/max/miniconda3/envs/xlstmpt220cu121/lib -lcudart -o vlstm_fwbw_v0.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module vlstm_fwbw_v0...\n"
     ]
    }
   ],
   "source": [
    "from src.vlstm_fwbw_v0.interface import vlstm_fwbw_torch_obw, vlstm_fwbw_cuda\n",
    "\n",
    "from src.vlstm_fwbw_v0.interface import vlstm_fw_torch, vlstm_fw_cuda\n",
    "from src.vlstm_fwbw_v0.interface import vlstm_bw_torch_obw, vlstm_bw_cuda\n",
    "from src.vlstm_fwbw_v0.torch_impl import vlstm_fw_tiled_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA vLSTM backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "S = 32 #16 #8 # seq len\n",
    "B = 1 # batch size\n",
    "NH = 1 # num heads\n",
    "DH = 8 # dim per head\n",
    "DTYPE = torch.float32\n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.0000,  0.1000,  0.2000,  0.3000,  0.4000,  0.5000,  0.6000,  0.7000],\n",
       "           [ 0.8000,  0.9000,  1.0000,  1.1000,  1.2000,  1.3000,  1.4000,  1.5000],\n",
       "           [ 1.6000,  1.7000,  1.8000,  1.9000,  2.0000,  2.1000,  2.2000,  2.3000],\n",
       "           [ 2.4000,  2.5000,  2.6000,  2.7000,  2.8000,  2.9000,  3.0000,  3.1000],\n",
       "           [ 3.2000,  3.3000,  3.4000,  3.5000,  3.6000,  3.7000,  3.8000,  3.9000],\n",
       "           [ 4.0000,  4.1000,  4.2000,  4.3000,  4.4000,  4.5000,  4.6000,  4.7000],\n",
       "           [ 4.8000,  4.9000,  5.0000,  5.1000,  5.2000,  5.3000,  5.4000,  5.5000],\n",
       "           [ 5.6000,  5.7000,  5.8000,  5.9000,  6.0000,  6.1000,  6.2000,  6.3000],\n",
       "           [ 6.4000,  6.5000,  6.6000,  6.7000,  6.8000,  6.9000,  7.0000,  7.1000],\n",
       "           [ 7.2000,  7.3000,  7.4000,  7.5000,  7.6000,  7.7000,  7.8000,  7.9000],\n",
       "           [ 8.0000,  8.1000,  8.2000,  8.3000,  8.4000,  8.5000,  8.6000,  8.7000],\n",
       "           [ 8.8000,  8.9000,  9.0000,  9.1000,  9.2000,  9.3000,  9.4000,  9.5000],\n",
       "           [ 9.6000,  9.7000,  9.8000,  9.9000, 10.0000, 10.1000, 10.2000, 10.3000],\n",
       "           [10.4000, 10.5000, 10.6000, 10.7000, 10.8000, 10.9000, 11.0000, 11.1000],\n",
       "           [11.2000, 11.3000, 11.4000, 11.5000, 11.6000, 11.7000, 11.8000, 11.9000],\n",
       "           [12.0000, 12.1000, 12.2000, 12.3000, 12.4000, 12.5000, 12.6000, 12.7000],\n",
       "           [12.8000, 12.9000, 13.0000, 13.1000, 13.2000, 13.3000, 13.4000, 13.5000],\n",
       "           [13.6000, 13.7000, 13.8000, 13.9000, 14.0000, 14.1000, 14.2000, 14.3000],\n",
       "           [14.4000, 14.5000, 14.6000, 14.7000, 14.8000, 14.9000, 15.0000, 15.1000],\n",
       "           [15.2000, 15.3000, 15.4000, 15.5000, 15.6000, 15.7000, 15.8000, 15.9000],\n",
       "           [16.0000, 16.1000, 16.2000, 16.3000, 16.4000, 16.5000, 16.6000, 16.7000],\n",
       "           [16.8000, 16.9000, 17.0000, 17.1000, 17.2000, 17.3000, 17.4000, 17.5000],\n",
       "           [17.6000, 17.7000, 17.8000, 17.9000, 18.0000, 18.1000, 18.2000, 18.3000],\n",
       "           [18.4000, 18.5000, 18.6000, 18.7000, 18.8000, 18.9000, 19.0000, 19.1000],\n",
       "           [19.2000, 19.3000, 19.4000, 19.5000, 19.6000, 19.7000, 19.8000, 19.9000],\n",
       "           [20.0000, 20.1000, 20.2000, 20.3000, 20.4000, 20.5000, 20.6000, 20.7000],\n",
       "           [20.8000, 20.9000, 21.0000, 21.1000, 21.2000, 21.3000, 21.4000, 21.5000],\n",
       "           [21.6000, 21.7000, 21.8000, 21.9000, 22.0000, 22.1000, 22.2000, 22.3000],\n",
       "           [22.4000, 22.5000, 22.6000, 22.7000, 22.8000, 22.9000, 23.0000, 23.1000],\n",
       "           [23.2000, 23.3000, 23.4000, 23.5000, 23.6000, 23.7000, 23.8000, 23.9000],\n",
       "           [24.0000, 24.1000, 24.2000, 24.3000, 24.4000, 24.5000, 24.6000, 24.7000],\n",
       "           [24.8000, 24.9000, 25.0000, 25.1000, 25.2000, 25.3000, 25.4000, 25.5000]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 32, 8]),\n",
       " 256)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create qkv, inputgates, forgetgates \n",
    "torch.manual_seed(0)\n",
    "qs = torch.arange((B*NH*S*DH), device=DEVICE, dtype=DTYPE).reshape((B, NH, S, DH)) / 10.\n",
    "ks = torch.ones((B, NH, S, DH), device=DEVICE, dtype=DTYPE) / 100.\n",
    "vs = torch.ones((B, NH, S, DH), device=DEVICE, dtype=DTYPE) / 100.\n",
    "# igs = (1. + torch.arange((B * NH * S), device=DEVICE, dtype=DTYPE)).reshape(B, NH, S, 1) / 10.\n",
    "# igs = torch.zeros((B, NH, S, 1), device=DEVICE, dtype=DTYPE) #/ 10.\n",
    "igs = torch.randn((B, NH, S, 1), device=DEVICE, dtype=DTYPE) #/ 10.\n",
    "fgs = torch.ones((B, NH, S, 1), device=DEVICE, dtype=DTYPE)\n",
    "fgs = torch.randn((B, NH, S, 1), device=DEVICE, dtype=DTYPE)\n",
    "\n",
    "dHs = qs.clone()\n",
    "qs, qs.shape, len(qs.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bw kernel match direct call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_pt, n_pt, m_pt, _, matD_pt = vlstm_fw_torch(queries=qs, keys=ks, values=vs, igate_preact=igs, fgate_preact=fgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dQs_pt, dKs_pt, dVs_pt, dIgs_pt, dFgs_pt, delta_D_pt, delta_Dtilde_pt, delta_fbar_pt, mat_P_pt, mat_C_pt = vlstm_bw_torch_obw(\n",
    "    delta_Htilde=dHs,\n",
    "    queries=qs,\n",
    "    keys=ks,\n",
    "    values=vs,\n",
    "    igate_preact=igs,\n",
    "    fgate_preact=fgs,\n",
    "    var_n=n_pt,\n",
    "    var_m=m_pt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rev_cumsum(x, dim=-1):\n",
    "    return x.flip(dims=(dim,)).cumsum(dim).flip(dims=(dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fgrads trick\n",
    "df = (dQs_pt * qs - dKs_pt * ks).sum(-1, keepdim=True)\n",
    "df = rev_cumsum(df, dim=-2)\n",
    "df_pt = df * torch.nn.functional.sigmoid(-fgs)\n",
    "# df_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pt - dFgs_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - float32!\n",
      "B: 1, NH: 1, S: 32, DH: 8\n",
      "blocksxy: 1-2, threadsxy: 4-4, shared_mem in bytes: 5700\n",
      "In FW-Kernel: gdim.x: 1, gdim.y: 2, gdim.z: 1, bdim.x: 4, bdim.y: 4\n",
      "In FW-Kernel: QtileDim: 8, KVtileDim: 8, TblockDim:4\n"
     ]
    }
   ],
   "source": [
    "hs_cu, n_cu, m_cu, _ = vlstm_fw_cuda(mat_Q=qs, mat_K=ks, mat_V=vs, igate_preact=igs, fgate_preact=fgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - float32!\n",
      "B: 1, NH: 1, S: 32, DH: 8\n",
      "blocksxy: 1-2, threadsxy: 4-4, shared_mem in bytes: 7648\n",
      "In BW-Kernel: gdim.x: 1, gdim.y: 2, gdim.z: 1, bdim.x: 4, bdim.y: 4\n",
      "In BW-Kernel: QtileDim: 8, KVtileDim: 8, TblockDim:4\n",
      "blockIdx(x,y)=(0,1), FTIdx=0, ijIdx(i,j)=(1,1), sTileXYIdx(x,y)=(8,8), csDYdimGBIdx=8, dcs_cv=0.000000, reachedMD=1\n",
      "blockIdx(x,y)=(0,1), FTIdx=1, ijIdx(i,j)=(1,1), sTileXYIdx(x,y)=(8,8), csDYdimGBIdx=9, dcs_cv=0.104561, reachedMD=1\n",
      "blockIdx(x,y)=(0,1), FTIdx=2, ijIdx(i,j)=(1,1), sTileXYIdx(x,y)=(8,8), csDYdimGBIdx=10, dcs_cv=0.063287, reachedMD=1\n",
      "blockIdx(x,y)=(0,1), FTIdx=3, ijIdx(i,j)=(1,1), sTileXYIdx(x,y)=(8,8), csDYdimGBIdx=11, dcs_cv=0.017909, reachedMD=1\n",
      "blockIdx(x,y)=(0,1), FTIdx=4, ijIdx(i,j)=(1,1), sTileXYIdx(x,y)=(8,8), csDYdimGBIdx=12, dcs_cv=0.651773, reachedMD=1\n",
      "blockIdx(x,y)=(0,1), FTIdx=5, ijIdx(i,j)=(1,1), sTileXYIdx(x,y)=(8,8), csDYdimGBIdx=13, dcs_cv=0.726957, reachedMD=1\n",
      "blockIdx(x,y)=(0,1), FTIdx=6, ijIdx(i,j)=(1,1), sTileXYIdx(x,y)=(8,8), csDYdimGBIdx=14, dcs_cv=0.306540, reachedMD=1\n",
      "blockIdx(x,y)=(0,1), FTIdx=7, ijIdx(i,j)=(1,1), sTileXYIdx(x,y)=(8,8), csDYdimGBIdx=15, dcs_cv=0.431364, reachedMD=1\n",
      "blockIdx(x,y)=(0,1), FTIdx=0, ijIdx(i,j)=(2,1), sTileXYIdx(x,y)=(8,16), csDYdimGBIdx=16, dcs_cv=0.773589, reachedMD=0\n",
      "blockIdx(x,y)=(0,1), FTIdx=1, ijIdx(i,j)=(2,1), sTileXYIdx(x,y)=(8,16), csDYdimGBIdx=17, dcs_cv=0.375782, reachedMD=0\n",
      "blockIdx(x,y)=(0,1), FTIdx=2, ijIdx(i,j)=(2,1), sTileXYIdx(x,y)=(8,16), csDYdimGBIdx=18, dcs_cv=0.122294, reachedMD=0\n",
      "blockIdx(x,y)=(0,1), FTIdx=3, ijIdx(i,j)=(2,1), sTileXYIdx(x,y)=(8,16), csDYdimGBIdx=19, dcs_cv=0.082258, reachedMD=0\n",
      "blockIdx(x,y)=(0,1), FTIdx=4, ijIdx(i,j)=(2,1), sTileXYIdx(x,y)=(8,16), csDYdimGBIdx=20, dcs_cv=0.076685, reachedMD=0\n",
      "blockIdx(x,y)=(0,1), FTIdx=5, ijIdx(i,j)=(2,1), sTileXYIdx(x,y)=(8,16), csDYdimGBIdx=21, dcs_cv=0.071804, reachedMD=0\n",
      "blockIdx(x,y)=(0,1), FTIdx=6, ijIdx(i,j)=(2,1), sTileXYIdx(x,y)=(8,16), csDYdimGBIdx=22, dcs_cv=0.047018, reachedMD=0\n",
      "blockIdx(x,y)=(0,1), FTIdx=7, ijIdx(i,j)=(2,1), sTileXYIdx(x,y)=(8,16), csDYdimGBIdx=23, dcs_cv=0.023921, reachedMD=0\n",
      "blockIdx(x,y)=(0,1), FTIdx=0, ijIdx(i,j)=(3,1), sTileXYIdx(x,y)=(8,24), csDYdimGBIdx=24, dcs_cv=0.011210, reachedMD=0\n",
      "blockIdx(x,y)=(0,1), FTIdx=1, ijIdx(i,j)=(3,1), sTileXYIdx(x,y)=(8,24), csDYdimGBIdx=25, dcs_cv=0.003828, reachedMD=0\n",
      "blockIdx(x,y)=(0,1), FTIdx=2, ijIdx(i,j)=(3,1), sTileXYIdx(x,y)=(8,24), csDYdimGBIdx=26, dcs_cv=0.000654, reachedMD=0\n",
      "blockIdx(x,y)=(0,1), FTIdx=3, ijIdx(i,j)=(3,1), sTileXYIdx(x,y)=(8,24), csDYdimGBIdx=27, dcs_cv=0.000401, reachedMD=0\n",
      "blockIdx(x,y)=(0,1), FTIdx=4, ijIdx(i,j)=(3,1), sTileXYIdx(x,y)=(8,24), csDYdimGBIdx=28, dcs_cv=0.000323, reachedMD=0\n",
      "blockIdx(x,y)=(0,1), FTIdx=5, ijIdx(i,j)=(3,1), sTileXYIdx(x,y)=(8,24), csDYdimGBIdx=29, dcs_cv=0.000288, reachedMD=0\n",
      "blockIdx(x,y)=(0,1), FTIdx=6, ijIdx(i,j)=(3,1), sTileXYIdx(x,y)=(8,24), csDYdimGBIdx=30, dcs_cv=0.000121, reachedMD=0\n",
      "blockIdx(x,y)=(0,1), FTIdx=7, ijIdx(i,j)=(3,1), sTileXYIdx(x,y)=(8,24), csDYdimGBIdx=31, dcs_cv=0.000104, reachedMD=0\n",
      "blockIdx(x,y)=(0,1), FTIdx=0, ijIdx(i,j)=(3,3), sTileXYIdx(x,y)=(24,24), csDYdimGBIdx=24, dcs_cv=0.000000, reachedMD=1\n",
      "blockIdx(x,y)=(0,1), FTIdx=1, ijIdx(i,j)=(3,3), sTileXYIdx(x,y)=(24,24), csDYdimGBIdx=25, dcs_cv=0.453375, reachedMD=1\n",
      "blockIdx(x,y)=(0,1), FTIdx=2, ijIdx(i,j)=(3,3), sTileXYIdx(x,y)=(24,24), csDYdimGBIdx=26, dcs_cv=0.236276, reachedMD=1\n",
      "blockIdx(x,y)=(0,1), FTIdx=3, ijIdx(i,j)=(3,3), sTileXYIdx(x,y)=(24,24), csDYdimGBIdx=27, dcs_cv=1.036784, reachedMD=1\n",
      "blockIdx(x,y)=(0,1), FTIdx=4, ijIdx(i,j)=(3,3), sTileXYIdx(x,y)=(24,24), csDYdimGBIdx=28, dcs_cv=1.417519, reachedMD=1\n",
      "blockIdx(x,y)=(0,1), FTIdx=5, ijIdx(i,j)=(3,3), sTileXYIdx(x,y)=(24,24), csDYdimGBIdx=29, dcs_cv=1.618716, reachedMD=1\n",
      "blockIdx(x,y)=(0,1), FTIdx=6, ijIdx(i,j)=(3,3), sTileXYIdx(x,y)=(24,24), csDYdimGBIdx=30, dcs_cv=0.794299, reachedMD=1\n",
      "blockIdx(x,y)=(0,1), FTIdx=7, ijIdx(i,j)=(3,3), sTileXYIdx(x,y)=(24,24), csDYdimGBIdx=31, dcs_cv=1.675420, reachedMD=1\n"
     ]
    }
   ],
   "source": [
    "dQs_cu, dKs_cu, dVs_cu, dIgs_cu, dFgs_cu, matC_cu, deltaDcsChunkArr_cu, deltaDcsVec_cu = vlstm_bw_cuda(\n",
    "    delta_Htilde=dHs,\n",
    "    mat_Q=qs,\n",
    "    mat_K=ks,\n",
    "    mat_V=vs,\n",
    "    igate_preact=igs,\n",
    "    fgate_preact=fgs,\n",
    "    n=n_pt,\n",
    "    m=m_pt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta Q match: True\n",
      "delta K match: False\n",
      "delta V match: False\n",
      "delta Igate match: True\n",
      "delta Fgate match: True\n"
     ]
    }
   ],
   "source": [
    "RTOL = 1e-5\n",
    "ATOL = 1e-5\n",
    "print(f\"delta Q match: {torch.allclose(dQs_cu, dQs_pt, rtol=RTOL, atol=ATOL)}\")\n",
    "print(f\"delta K match: {torch.allclose(dKs_cu, dKs_pt, rtol=RTOL, atol=ATOL)}\")\n",
    "print(f\"delta V match: {torch.allclose(dVs_cu, dVs_pt, rtol=RTOL, atol=ATOL)}\")\n",
    "print(f\"delta Igate match: {torch.allclose(dIgs_cu, dIgs_pt.squeeze(), rtol=RTOL, atol=ATOL)}\")\n",
    "print(f\"delta Fgate match: {torch.allclose(dFgs_cu, delta_fbar_pt.squeeze(-1), rtol=RTOL, atol=ATOL)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [6.2363e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.3372e-02, 3.5437e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.0824e-02, 3.1574e-02, 8.2969e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.8426e-02, 2.7937e-02, 7.3413e-02, 1.0456e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [6.0027e-03, 9.1013e-03, 2.3916e-02, 3.4064e-02, 6.3287e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.1565e-03, 1.7534e-03, 4.6076e-03, 6.5626e-03, 1.2193e-02, 1.7908e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.0297e-03, 1.5613e-03, 4.1026e-03, 5.8433e-03, 1.0856e-02, 1.5946e-02, 6.5177e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.4040e-04, 1.4258e-03, 3.7468e-03, 5.3365e-03, 9.9148e-03, 1.4563e-02, 5.9524e-01, 7.2696e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.3520e-04, 5.0823e-04, 1.3355e-03, 1.9022e-03, 3.5341e-03, 5.1907e-03, 2.1217e-01, 2.5912e-01, 3.0654e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.5649e-04, 2.3726e-04, 6.2348e-04, 8.8801e-04, 1.6499e-03, 2.4233e-03, 9.9049e-02, 1.2097e-01, 1.4311e-01, 4.3136e-01, 0.0000e+00, 0.0000e+00],\n",
       "          [1.2253e-04, 1.8577e-04, 4.8817e-04, 6.9530e-04, 1.2918e-03, 1.8974e-03, 7.7554e-02, 9.4716e-02, 1.1205e-01, 3.3775e-01, 7.7359e-01, 0.0000e+00],\n",
       "          [5.9519e-05, 9.0243e-05, 2.3714e-04, 3.3775e-04, 6.2752e-04, 9.2168e-04, 3.7673e-02, 4.6010e-02, 5.4430e-02, 1.6407e-01, 3.7578e-01, 5.1102e-01],\n",
       "          [1.9370e-05, 2.9368e-05, 7.7174e-05, 1.0992e-04, 2.0422e-04, 2.9995e-04, 1.2260e-02, 1.4973e-02, 1.7714e-02, 5.3394e-02, 1.2229e-01, 1.6631e-01],\n",
       "          [1.3029e-05, 1.9754e-05, 5.1909e-05, 7.3933e-05, 1.3736e-04, 2.0175e-04, 8.2466e-03, 1.0071e-02, 1.1915e-02, 3.5914e-02, 8.2258e-02, 1.1186e-01],\n",
       "          [1.2146e-05, 1.8416e-05, 4.8392e-05, 6.8924e-05, 1.2806e-04, 1.8808e-04, 7.6878e-03, 9.3890e-03, 1.1107e-02, 3.3481e-02, 7.6684e-02, 1.0428e-01],\n",
       "          [1.1373e-05, 1.7244e-05, 4.5312e-05, 6.4538e-05, 1.1991e-04, 1.7611e-04, 7.1986e-03, 8.7915e-03, 1.0400e-02, 3.1350e-02, 7.1804e-02, 9.7647e-02],\n",
       "          [7.4470e-06, 1.1291e-05, 2.9670e-05, 4.2259e-05, 7.8514e-05, 1.1532e-04, 4.7136e-03, 5.7567e-03, 6.8102e-03, 2.0528e-02, 4.7017e-02, 6.3939e-02],\n",
       "          [3.7888e-06, 5.7445e-06, 1.5095e-05, 2.1500e-05, 3.9945e-05, 5.8671e-05, 2.3981e-03, 2.9288e-03, 3.4648e-03, 1.0444e-02, 2.3921e-02, 3.2530e-02],\n",
       "          [1.7755e-06, 2.6920e-06, 7.0740e-06, 1.0075e-05, 1.8719e-05, 2.7495e-05, 1.1238e-03, 1.3725e-03, 1.6237e-03, 4.8943e-03, 1.1210e-02, 1.5244e-02],\n",
       "          [6.0636e-07, 9.1936e-07, 2.4159e-06, 3.4409e-06, 6.3929e-06, 9.3897e-06, 3.8380e-04, 4.6873e-04, 5.5451e-04, 1.6715e-03, 3.8283e-03, 5.2061e-03],\n",
       "          [1.0355e-07, 1.5701e-07, 4.1258e-07, 5.8764e-07, 1.0918e-06, 1.6036e-06, 6.5545e-05, 8.0050e-05, 9.4700e-05, 2.8545e-04, 6.5380e-04, 8.8910e-04],\n",
       "          [6.3453e-08, 9.6208e-08, 2.5281e-07, 3.6008e-07, 6.6900e-07, 9.8260e-07, 4.0163e-05, 4.9051e-05, 5.8028e-05, 1.7491e-04, 4.0062e-04, 5.4480e-04],\n",
       "          [5.1222e-08, 7.7663e-08, 2.0408e-07, 2.9067e-07, 5.4004e-07, 7.9320e-07, 3.2422e-05, 3.9596e-05, 4.6843e-05, 1.4120e-04, 3.2340e-04, 4.3979e-04],\n",
       "          [4.5557e-08, 6.9074e-08, 1.8151e-07, 2.5852e-07, 4.8032e-07, 7.0548e-07, 2.8836e-05, 3.5217e-05, 4.1662e-05, 1.2558e-04, 2.8763e-04, 3.9115e-04],\n",
       "          [1.9207e-08, 2.9122e-08, 7.6526e-08, 1.0899e-07, 2.0250e-07, 2.9743e-07, 1.2157e-05, 1.4848e-05, 1.7565e-05, 5.2946e-05, 1.2127e-04, 1.6491e-04],\n",
       "          [1.6519e-08, 2.5047e-08, 6.5818e-08, 9.3743e-08, 1.7417e-07, 2.5581e-07, 1.0456e-05, 1.2770e-05, 1.5107e-05, 4.5537e-05, 1.0430e-04, 1.4183e-04]]]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_Dtilde_pt.cumsum(-1).tril(-1)[:, :, :, 5:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0000e+00],\n",
       "           [0.0000e+00],\n",
       "           [0.0000e+00],\n",
       "           [0.0000e+00],\n",
       "           [0.0000e+00],\n",
       "           [0.0000e+00],\n",
       "           [0.0000e+00],\n",
       "           [0.0000e+00],\n",
       "           [0.0000e+00],\n",
       "           [0.0000e+00],\n",
       "           [0.0000e+00],\n",
       "           [0.0000e+00],\n",
       "           [0.0000e+00],\n",
       "           [0.0000e+00],\n",
       "           [0.0000e+00],\n",
       "           [0.0000e+00],\n",
       "           [7.7359e-01],\n",
       "           [3.7578e-01],\n",
       "           [1.2229e-01],\n",
       "           [8.2258e-02],\n",
       "           [7.6685e-02],\n",
       "           [7.1804e-02],\n",
       "           [4.7018e-02],\n",
       "           [2.3921e-02],\n",
       "           [1.1210e-02],\n",
       "           [3.8283e-03],\n",
       "           [6.5380e-04],\n",
       "           [4.0062e-04],\n",
       "           [3.2340e-04],\n",
       "           [2.8763e-04],\n",
       "           [1.2127e-04],\n",
       "           [1.0430e-04]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 32]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaDcsVec_cu.unsqueeze(-1), deltaDcsVec_cu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  2.7940e-09,  1.8626e-08,  1.6764e-08,  7.4506e-08,  7.4506e-08,  4.4703e-08,  5.9605e-08,  8.9407e-08,  1.0431e-07,  1.3411e-07,  1.7136e-07,  3.8147e-06,  2.1458e-06,\n",
       "           7.1526e-07,  1.3113e-06,  2.2650e-06,  1.4305e-06,  9.5367e-07,  2.6226e-06,  3.0994e-06,  2.1458e-06,  4.7684e-07, -5.9605e-07,  5.9605e-08,  7.7486e-07,  9.5367e-07,  2.6226e-06,\n",
       "           4.7684e-07,  0.0000e+00,  0.0000e+00,  5.9605e-07]]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dFgs_cu - delta_fbar_pt.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.0239,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.0112, -0.0112,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.0038, -0.0038, -0.0038,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.0007, -0.0007, -0.0007, -0.0007,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.0004, -0.0004, -0.0004, -0.0004, -0.0004,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.0003, -0.0003, -0.0003, -0.0003, -0.0003, -0.0003,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.0003, -0.0003, -0.0003, -0.0003, -0.0003, -0.0003, -0.0003,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.0001, -0.0001, -0.0001, -0.0001, -0.0001, -0.0001, -0.0001, -0.0001,  0.0000,  0.0000],\n",
       "          [-0.0001, -0.0001, -0.0001, -0.0001, -0.0001, -0.0001, -0.0001, -0.0001, -0.0001,  0.0000]]]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matC_cu[:, :, :, -10:] - delta_Dtilde_pt.cumsum(-1).tril(-1)[:, :, :, -10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000],\n",
       "          [0.0000],\n",
       "          [0.0000],\n",
       "          [0.0000],\n",
       "          [0.0000],\n",
       "          [0.0000],\n",
       "          [0.0000],\n",
       "          [0.0000],\n",
       "          [0.0830],\n",
       "          [0.0734],\n",
       "          [0.0239],\n",
       "          [0.0046],\n",
       "          [0.0041],\n",
       "          [0.0037],\n",
       "          [0.0013],\n",
       "          [0.0006]]]], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matC_cu[:, :, :, -9:-8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0830, 0.0734, 0.0239, 0.0046, 0.0041, 0.0037, 0.0013, 0.0006],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaDcsChunkArr_cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 16]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaDcsVec_cu, deltaDcsVec_cu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [4.3366e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [8.9254e-04, 4.9177e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [5.7532e-04, 3.1699e-03, 3.5641e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [7.6602e-04, 4.2206e-03, 4.7456e-03, 3.1090e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [8.1001e-04, 4.4630e-03, 5.0181e-03, 3.2875e-02, 5.9596e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [6.0442e-04, 3.3302e-03, 3.7444e-03, 2.4531e-02, 4.4470e-02, 6.2363e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2652e-04, 1.2481e-03, 1.4033e-03, 9.1937e-03, 1.6666e-02, 2.3372e-02, 3.5437e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.0183e-04, 1.1120e-03, 1.2503e-03, 8.1914e-03, 1.4849e-02, 2.0824e-02, 3.1574e-02, 8.2969e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.7858e-04, 9.8395e-04, 1.1063e-03, 7.2480e-03, 1.3139e-02, 1.8426e-02, 2.7937e-02, 7.3413e-02, 1.0456e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [5.8178e-05, 3.2055e-04, 3.6042e-04, 2.3612e-03, 4.2804e-03, 6.0027e-03, 9.1013e-03, 2.3916e-02, 3.4064e-02, 6.3287e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.1208e-05, 6.1756e-05, 6.9437e-05, 4.5491e-04, 8.2465e-04, 1.1565e-03, 1.7534e-03, 4.6076e-03, 6.5626e-03, 1.2193e-02, 1.7908e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9800e-06, 5.4988e-05, 6.1827e-05, 4.0505e-04, 7.3427e-04, 1.0297e-03, 1.5613e-03, 4.1026e-03, 5.8433e-03, 1.0856e-02, 1.5946e-02, 6.5177e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.1144e-06, 5.0218e-05, 5.6464e-05, 3.6992e-04, 6.7058e-04, 9.4040e-04, 1.4258e-03, 3.7468e-03, 5.3365e-03, 9.9148e-03, 1.4563e-02, 5.9524e-01, 7.2696e-01, 0.0000e+00, 0.0000e+00],\n",
       "          [3.2487e-06, 1.7900e-05, 2.0126e-05, 1.3185e-04, 2.3902e-04, 3.3520e-04, 5.0823e-04, 1.3355e-03, 1.9022e-03, 3.5341e-03, 5.1907e-03, 2.1217e-01, 2.5912e-01, 3.0654e-01, 0.0000e+00],\n",
       "          [1.5167e-06, 8.3565e-06, 9.3958e-06, 6.1555e-05, 1.1159e-04, 1.5649e-04, 2.3726e-04, 6.2348e-04, 8.8801e-04, 1.6499e-03, 2.4233e-03, 9.9049e-02, 1.2097e-01, 1.4311e-01, 4.3136e-01]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_Dtilde_pt.cumsum(-1).tril(-1)[:, :, :, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0914e-11,  1.0914e-11,  1.0914e-11,  1.0914e-11,  1.0914e-11,  1.0914e-11,  1.0914e-11,  1.0914e-11],\n",
       "          [ 1.4552e-10,  1.4552e-10,  1.4552e-10,  1.4552e-10,  1.4552e-10,  1.4552e-10,  1.4552e-10,  1.4552e-10],\n",
       "          [ 1.4552e-10,  1.4552e-10,  1.4552e-10,  1.4552e-10,  1.4552e-10,  1.4552e-10,  1.4552e-10,  1.4552e-10],\n",
       "          [ 1.1642e-09,  1.1642e-09,  1.1642e-09,  1.1642e-09,  1.1642e-09,  1.1642e-09,  1.1642e-09,  1.1642e-09],\n",
       "          [ 1.3970e-09,  1.3970e-09,  1.3970e-09,  1.3970e-09,  1.3970e-09,  1.3970e-09,  1.3970e-09,  1.3970e-09],\n",
       "          [ 1.1642e-09,  1.1642e-09,  1.1642e-09,  1.1642e-09,  1.1642e-09,  1.1642e-09,  1.1642e-09,  1.1642e-09],\n",
       "          [ 1.3970e-09,  1.3970e-09,  1.3970e-09,  1.3970e-09,  1.3970e-09,  1.3970e-09,  1.3970e-09,  1.3970e-09],\n",
       "          [ 1.1642e-09,  1.1642e-09,  1.1642e-09,  1.1642e-09,  1.1642e-09,  1.1642e-09,  1.1642e-09,  1.1642e-09],\n",
       "          [ 6.9849e-10,  6.9849e-10,  6.9849e-10,  6.9849e-10,  6.9849e-10,  6.9849e-10,  6.9849e-10,  6.9849e-10],\n",
       "          [ 1.6298e-09,  1.6298e-09,  1.6298e-09,  1.6298e-09,  1.6298e-09,  1.6298e-09,  1.6298e-09,  1.6298e-09],\n",
       "          [ 2.3283e-10,  2.3283e-10,  2.3283e-10,  2.3283e-10,  2.3283e-10,  2.3283e-10,  2.3283e-10,  2.3283e-10],\n",
       "          [ 3.7253e-08,  3.7253e-08,  3.7253e-08,  3.7253e-08,  3.7253e-08,  3.7253e-08,  3.7253e-08,  3.7253e-08],\n",
       "          [ 3.0734e-08,  3.0734e-08,  3.0734e-08,  3.0734e-08,  3.0734e-08,  3.0734e-08,  3.0734e-08,  3.0734e-08],\n",
       "          [ 2.0489e-08,  2.0489e-08,  2.0489e-08,  2.0489e-08,  2.0489e-08,  2.0489e-08,  2.0489e-08,  2.0489e-08],\n",
       "          [ 1.7695e-08,  1.7695e-08,  1.7695e-08,  1.7695e-08,  1.7695e-08,  1.7695e-08,  1.7695e-08,  1.7695e-08],\n",
       "          [ 1.3970e-08,  1.3970e-08,  1.3970e-08,  1.3970e-08,  1.3970e-08,  1.3970e-08,  1.3970e-08,  1.3970e-08],\n",
       "          [ 1.3039e-08,  1.3039e-08,  1.3039e-08,  1.3039e-08,  1.3039e-08,  1.3039e-08,  1.3039e-08,  1.3039e-08],\n",
       "          [ 1.3970e-08,  1.3970e-08,  1.3970e-08,  1.3970e-08,  1.3970e-08,  1.3970e-08,  1.3970e-08,  1.3970e-08],\n",
       "          [ 1.3970e-08,  1.3970e-08,  1.3970e-08,  1.3970e-08,  1.3970e-08,  1.3970e-08,  1.3970e-08,  1.3970e-08],\n",
       "          [ 7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09],\n",
       "          [ 8.3819e-09,  8.3819e-09,  8.3819e-09,  8.3819e-09,  8.3819e-09,  8.3819e-09,  8.3819e-09,  8.3819e-09],\n",
       "          [ 1.1176e-08,  1.1176e-08,  1.1176e-08,  1.1176e-08,  1.1176e-08,  1.1176e-08,  1.1176e-08,  1.1176e-08],\n",
       "          [ 6.5193e-09,  6.5193e-09,  6.5193e-09,  6.5193e-09,  6.5193e-09,  6.5193e-09,  6.5193e-09,  6.5193e-09],\n",
       "          [-2.3283e-09, -2.3283e-09, -2.3283e-09, -2.3283e-09, -2.3283e-09, -2.3283e-09, -2.3283e-09, -2.3283e-09],\n",
       "          [ 4.6566e-09,  4.6566e-09,  4.6566e-09,  4.6566e-09,  4.6566e-09,  4.6566e-09,  4.6566e-09,  4.6566e-09],\n",
       "          [ 7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09],\n",
       "          [ 1.4901e-08,  1.4901e-08,  1.4901e-08,  1.4901e-08,  1.4901e-08,  1.4901e-08,  1.4901e-08,  1.4901e-08],\n",
       "          [ 1.2107e-08,  1.2107e-08,  1.2107e-08,  1.2107e-08,  1.2107e-08,  1.2107e-08,  1.2107e-08,  1.2107e-08],\n",
       "          [ 4.6566e-09,  4.6566e-09,  4.6566e-09,  4.6566e-09,  4.6566e-09,  4.6566e-09,  4.6566e-09,  4.6566e-09],\n",
       "          [ 9.3132e-10,  9.3132e-10,  9.3132e-10,  9.3132e-10,  9.3132e-10,  9.3132e-10,  9.3132e-10,  9.3132e-10],\n",
       "          [ 3.7253e-09,  3.7253e-09,  3.7253e-09,  3.7253e-09,  3.7253e-09,  3.7253e-09,  3.7253e-09,  3.7253e-09],\n",
       "          [ 3.7253e-09,  3.7253e-09,  3.7253e-09,  3.7253e-09,  3.7253e-09,  3.7253e-09,  3.7253e-09,  3.7253e-09]]]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dQs_cu - dQs_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2.9802e-08, 3.3528e-08, 2.9802e-08, 3.3528e-08, 3.3528e-08, 3.7253e-08, 2.9802e-08, 2.9802e-08],\n",
       "          [1.4901e-07, 1.4901e-07, 2.0862e-07, 1.7881e-07, 2.0862e-07, 1.7881e-07, 2.0862e-07, 2.3842e-07],\n",
       "          [2.2352e-08, 2.6077e-08, 2.2352e-08, 2.6077e-08, 2.6077e-08, 2.9802e-08, 2.9802e-08, 3.3528e-08],\n",
       "          [8.3447e-07, 9.5367e-07, 8.3447e-07, 9.5367e-07, 9.5367e-07, 8.3447e-07, 1.0729e-06, 9.5367e-07],\n",
       "          [7.1526e-07, 7.1526e-07, 5.9605e-07, 7.1526e-07, 7.1526e-07, 8.3447e-07, 7.1526e-07, 5.9605e-07],\n",
       "          [5.3644e-07, 4.1723e-07, 3.5763e-07, 4.1723e-07, 4.7684e-07, 5.3644e-07, 4.1723e-07, 5.3644e-07],\n",
       "          [2.9802e-07, 3.5763e-07, 2.9802e-07, 2.9802e-07, 2.9802e-07, 2.3842e-07, 4.1723e-07, 2.3842e-07],\n",
       "          [9.5367e-07, 1.4305e-06, 1.1921e-06, 7.1526e-07, 1.1921e-06, 1.1921e-06, 1.1921e-06, 1.1921e-06],\n",
       "          [5.9605e-07, 5.9605e-07, 8.3447e-07, 7.1526e-07, 5.9605e-07, 7.1526e-07, 5.9605e-07, 4.7684e-07],\n",
       "          [1.4305e-06, 1.6689e-06, 1.5497e-06, 1.3113e-06, 1.6689e-06, 1.4305e-06, 1.4305e-06, 1.5497e-06],\n",
       "          [7.1526e-07, 6.5565e-07, 6.5565e-07, 7.1526e-07, 7.1526e-07, 7.7486e-07, 7.1526e-07, 7.7486e-07],\n",
       "          [7.6294e-05, 8.0109e-05, 7.8201e-05, 8.2016e-05, 8.2016e-05, 8.0109e-05, 8.5831e-05, 8.2016e-05],\n",
       "          [9.0599e-06, 8.5831e-06, 8.1062e-06, 8.5831e-06, 9.0599e-06, 9.0599e-06, 8.5831e-06, 8.5831e-06],\n",
       "          [5.0068e-06, 5.0068e-06, 5.0068e-06, 5.0068e-06, 5.0068e-06, 5.2452e-06, 5.4836e-06, 5.2452e-06],\n",
       "          [2.8610e-05, 2.6703e-05, 2.8610e-05, 2.6703e-05, 3.2425e-05, 3.0518e-05, 2.8610e-05, 3.0518e-05],\n",
       "          [2.4796e-05, 2.6703e-05, 2.8610e-05, 2.4796e-05, 2.6703e-05, 2.6703e-05, 2.8610e-05, 2.6703e-05],\n",
       "          [5.2527e-02, 5.5001e-02, 5.7477e-02, 5.9950e-02, 6.2426e-02, 6.4901e-02, 6.7375e-02, 6.9851e-02],\n",
       "          [2.3687e-01, 2.4626e-01, 2.5565e-01, 2.6504e-01, 2.7443e-01, 2.8382e-01, 2.9321e-01, 3.0260e-01],\n",
       "          [3.3440e-02, 3.4550e-02, 3.5648e-02, 3.6751e-02, 3.7861e-02, 3.8963e-02, 4.0066e-02, 4.1176e-02],\n",
       "          [1.3617e+00, 1.3975e+00, 1.4332e+00, 1.4689e+00, 1.5046e+00, 1.5403e+00, 1.5760e+00, 1.6117e+00],\n",
       "          [1.0991e+00, 1.1248e+00, 1.1504e+00, 1.1760e+00, 1.2017e+00, 1.2273e+00, 1.2529e+00, 1.2786e+00],\n",
       "          [7.3086e-01, 7.4587e-01, 7.6089e-01, 7.7591e-01, 7.9093e-01, 8.0594e-01, 8.2096e-01, 8.3598e-01],\n",
       "          [8.1901e-01, 8.3363e-01, 8.4825e-01, 8.6287e-01, 8.7749e-01, 8.9211e-01, 9.0673e-01, 9.2135e-01],\n",
       "          [2.1226e+00, 2.1552e+00, 2.1877e+00, 2.2202e+00, 2.2528e+00, 2.2853e+00, 2.3179e+00, 2.3504e+00],\n",
       "          [9.8906e-01, 1.0030e+00, 1.0170e+00, 1.0310e+00, 1.0450e+00, 1.0590e+00, 1.0730e+00, 1.0870e+00],\n",
       "          [1.6468e+00, 1.6684e+00, 1.6899e+00, 1.7114e+00, 1.7329e+00, 1.7544e+00, 1.7760e+00, 1.7975e+00],\n",
       "          [5.8630e-01, 5.9307e-01, 5.9983e-01, 6.0661e-01, 6.1337e-01, 6.2015e-01, 6.2691e-01, 6.3369e-01],\n",
       "          [2.8853e+01, 2.9144e+01, 2.9435e+01, 2.9726e+01, 3.0017e+01, 3.0308e+01, 3.0599e+01, 3.0890e+01],\n",
       "          [4.5976e+00, 4.6415e+00, 4.6853e+00, 4.7292e+00, 4.7731e+00, 4.8169e+00, 4.8608e+00, 4.9047e+00],\n",
       "          [2.8870e+00, 2.9130e+00, 2.9390e+00, 2.9650e+00, 2.9910e+00, 3.0170e+00, 3.0430e+00, 3.0690e+00],\n",
       "          [1.6641e+01, 1.6778e+01, 1.6915e+01, 1.7053e+01, 1.7190e+01, 1.7327e+01, 1.7464e+01, 1.7601e+01],\n",
       "          [1.7682e+01, 1.7818e+01, 1.7954e+01, 1.8089e+01, 1.8225e+01, 1.8361e+01, 1.8497e+01, 1.8633e+01]]]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dKs_cu - dKs_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2.6077e-08, 3.3528e-08, 3.7253e-08, 2.9802e-08, 2.9802e-08, 4.4703e-08, 3.7253e-08, 2.9802e-08],\n",
       "          [1.7881e-07, 1.7881e-07, 2.3842e-07, 2.0862e-07, 2.0862e-07, 2.0862e-07, 1.7881e-07, 2.0862e-07],\n",
       "          [2.2352e-08, 2.2352e-08, 2.2352e-08, 2.6077e-08, 2.6077e-08, 2.6077e-08, 3.3528e-08, 2.9802e-08],\n",
       "          [9.5367e-07, 9.5367e-07, 9.5367e-07, 1.0729e-06, 1.0729e-06, 1.1921e-06, 9.5367e-07, 1.0729e-06],\n",
       "          [7.1526e-07, 7.1526e-07, 7.1526e-07, 8.3447e-07, 7.1526e-07, 7.1526e-07, 7.1526e-07, 5.9605e-07],\n",
       "          [5.3644e-07, 4.7684e-07, 4.1723e-07, 4.7684e-07, 4.7684e-07, 4.7684e-07, 4.7684e-07, 4.1723e-07],\n",
       "          [3.5763e-07, 2.9802e-07, 2.3842e-07, 3.5763e-07, 2.3842e-07, 2.9802e-07, 3.5763e-07, 4.1723e-07],\n",
       "          [1.1921e-06, 1.4305e-06, 9.5367e-07, 7.1526e-07, 9.5367e-07, 1.1921e-06, 7.1526e-07, 9.5367e-07],\n",
       "          [5.3644e-07, 5.9605e-07, 4.7684e-07, 4.7684e-07, 4.7684e-07, 4.7684e-07, 5.9605e-07, 3.5763e-07],\n",
       "          [1.4305e-06, 1.4305e-06, 1.4305e-06, 1.4305e-06, 1.4305e-06, 1.4305e-06, 1.1921e-06, 1.6689e-06],\n",
       "          [7.1526e-07, 6.5565e-07, 7.7486e-07, 6.5565e-07, 7.1526e-07, 7.7486e-07, 7.7486e-07, 7.7486e-07],\n",
       "          [7.8201e-05, 8.0109e-05, 8.0109e-05, 7.6294e-05, 8.5831e-05, 8.2016e-05, 8.3923e-05, 8.0109e-05],\n",
       "          [8.5831e-06, 8.1062e-06, 8.1062e-06, 8.5831e-06, 9.5367e-06, 9.0599e-06, 8.5831e-06, 8.5831e-06],\n",
       "          [5.2452e-06, 5.2452e-06, 5.4836e-06, 5.4836e-06, 5.2452e-06, 5.4836e-06, 5.2452e-06, 5.7220e-06],\n",
       "          [2.8610e-05, 2.6703e-05, 2.6703e-05, 2.6703e-05, 2.8610e-05, 2.8610e-05, 2.6703e-05, 2.8610e-05],\n",
       "          [2.2888e-05, 2.6703e-05, 2.6703e-05, 2.6703e-05, 2.6703e-05, 2.4796e-05, 2.8610e-05, 2.6703e-05],\n",
       "          [5.2528e-02, 5.5002e-02, 5.7477e-02, 5.9952e-02, 6.2426e-02, 6.4901e-02, 6.7374e-02, 6.9850e-02],\n",
       "          [2.3687e-01, 2.4626e-01, 2.5565e-01, 2.6504e-01, 2.7443e-01, 2.8382e-01, 2.9321e-01, 3.0260e-01],\n",
       "          [3.3443e-02, 3.4550e-02, 3.5648e-02, 3.6762e-02, 3.7865e-02, 3.8963e-02, 4.0070e-02, 4.1176e-02],\n",
       "          [1.3617e+00, 1.3975e+00, 1.4332e+00, 1.4689e+00, 1.5046e+00, 1.5403e+00, 1.5760e+00, 1.6117e+00],\n",
       "          [1.0991e+00, 1.1248e+00, 1.1504e+00, 1.1760e+00, 1.2017e+00, 1.2273e+00, 1.2529e+00, 1.2786e+00],\n",
       "          [7.3086e-01, 7.4587e-01, 7.6089e-01, 7.7591e-01, 7.9093e-01, 8.0594e-01, 8.2096e-01, 8.3598e-01],\n",
       "          [8.1901e-01, 8.3363e-01, 8.4825e-01, 8.6287e-01, 8.7749e-01, 8.9211e-01, 9.0673e-01, 9.2135e-01],\n",
       "          [2.1226e+00, 2.1552e+00, 2.1877e+00, 2.2202e+00, 2.2528e+00, 2.2853e+00, 2.3179e+00, 2.3504e+00],\n",
       "          [9.8906e-01, 1.0030e+00, 1.0170e+00, 1.0310e+00, 1.0450e+00, 1.0590e+00, 1.0730e+00, 1.0870e+00],\n",
       "          [1.6468e+00, 1.6683e+00, 1.6899e+00, 1.7114e+00, 1.7329e+00, 1.7544e+00, 1.7760e+00, 1.7975e+00],\n",
       "          [5.8630e-01, 5.9307e-01, 5.9984e-01, 6.0661e-01, 6.1338e-01, 6.2015e-01, 6.2693e-01, 6.3369e-01],\n",
       "          [2.8853e+01, 2.9144e+01, 2.9435e+01, 2.9726e+01, 3.0017e+01, 3.0308e+01, 3.0599e+01, 3.0890e+01],\n",
       "          [4.5976e+00, 4.6415e+00, 4.6853e+00, 4.7292e+00, 4.7731e+00, 4.8169e+00, 4.8608e+00, 4.9047e+00],\n",
       "          [2.8870e+00, 2.9130e+00, 2.9390e+00, 2.9650e+00, 2.9910e+00, 3.0170e+00, 3.0430e+00, 3.0690e+00],\n",
       "          [1.6641e+01, 1.6778e+01, 1.6915e+01, 1.7053e+01, 1.7190e+01, 1.7327e+01, 1.7464e+01, 1.7601e+01],\n",
       "          [1.7682e+01, 1.7818e+01, 1.7954e+01, 1.8089e+01, 1.8225e+01, 1.8361e+01, 1.8497e+01, 1.8633e+01]]]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dVs_cu - dVs_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.3283e-09,  1.4901e-08,  1.8626e-09,  9.6858e-08,  5.9605e-08,  3.7253e-08,  2.2352e-08,  7.4506e-08,  3.7253e-08,  1.3411e-07,  5.2154e-08,  6.4373e-06,  7.1526e-07,  4.0233e-07,\n",
       "            2.2650e-06,  2.0266e-06,  4.7684e-07,  1.4305e-06,  3.3379e-06,  1.9073e-06,  3.2783e-07,  5.9605e-08,  0.0000e+00,  8.9407e-08,  1.5497e-06,  1.3113e-06,  4.2915e-06, -4.7684e-07,\n",
       "            1.1921e-07,  1.1921e-07,  1.6689e-06,  1.1921e-07]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 32]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dIgs_cu - dIgs_pt.squeeze(-1), dIgs_cu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  2.7940e-09,  1.8626e-08,  1.6764e-08,  7.4506e-08,  7.4506e-08,  4.4703e-08,  5.9605e-08,  8.9407e-08,  1.0431e-07,  1.3411e-07,  1.7136e-07,  3.8147e-06,  2.1458e-06,\n",
       "           7.1526e-07,  1.3113e-06,  2.2650e-06,  1.4305e-06,  9.5367e-07,  2.6226e-06,  3.0994e-06,  2.1458e-06,  4.7684e-07, -5.9605e-07,  5.9605e-08,  7.7486e-07,  9.5367e-07,  2.6226e-06,\n",
       "           4.7684e-07,  0.0000e+00,  0.0000e+00,  5.9605e-07]]], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dFgs_cu - delta_fbar_pt.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matC_cu - mat_P_pt\n",
    "# matC_cu - mat_C_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [4.3366e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [8.9254e-04, 4.9177e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [5.7532e-04, 3.1699e-03, 3.5641e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [7.6602e-04, 4.2206e-03, 4.7456e-03, 3.1090e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [8.1001e-04, 4.4630e-03, 5.0181e-03, 3.2875e-02, 5.9596e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [6.0442e-04, 3.3302e-03, 3.7444e-03, 2.4531e-02, 4.4470e-02, 6.2363e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2652e-04, 1.2481e-03, 1.4033e-03, 9.1937e-03, 1.6666e-02, 2.3372e-02, 3.5437e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.0183e-04, 1.1120e-03, 1.2503e-03, 8.1914e-03, 1.4849e-02, 2.0824e-02, 3.1574e-02, 8.2969e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.7858e-04, 9.8395e-04, 1.1063e-03, 7.2480e-03, 1.3139e-02, 1.8426e-02, 2.7937e-02, 7.3413e-02, 3.1148e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [5.8178e-05, 3.2055e-04, 3.6042e-04, 2.3612e-03, 4.2804e-03, 6.0027e-03, 9.1013e-03, 2.3916e-02, 1.0147e-02, 3.9371e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.1208e-05, 6.1756e-05, 6.9437e-05, 4.5491e-04, 8.2465e-04, 1.1565e-03, 1.7534e-03, 4.6077e-03, 1.9550e-03, 7.5852e-03, 1.3301e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9800e-06, 5.4988e-05, 6.1827e-05, 4.0505e-04, 7.3427e-04, 1.0297e-03, 1.5613e-03, 4.1027e-03, 1.7407e-03, 6.7538e-03, 1.1843e-02, 6.4767e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.1144e-06, 5.0218e-05, 5.6464e-05, 3.6992e-04, 6.7058e-04, 9.4041e-04, 1.4258e-03, 3.7468e-03, 1.5897e-03, 6.1680e-03, 1.0816e-02, 5.9149e-01, 7.2321e-01, 0.0000e+00, 0.0000e+00],\n",
       "          [3.2487e-06, 1.7900e-05, 2.0126e-05, 1.3185e-04, 2.3902e-04, 3.3520e-04, 5.0823e-04, 1.3355e-03, 5.6664e-04, 2.1985e-03, 3.8552e-03, 2.1083e-01, 2.5778e-01, 3.0520e-01, 0.0000e+00],\n",
       "          [1.5167e-06, 8.3565e-06, 9.3958e-06, 6.1555e-05, 1.1159e-04, 1.5649e-04, 2.3726e-04, 6.2348e-04, 2.6453e-04, 1.0264e-03, 1.7998e-03, 9.8426e-02, 1.2034e-01, 1.4248e-01, 4.3074e-01]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matC_cu[:, :, :, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [4.3366e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [8.9254e-04, 4.9177e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [5.7532e-04, 3.1699e-03, 3.5641e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [7.6602e-04, 4.2206e-03, 4.7456e-03, 3.1090e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [8.1001e-04, 4.4630e-03, 5.0181e-03, 3.2875e-02, 5.9596e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [6.0442e-04, 3.3302e-03, 3.7444e-03, 2.4531e-02, 4.4470e-02, 6.2363e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2652e-04, 1.2481e-03, 1.4033e-03, 9.1937e-03, 1.6666e-02, 2.3372e-02, 3.5437e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.0183e-04, 1.1120e-03, 1.2503e-03, 8.1914e-03, 1.4849e-02, 2.0824e-02, 3.1574e-02, 8.2969e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.7858e-04, 9.8395e-04, 1.1063e-03, 7.2480e-03, 1.3139e-02, 1.8426e-02, 2.7937e-02, 7.3413e-02, 1.0456e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [5.8178e-05, 3.2055e-04, 3.6042e-04, 2.3612e-03, 4.2804e-03, 6.0027e-03, 9.1013e-03, 2.3916e-02, 3.4064e-02, 6.3287e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.1208e-05, 6.1756e-05, 6.9437e-05, 4.5491e-04, 8.2465e-04, 1.1565e-03, 1.7534e-03, 4.6076e-03, 6.5626e-03, 1.2193e-02, 1.7908e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9800e-06, 5.4988e-05, 6.1827e-05, 4.0505e-04, 7.3427e-04, 1.0297e-03, 1.5613e-03, 4.1026e-03, 5.8433e-03, 1.0856e-02, 1.5946e-02, 6.5177e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.1144e-06, 5.0218e-05, 5.6464e-05, 3.6992e-04, 6.7058e-04, 9.4040e-04, 1.4258e-03, 3.7468e-03, 5.3365e-03, 9.9148e-03, 1.4563e-02, 5.9524e-01, 7.2696e-01, 0.0000e+00, 0.0000e+00],\n",
       "          [3.2487e-06, 1.7900e-05, 2.0126e-05, 1.3185e-04, 2.3902e-04, 3.3520e-04, 5.0823e-04, 1.3355e-03, 1.9022e-03, 3.5341e-03, 5.1907e-03, 2.1217e-01, 2.5912e-01, 3.0654e-01, 0.0000e+00],\n",
       "          [1.5167e-06, 8.3565e-06, 9.3958e-06, 6.1555e-05, 1.1159e-04, 1.5649e-04, 2.3726e-04, 6.2348e-04, 8.8801e-04, 1.6499e-03, 2.4233e-03, 9.9049e-02, 1.2097e-01, 1.4311e-01, 4.3136e-01]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_Dtilde_pt.cumsum(-1).tril(-1)[:, :, :, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-2.0373e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-2.3283e-10, -1.8626e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-5.8208e-10, -3.9581e-09, -4.4238e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-4.0745e-10, -2.7940e-09, -3.2596e-09, -2.4214e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-2.9104e-10, -2.3283e-09, -2.7940e-09, -1.8626e-08, -2.9802e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-4.6566e-10, -2.3283e-09, -2.3283e-09, -1.3039e-08, -2.2352e-08, -3.3528e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-1.1642e-10, -6.9849e-10, -9.3132e-10, -4.6566e-09, -9.3132e-09, -1.4901e-08, -1.8626e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-2.9104e-11, -2.3283e-10, -4.6566e-10, -1.8626e-09, -2.7940e-09, -3.7253e-09, -3.7253e-09, -1.4901e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-7.2760e-11, -1.1642e-09, -1.2806e-09, -2.7940e-09, -3.7253e-09, -5.5879e-09, -5.5879e-09, -1.4901e-08,  7.3413e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-2.9104e-11, -3.2014e-10, -3.7835e-10, -9.3132e-10, -1.3970e-09, -2.3283e-09, -2.7940e-09, -5.5879e-09,  2.3916e-02,  2.3916e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-4.0018e-11, -2.9104e-10, -3.2742e-10, -1.8044e-09, -3.0268e-09, -4.3074e-09, -6.5193e-09, -1.6298e-08,  4.6076e-03,  4.6076e-03,  4.6076e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-2.8194e-11, -2.4374e-10, -2.6921e-10, -1.3679e-09, -2.5029e-09, -3.4925e-09, -5.4715e-09, -1.3504e-08,  4.1026e-03,  4.1026e-03,  4.1026e-03,  4.1006e-03,  0.0000e+00,  0.0000e+00],\n",
       "          [-2.0009e-11, -1.6371e-10, -1.8190e-10, -9.8953e-10, -1.6298e-09, -2.2701e-09, -3.4925e-09, -8.3819e-09,  3.7468e-03,  3.7468e-03,  3.7468e-03,  3.7454e-03,  3.7453e-03,  0.0000e+00],\n",
       "          [-2.9559e-12, -3.0923e-11, -3.6380e-11, -1.6007e-10, -3.6380e-10, -5.5297e-10, -7.5670e-10, -1.6298e-09,  1.3355e-03,  1.3355e-03,  1.3355e-03,  1.3352e-03,  1.3351e-03,  1.3351e-03],\n",
       "          [-1.5916e-12, -1.0914e-11, -1.2733e-11, -9.4587e-11, -1.8190e-10, -2.4738e-10, -3.7835e-10, -5.8208e-10,  6.2348e-04,  6.2348e-04,  6.2348e-04,  6.2340e-04,  6.2338e-04,  6.2336e-04]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_Dtilde_pt.cumsum(-1).tril(-1) - matC_cu)[:,:,:,:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0048, 0.0240, 0.0214, 0.1169, 0.1556, 0.1346, 0.1095, 0.1947, 0.0474, 0.0631, 0.0416, 1.5484, 1.1013, 0.4477, 0.4307]]], device='cuda:0'),\n",
       " tensor([[[0.0000, 0.0048, 0.0240, 0.0214, 0.1169, 0.1556, 0.1346, 0.1095, 0.1947, 0.1592, 0.1014, 0.0560, 1.5582, 1.1070, 0.4496, 0.4314]]], device='cuda:0'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dFgs_cu, delta_fbar_pt.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  2.7940e-09,  1.8626e-08,  1.6764e-08,  7.4506e-08,  7.4506e-08,  4.4703e-08,  5.9605e-08,  8.9407e-08,  1.0431e-07,  1.3411e-07,  1.7136e-07,  3.8147e-06,  2.1458e-06,\n",
       "           7.1526e-07,  1.3113e-06,  2.2650e-06,  1.4305e-06,  9.5367e-07,  2.6226e-06,  3.0994e-06,  2.1458e-06,  4.7684e-07, -5.9605e-07,  5.9605e-08,  7.7486e-07,  9.5367e-07,  2.6226e-06,\n",
       "           4.7684e-07,  0.0000e+00,  0.0000e+00,  5.9605e-07]]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dFgs_cu - delta_fbar_pt.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 16]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaDcsVec_cu, deltaDcsVec_cu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bw kernel match through autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgs_pt = fgs.clone().detach().requires_grad_(True)\n",
    "igs_pt = igs.clone().detach().requires_grad_(True)\n",
    "qs_pt = qs.clone().detach().requires_grad_(True)\n",
    "ks_pt = ks.clone().detach().requires_grad_(True)\n",
    "vs_pt = vs.clone().detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch version QKV product (To test if this is still correct after changes to the code.)\n",
    "# at some point we have to compare to the vlstm_fw_torch version.\n",
    "# rs = qs @ ks.transpose(-1, -2) @ vs\n",
    "# rs, rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs_torch = (qs @ ks.transpose(-1, -2) * torch.tril(torch.ones((B, NH, S, S))).to(device=DEVICE, dtype=DTYPE)) @ vs\n",
    "# rs_torch, rs_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05],\n",
       "           [2.5971e-04, 2.5971e-04, 2.5971e-04, 2.5971e-04, 2.5971e-04, 2.5971e-04, 2.5971e-04, 2.5971e-04],\n",
       "           [3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04],\n",
       "           [1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03],\n",
       "           [1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03],\n",
       "           [2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03],\n",
       "           [2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03],\n",
       "           [1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03],\n",
       "           [2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03],\n",
       "           [3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03],\n",
       "           [1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02]]]], device='cuda:0', grad_fn=<vLSTMParallelFwBwWithGroupNormBackward>),\n",
       " torch.Size([1, 1, 16, 8]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch version\n",
    "hs_pt, n_pt, m_pt = vlstm_fwbw_torch_obw(\n",
    "    queries=qs_pt,\n",
    "    keys=ks_pt,\n",
    "    values=vs_pt,\n",
    "    igate_preact=igs_pt,\n",
    "    fgate_preact=fgs_pt,\n",
    ")\n",
    "hs_pt, hs_pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgs_cu = fgs.clone().detach().requires_grad_(True)\n",
    "igs_cu = igs.clone().detach().requires_grad_(True)\n",
    "qs_cu = qs.clone().detach().requires_grad_(True)\n",
    "ks_cu = ks.clone().detach().requires_grad_(True)\n",
    "vs_cu = vs.clone().detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - float32!\n",
      "B: 1, NH: 1, S: 16, DH: 8\n",
      "blocksxy: 1-2, threadsxy: 4-4, shared_mem in bytes: 5700\n",
      "In FW-Kernel: gdim.x: 1, gdim.y: 2, gdim.z: 1, bdim.x: 4, bdim.y: 4\n",
      "In FW-Kernel: QtileDim: 8, KVtileDim: 8, TblockDim:4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05],\n",
       "           [2.5972e-04, 2.5972e-04, 2.5972e-04, 2.5972e-04, 2.5972e-04, 2.5972e-04, 2.5972e-04, 2.5972e-04],\n",
       "           [3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04],\n",
       "           [1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03],\n",
       "           [1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03],\n",
       "           [2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03],\n",
       "           [2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03],\n",
       "           [1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03],\n",
       "           [2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03],\n",
       "           [3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03],\n",
       "           [1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02]]]], device='cuda:0', grad_fn=<vLSTMParallelFwBwCudaBackward>),\n",
       " torch.Size([1, 1, 16, 8]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cuda kernel\n",
    "hs_cu, n_cu, m_cu, matD_cu = vlstm_fwbw_cuda(mat_Q=qs_cu, mat_K=ks_cu, mat_V=vs_cu, igate_preact=igs_cu.squeeze(-1), fgate_preact=fgs_cu.squeeze(-1))\n",
    "hs_cu, hs_cu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.4552e-11, -1.4552e-11, -1.4552e-11, -1.4552e-11, -1.4552e-11, -1.4552e-11, -1.4552e-11, -1.4552e-11],\n",
       "          [-1.4552e-10, -1.4552e-10, -1.4552e-10, -1.4552e-10, -1.4552e-10, -1.4552e-10, -1.4552e-10, -1.4552e-10],\n",
       "          [-1.1642e-10, -1.1642e-10, -1.1642e-10, -1.1642e-10, -1.1642e-10, -1.1642e-10, -1.1642e-10, -1.1642e-10],\n",
       "          [-1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09],\n",
       "          [-1.6298e-09, -1.6298e-09, -1.6298e-09, -1.6298e-09, -1.6298e-09, -1.6298e-09, -1.6298e-09, -1.6298e-09],\n",
       "          [-6.9849e-10, -6.9849e-10, -6.9849e-10, -6.9849e-10, -6.9849e-10, -6.9849e-10, -6.9849e-10, -6.9849e-10],\n",
       "          [-1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09],\n",
       "          [-1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09],\n",
       "          [-2.3283e-10, -2.3283e-10, -2.3283e-10, -2.3283e-10, -2.3283e-10, -2.3283e-10, -2.3283e-10, -2.3283e-10],\n",
       "          [-1.8626e-09, -1.8626e-09, -1.8626e-09, -1.8626e-09, -1.8626e-09, -1.8626e-09, -1.8626e-09, -1.8626e-09],\n",
       "          [-5.8208e-10, -5.8208e-10, -5.8208e-10, -5.8208e-10, -5.8208e-10, -5.8208e-10, -5.8208e-10, -5.8208e-10],\n",
       "          [-3.8184e-08, -3.8184e-08, -3.8184e-08, -3.8184e-08, -3.8184e-08, -3.8184e-08, -3.8184e-08, -3.8184e-08],\n",
       "          [-2.8871e-08, -2.8871e-08, -2.8871e-08, -2.8871e-08, -2.8871e-08, -2.8871e-08, -2.8871e-08, -2.8871e-08],\n",
       "          [-2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08],\n",
       "          [-2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08],\n",
       "          [-1.5832e-08, -1.5832e-08, -1.5832e-08, -1.5832e-08, -1.5832e-08, -1.5832e-08, -1.5832e-08, -1.5832e-08]]]], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward match\n",
    "hs_pt - hs_cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhs_pt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/autograd/function.py:289\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/torch_impl.py:459\u001b[0m, in \u001b[0;36mvLSTMParallelFwBwWithGroupNorm.backward\u001b[0;34m(ctx, delta_Htilde, grad_var_n_unused, grad_var_m_unused)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\n\u001b[1;32m    451\u001b[0m     ctx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    454\u001b[0m     grad_var_m_unused: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    455\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m    456\u001b[0m     (queries, keys, values, igate_preact, fgate_preact, var_n, var_m) \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    457\u001b[0m         ctx\u001b[38;5;241m.\u001b[39msaved_tensors\n\u001b[1;32m    458\u001b[0m     )\n\u001b[0;32m--> 459\u001b[0m     delta_Q, delta_K, delta_V, delta_i, delta_f \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    460\u001b[0m         vlstm_parallel_bw_torch_w_groupnorm(\n\u001b[1;32m    461\u001b[0m             delta_Htilde\u001b[38;5;241m=\u001b[39mdelta_Htilde,\n\u001b[1;32m    462\u001b[0m             queries\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[1;32m    463\u001b[0m             keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[1;32m    464\u001b[0m             values\u001b[38;5;241m=\u001b[39mvalues,\n\u001b[1;32m    465\u001b[0m             igate_preact\u001b[38;5;241m=\u001b[39migate_preact,\n\u001b[1;32m    466\u001b[0m             fgate_preact\u001b[38;5;241m=\u001b[39mfgate_preact,\n\u001b[1;32m    467\u001b[0m             var_n\u001b[38;5;241m=\u001b[39mvar_n,\n\u001b[1;32m    468\u001b[0m             var_m\u001b[38;5;241m=\u001b[39mvar_m,\n\u001b[1;32m    469\u001b[0m         )\n\u001b[1;32m    470\u001b[0m     )\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m delta_Q, delta_K, delta_V, delta_i, delta_f, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 5)"
     ]
    }
   ],
   "source": [
    "hs_pt.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
       "           [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
       "           [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
       "           [0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
       "           [0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
       "           [0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
       "           [0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
       "           [0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
       "           [0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
       "           [0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
       "           [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
       "           [0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
       "           [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
       "           [0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
       "           [0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
       "           [0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008]]]], device='cuda:0'),\n",
       " tensor([[[[0.0174, 0.0195, 0.0216, 0.0237, 0.0258, 0.0279, 0.0300, 0.0321],\n",
       "           [0.0785, 0.0829, 0.0873, 0.0917, 0.0961, 0.1005, 0.1049, 0.1093],\n",
       "           [0.0097, 0.0101, 0.0105, 0.0108, 0.0112, 0.0116, 0.0120, 0.0124],\n",
       "           [0.3243, 0.3337, 0.3430, 0.3524, 0.3618, 0.3711, 0.3805, 0.3898],\n",
       "           [0.2358, 0.2417, 0.2475, 0.2533, 0.2592, 0.2650, 0.2708, 0.2767],\n",
       "           [0.1397, 0.1426, 0.1456, 0.1486, 0.1516, 0.1546, 0.1576, 0.1606],\n",
       "           [0.1373, 0.1398, 0.1423, 0.1448, 0.1474, 0.1499, 0.1524, 0.1549],\n",
       "           [0.3080, 0.3129, 0.3177, 0.3225, 0.3273, 0.3321, 0.3370, 0.3418],\n",
       "           [0.1329, 0.1348, 0.1367, 0.1386, 0.1405, 0.1424, 0.1443, 0.1462],\n",
       "           [0.2046, 0.2073, 0.2101, 0.2128, 0.2155, 0.2182, 0.2209, 0.2236],\n",
       "           [0.0640, 0.0647, 0.0655, 0.0662, 0.0670, 0.0677, 0.0685, 0.0692],\n",
       "           [2.6744, 2.7021, 2.7298, 2.7575, 2.7852, 2.8129, 2.8406, 2.8683],\n",
       "           [0.3938, 0.3977, 0.4015, 0.4054, 0.4093, 0.4131, 0.4170, 0.4209],\n",
       "           [0.2212, 0.2233, 0.2253, 0.2274, 0.2295, 0.2315, 0.2336, 0.2357],\n",
       "           [0.9315, 0.9396, 0.9478, 0.9559, 0.9641, 0.9722, 0.9804, 0.9885],\n",
       "           [0.5474, 0.5520, 0.5566, 0.5611, 0.5657, 0.5702, 0.5748, 0.5794]]]], device='cuda:0'),\n",
       " tensor([[[[0.0247, 0.0247, 0.0247, 0.0247, 0.0247, 0.0247, 0.0247, 0.0247],\n",
       "           [0.0939, 0.0939, 0.0939, 0.0939, 0.0939, 0.0939, 0.0939, 0.0939],\n",
       "           [0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110],\n",
       "           [0.3571, 0.3571, 0.3571, 0.3571, 0.3571, 0.3571, 0.3571, 0.3571],\n",
       "           [0.2562, 0.2562, 0.2562, 0.2562, 0.2562, 0.2562, 0.2562, 0.2562],\n",
       "           [0.1501, 0.1501, 0.1501, 0.1501, 0.1501, 0.1501, 0.1501, 0.1501],\n",
       "           [0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461],\n",
       "           [0.3249, 0.3249, 0.3249, 0.3249, 0.3249, 0.3249, 0.3249, 0.3249],\n",
       "           [0.1395, 0.1395, 0.1395, 0.1395, 0.1395, 0.1395, 0.1395, 0.1395],\n",
       "           [0.2141, 0.2141, 0.2141, 0.2141, 0.2141, 0.2141, 0.2141, 0.2141],\n",
       "           [0.0666, 0.0666, 0.0666, 0.0666, 0.0666, 0.0666, 0.0666, 0.0666],\n",
       "           [2.7713, 2.7713, 2.7713, 2.7713, 2.7713, 2.7713, 2.7713, 2.7713],\n",
       "           [0.4073, 0.4073, 0.4073, 0.4073, 0.4073, 0.4073, 0.4073, 0.4073],\n",
       "           [0.2284, 0.2284, 0.2284, 0.2284, 0.2284, 0.2284, 0.2284, 0.2284],\n",
       "           [0.9600, 0.9600, 0.9600, 0.9600, 0.9600, 0.9600, 0.9600, 0.9600],\n",
       "           [0.5634, 0.5634, 0.5634, 0.5634, 0.5634, 0.5634, 0.5634, 0.5634]]]], device='cuda:0'),\n",
       " tensor([[[[0.0020],\n",
       "           [0.0075],\n",
       "           [0.0009],\n",
       "           [0.0286],\n",
       "           [0.0205],\n",
       "           [0.0120],\n",
       "           [0.0117],\n",
       "           [0.0260],\n",
       "           [0.0112],\n",
       "           [0.0171],\n",
       "           [0.0053],\n",
       "           [0.2217],\n",
       "           [0.0326],\n",
       "           [0.0183],\n",
       "           [0.0768],\n",
       "           [0.0451]]]], device='cuda:0'),\n",
       " tensor([[[[0.0000],\n",
       "           [0.0011],\n",
       "           [0.0020],\n",
       "           [0.0035],\n",
       "           [0.0051],\n",
       "           [0.0088],\n",
       "           [0.0106],\n",
       "           [0.0115],\n",
       "           [0.0081],\n",
       "           [0.0058],\n",
       "           [0.0083],\n",
       "           [0.0033],\n",
       "           [0.0553],\n",
       "           [0.0396],\n",
       "           [0.0254],\n",
       "           [0.0186]]]], device='cuda:0'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs_pt.grad, ks_pt.grad, vs_pt.grad, igs_pt.grad, fgs_pt.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - float32!\n",
      "B: 1, NH: 1, S: 16, DH: 8\n",
      "blocksxy: 1-2, threadsxy: 4-4, shared_mem in bytes: 7136\n",
      "In BW-Kernel: gdim.x: 1, gdim.y: 2, gdim.z: 1, bdim.x: 4, bdim.y: 4\n",
      "In BW-Kernel: QtileDim: 8, KVtileDim: 8, TblockDim:4\n"
     ]
    }
   ],
   "source": [
    "hs_cu.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.]]]], device='cuda:0'),\n",
       " tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.]]]], device='cuda:0'),\n",
       " tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.]]]], device='cuda:0'),\n",
       " tensor([[[[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]]]], device='cuda:0'),\n",
       " tensor([[[[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]]]], device='cuda:0'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs_cu.grad, ks_cu.grad, vs_cu.grad, igs_cu.grad, fgs_cu.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstmpt21cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
