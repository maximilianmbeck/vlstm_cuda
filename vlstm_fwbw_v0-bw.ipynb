{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "sys.path.append(\".\")\n",
    "# os.environ[\"MAX_JOBS\"] = \"100\"\n",
    "\n",
    "import torch\n",
    "torch.set_printoptions(linewidth=200)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCLUDE: ['/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include', '/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/torch/csrc/api/include', '/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/TH', '/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/THC', '/home/max/miniconda3/envs/xlstmpt220cu121/include']\n",
      "/home/max/myrepos/vlstm_cuda/src\n",
      "/home/max/cpplibs/libtorch/lib:/usr/local/cuda-12.3/lib64:\n",
      "/home/max/miniconda3/envs/xlstmpt220cu121/lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/max/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/max/.cache/torch_extensions/py311_cu121/vlstm_fwbw_v0/build.ninja...\n",
      "Building extension module vlstm_fwbw_v0...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] /home/max/miniconda3/envs/xlstmpt220cu121/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel_bw.cuda.o.d -ccbin /home/max/miniconda3/envs/xlstmpt220cu121/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=vlstm_fwbw_v0 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include -isystem /home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/TH -isystem /home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/THC -isystem /home/max/miniconda3/envs/xlstmpt220cu121/include -isystem /home/max/miniconda3/envs/xlstmpt220cu121/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -gencode arch=compute_80,code=compute_80 -res-usage --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=13407807929942597099574024998205846127479365820592393377723561443721764030073546976801874298166903427690031858186486050853753882811946569946433649006084096 -std=c++17 -c /home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/kernel_bw.cu -o kernel_bw.cuda.o \n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh(264): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh:264 CUDART_VERSION: 12010, arch: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh\" \":\" \"264\" \" CUDART_VERSION: \" \"12010\" \", arch: \" \"890\")\n",
      "                                                                                                                                                 ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh(269): remark #20200-D: #pragma message: \"INCLUDING FP16\"\n",
      "  #pragma message(\"INCLUDING FP16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                                    ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                                     ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh(282): remark #20200-D: #pragma message: \"INCLUDING BF16\"\n",
      "  #pragma message(\"INCLUDING BF16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_bf16.cuh:29 CUDART_VERSION with BF16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with BF16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                                    ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2bf16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                                     ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/kernel_bw.cu(336): warning #177-D: variable \"sTileYdimBlockYIdx\" was declared but never referenced\n",
      "          const uint sTileYdimBlockYIdx = sTileYdimGridYIdx;\n",
      "                     ^\n",
      "          detected during instantiation of \"void vlstm::kernel_dispatchers::vlstm_bw_dispatch(scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, int, int, int, int) [with scalar_t=__nv_bfloat16]\" at line 547\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/kernel_bw.cu(236): warning #177-D: variable \"sTileXdimBlockYIdx\" was declared but never referenced\n",
      "        const uint sTileXdimBlockYIdx =\n",
      "                   ^\n",
      "          detected during instantiation of \"void vlstm::kernel_dispatchers::vlstm_bw_dispatch(scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, int, int, int, int) [with scalar_t=__nv_bfloat16]\" at line 547\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/kernel_bw.cu(201): warning #177-D: variable \"batchHeadGridXGlobalMemIdxCD\" was declared but never referenced\n",
      "      uint batchHeadGridXGlobalMemIdxCD =\n",
      "           ^\n",
      "          detected during instantiation of \"void vlstm::kernel_dispatchers::vlstm_bw_dispatch(scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, int, int, int, int) [with scalar_t=__nv_bfloat16]\" at line 547\n",
      "\n",
      "ptxas info    : 150 bytes gmem\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels8vlstm_bwIfLi4ELi8ELi8EEEvPT_S3_S3_S3_S3_S3_S3_S3_S3_S3_S3_S3_S3_iiii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels8vlstm_bwIfLi4ELi8ELi8EEEvPT_S3_S3_S3_S3_S3_S3_S3_S3_S3_S3_S3_S3_iiii\n",
      "    24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 472 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels8vlstm_bwI6__halfLi4ELi8ELi8EEEvPT_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_iiii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels8vlstm_bwI6__halfLi4ELi8ELi8EEEvPT_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_iiii\n",
      "    24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 472 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm7kernels8vlstm_bwI13__nv_bfloat16Li4ELi8ELi8EEEvPT_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_iiii' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm7kernels8vlstm_bwI13__nv_bfloat16Li4ELi8ELi8EEEvPT_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_iiii\n",
      "    24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 472 bytes cmem[0]\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh(264): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh:264 CUDART_VERSION: 12010, arch: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh\" \":\" \"264\" \" CUDART_VERSION: \" \"12010\" \", arch: \" \"800\")\n",
      "                                                                                                                                                 ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh(269): remark #20200-D: #pragma message: \"INCLUDING FP16\"\n",
      "  #pragma message(\"INCLUDING FP16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                                    ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                                     ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh(282): remark #20200-D: #pragma message: \"INCLUDING BF16\"\n",
      "  #pragma message(\"INCLUDING BF16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_bf16.cuh:29 CUDART_VERSION with BF16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with BF16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                                    ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2bf16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops_2bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                                     ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/kernel_bw.cu(336): warning #177-D: variable \"sTileYdimBlockYIdx\" was declared but never referenced\n",
      "          const uint sTileYdimBlockYIdx = sTileYdimGridYIdx;\n",
      "                     ^\n",
      "          detected during instantiation of \"void vlstm::kernel_dispatchers::vlstm_bw_dispatch(scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, int, int, int, int) [with scalar_t=__nv_bfloat16]\" at line 547\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/kernel_bw.cu(236): warning #177-D: variable \"sTileXdimBlockYIdx\" was declared but never referenced\n",
      "        const uint sTileXdimBlockYIdx =\n",
      "                   ^\n",
      "          detected during instantiation of \"void vlstm::kernel_dispatchers::vlstm_bw_dispatch(scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, int, int, int, int) [with scalar_t=__nv_bfloat16]\" at line 547\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/kernel_bw.cu(201): warning #177-D: variable \"batchHeadGridXGlobalMemIdxCD\" was declared but never referenced\n",
      "      uint batchHeadGridXGlobalMemIdxCD =\n",
      "           ^\n",
      "          detected during instantiation of \"void vlstm::kernel_dispatchers::vlstm_bw_dispatch(scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, scalar_t *, int, int, int, int) [with scalar_t=__nv_bfloat16]\" at line 547\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh:264:138: note: '#pragma message: /home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh:264 CUDART_VERSION: 12010, arch: __CUDA_ARCH__'\n",
      "  264 | #pragma message(AT \" CUDART_VERSION: \" TOSTRING(                               \\\n",
      "      |                                                                                                                                          ^\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh:276:90: note: '#pragma message: SKIPPING FP16, because of CUDART_VERSION: 12010, arch: __CUDA_ARCH__'\n",
      "  276 | #pragma message(\"SKIPPING FP16, because of CUDART_VERSION: \" TOSTRING(         \\\n",
      "      |                                                                                          ^\n",
      "/home/max/myrepos/vlstm_cuda/src/vlstm_fwbw_v0/../util/inline_ops.cuh:289:90: note: '#pragma message: SKIPPING BF16, because of CUDART_VERSION: 12010, arch: __CUDA_ARCH__'\n",
      "  289 | #pragma message(\"SKIPPING BF16, because of CUDART_VERSION: \" TOSTRING(         \\\n",
      "      |                                                                                          ^\n",
      "[2/2] /home/max/miniconda3/envs/xlstmpt220cu121/bin/x86_64-conda-linux-gnu-c++ interface.o kernel_fw.cuda.o kernel_bw.cuda.o -shared -L/home/max/miniconda3/envs/xlstmpt220cu121/lib -lcublas -L/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/home/max/miniconda3/envs/xlstmpt220cu121/lib -lcudart -o vlstm_fwbw_v0.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module vlstm_fwbw_v0...\n"
     ]
    }
   ],
   "source": [
    "from src.vlstm_fwbw_v0.interface import vlstm_fwbw_torch_obw, vlstm_fwbw_cuda\n",
    "\n",
    "from src.vlstm_fwbw_v0.interface import vlstm_fw_torch, vlstm_fw_cuda\n",
    "from src.vlstm_fwbw_v0.interface import vlstm_bw_torch_obw, vlstm_bw_cuda\n",
    "from src.vlstm_fwbw_v0.torch_impl import vlstm_fw_tiled_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA vLSTM backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "S = 16 #16 #8 # seq len\n",
    "B = 1 # batch size\n",
    "NH = 1 # num heads\n",
    "DH = 8 # dim per head\n",
    "DTYPE = torch.float32\n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.0000,  0.1000,  0.2000,  0.3000,  0.4000,  0.5000,  0.6000,  0.7000],\n",
       "           [ 0.8000,  0.9000,  1.0000,  1.1000,  1.2000,  1.3000,  1.4000,  1.5000],\n",
       "           [ 1.6000,  1.7000,  1.8000,  1.9000,  2.0000,  2.1000,  2.2000,  2.3000],\n",
       "           [ 2.4000,  2.5000,  2.6000,  2.7000,  2.8000,  2.9000,  3.0000,  3.1000],\n",
       "           [ 3.2000,  3.3000,  3.4000,  3.5000,  3.6000,  3.7000,  3.8000,  3.9000],\n",
       "           [ 4.0000,  4.1000,  4.2000,  4.3000,  4.4000,  4.5000,  4.6000,  4.7000],\n",
       "           [ 4.8000,  4.9000,  5.0000,  5.1000,  5.2000,  5.3000,  5.4000,  5.5000],\n",
       "           [ 5.6000,  5.7000,  5.8000,  5.9000,  6.0000,  6.1000,  6.2000,  6.3000],\n",
       "           [ 6.4000,  6.5000,  6.6000,  6.7000,  6.8000,  6.9000,  7.0000,  7.1000],\n",
       "           [ 7.2000,  7.3000,  7.4000,  7.5000,  7.6000,  7.7000,  7.8000,  7.9000],\n",
       "           [ 8.0000,  8.1000,  8.2000,  8.3000,  8.4000,  8.5000,  8.6000,  8.7000],\n",
       "           [ 8.8000,  8.9000,  9.0000,  9.1000,  9.2000,  9.3000,  9.4000,  9.5000],\n",
       "           [ 9.6000,  9.7000,  9.8000,  9.9000, 10.0000, 10.1000, 10.2000, 10.3000],\n",
       "           [10.4000, 10.5000, 10.6000, 10.7000, 10.8000, 10.9000, 11.0000, 11.1000],\n",
       "           [11.2000, 11.3000, 11.4000, 11.5000, 11.6000, 11.7000, 11.8000, 11.9000],\n",
       "           [12.0000, 12.1000, 12.2000, 12.3000, 12.4000, 12.5000, 12.6000, 12.7000]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 16, 8]),\n",
       " 128)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create qkv, inputgates, forgetgates \n",
    "torch.manual_seed(0)\n",
    "qs = torch.arange((B*NH*S*DH), device=DEVICE, dtype=DTYPE).reshape((B, NH, S, DH)) / 10.\n",
    "ks = torch.ones((B, NH, S, DH), device=DEVICE, dtype=DTYPE) / 100.\n",
    "vs = torch.ones((B, NH, S, DH), device=DEVICE, dtype=DTYPE) / 100.\n",
    "# igs = (1. + torch.arange((B * NH * S), device=DEVICE, dtype=DTYPE)).reshape(B, NH, S, 1) / 10.\n",
    "# igs = torch.zeros((B, NH, S, 1), device=DEVICE, dtype=DTYPE) #/ 10.\n",
    "igs = torch.randn((B, NH, S, 1), device=DEVICE, dtype=DTYPE) #/ 10.\n",
    "fgs = torch.ones((B, NH, S, 1), device=DEVICE, dtype=DTYPE)\n",
    "fgs = torch.randn((B, NH, S, 1), device=DEVICE, dtype=DTYPE)\n",
    "\n",
    "dHs = qs.clone()\n",
    "qs, qs.shape, len(qs.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bw kernel match direct call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_pt, n_pt, m_pt, _, _ = vlstm_fw_torch(queries=qs, keys=ks, values=vs, igate_preact=igs, fgate_preact=fgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dQs_pt, dKs_pt, dVs_pt, dIgs_pt, dFgs_pt = vlstm_bw_torch_obw(\n",
    "    delta_Htilde=dHs,\n",
    "    queries=qs,\n",
    "    keys=ks,\n",
    "    values=vs,\n",
    "    igate_preact=igs,\n",
    "    fgate_preact=fgs,\n",
    "    var_n=n_pt,\n",
    "    var_m=m_pt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05],\n",
       "          [2.5971e-04, 2.5971e-04, 2.5971e-04, 2.5971e-04, 2.5971e-04, 2.5971e-04, 2.5971e-04, 2.5971e-04],\n",
       "          [3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04],\n",
       "          [1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03],\n",
       "          [1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03],\n",
       "          [2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03],\n",
       "          [2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03],\n",
       "          [1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03],\n",
       "          [2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03],\n",
       "          [3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03],\n",
       "          [1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03],\n",
       "          [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "          [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "          [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "          [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "          [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02]]]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dQs_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - float32!\n",
      "B: 1, NH: 1, S: 16, DH: 8\n",
      "blocksxy: 1-2, threadsxy: 4-4, shared_mem in bytes: 5700\n",
      "In FW-Kernel: gdim.x: 1, gdim.y: 2, gdim.z: 1, bdim.x: 4, bdim.y: 4\n",
      "In FW-Kernel: QtileDim: 8, KVtileDim: 8, TblockDim:4\n"
     ]
    }
   ],
   "source": [
    "hs_cu, n_cu, m_cu, _ = vlstm_fw_cuda(mat_Q=qs, mat_K=ks, mat_V=vs, igate_preact=igs, fgate_preact=fgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - float32!\n",
      "B: 1, NH: 1, S: 16, DH: 8\n",
      "blocksxy: 1-2, threadsxy: 4-4, shared_mem in bytes: 7136\n",
      "In BW-Kernel: gdim.x: 1, gdim.y: 2, gdim.z: 1, bdim.x: 4, bdim.y: 4\n",
      "In BW-Kernel: QtileDim: 8, KVtileDim: 8, TblockDim:4\n"
     ]
    }
   ],
   "source": [
    "dQs_cu, dKs_cu, dVs_cu, dIgs_cu, dFgs_cu = vlstm_bw_cuda(\n",
    "    delta_Htilde=dHs,\n",
    "    mat_Q=qs,\n",
    "    mat_K=ks,\n",
    "    mat_V=vs,\n",
    "    igate_preact=igs,\n",
    "    fgate_preact=fgs,\n",
    "    n=n_pt,\n",
    "    m=m_pt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.]]]], device='cuda:0'),\n",
       " torch.Size([1, 1, 16, 8]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dQs_cu, dQs_cu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bw kernel match through autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgs_pt = fgs.clone().detach().requires_grad_(True)\n",
    "igs_pt = igs.clone().detach().requires_grad_(True)\n",
    "qs_pt = qs.clone().detach().requires_grad_(True)\n",
    "ks_pt = ks.clone().detach().requires_grad_(True)\n",
    "vs_pt = vs.clone().detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch version QKV product (To test if this is still correct after changes to the code.)\n",
    "# at some point we have to compare to the vlstm_fw_torch version.\n",
    "# rs = qs @ ks.transpose(-1, -2) @ vs\n",
    "# rs, rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs_torch = (qs @ ks.transpose(-1, -2) * torch.tril(torch.ones((B, NH, S, S))).to(device=DEVICE, dtype=DTYPE)) @ vs\n",
    "# rs_torch, rs_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05],\n",
       "           [2.5971e-04, 2.5971e-04, 2.5971e-04, 2.5971e-04, 2.5971e-04, 2.5971e-04, 2.5971e-04, 2.5971e-04],\n",
       "           [3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04],\n",
       "           [1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03],\n",
       "           [1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03],\n",
       "           [2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03],\n",
       "           [2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03],\n",
       "           [1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03],\n",
       "           [2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03],\n",
       "           [3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03],\n",
       "           [1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02]]]], device='cuda:0', grad_fn=<vLSTMParallelFwBwWithGroupNormBackward>),\n",
       " torch.Size([1, 1, 16, 8]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch version\n",
    "hs_pt, n_pt, m_pt = vlstm_fwbw_torch_obw(\n",
    "    queries=qs_pt,\n",
    "    keys=ks_pt,\n",
    "    values=vs_pt,\n",
    "    igate_preact=igs_pt,\n",
    "    fgate_preact=fgs_pt,\n",
    ")\n",
    "hs_pt, hs_pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgs_cu = fgs.clone().detach().requires_grad_(True)\n",
    "igs_cu = igs.clone().detach().requires_grad_(True)\n",
    "qs_cu = qs.clone().detach().requires_grad_(True)\n",
    "ks_cu = ks.clone().detach().requires_grad_(True)\n",
    "vs_cu = vs.clone().detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - float32!\n",
      "B: 1, NH: 1, S: 16, DH: 8\n",
      "blocksxy: 1-2, threadsxy: 4-4, shared_mem in bytes: 5700\n",
      "In FW-Kernel: gdim.x: 1, gdim.y: 2, gdim.z: 1, bdim.x: 4, bdim.y: 4\n",
      "In FW-Kernel: QtileDim: 8, KVtileDim: 8, TblockDim:4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05, 3.9268e-05],\n",
       "           [2.5972e-04, 2.5972e-04, 2.5972e-04, 2.5972e-04, 2.5972e-04, 2.5972e-04, 2.5972e-04, 2.5972e-04],\n",
       "           [3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04, 3.5444e-04],\n",
       "           [1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03, 1.0614e-03],\n",
       "           [1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03, 1.9845e-03],\n",
       "           [2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03, 2.4016e-03],\n",
       "           [2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03, 2.2950e-03],\n",
       "           [1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03, 1.9563e-03],\n",
       "           [2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03, 2.1884e-03],\n",
       "           [3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03, 3.2163e-03],\n",
       "           [1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03, 1.3915e-03],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02],\n",
       "           [1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0000e-02]]]], device='cuda:0', grad_fn=<vLSTMParallelFwBwCudaBackward>),\n",
       " torch.Size([1, 1, 16, 8]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cuda kernel\n",
    "hs_cu, n_cu, m_cu, matD_cu = vlstm_fwbw_cuda(mat_Q=qs_cu, mat_K=ks_cu, mat_V=vs_cu, igate_preact=igs_cu.squeeze(-1), fgate_preact=fgs_cu.squeeze(-1))\n",
    "hs_cu, hs_cu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.4552e-11, -1.4552e-11, -1.4552e-11, -1.4552e-11, -1.4552e-11, -1.4552e-11, -1.4552e-11, -1.4552e-11],\n",
       "          [-1.4552e-10, -1.4552e-10, -1.4552e-10, -1.4552e-10, -1.4552e-10, -1.4552e-10, -1.4552e-10, -1.4552e-10],\n",
       "          [-1.1642e-10, -1.1642e-10, -1.1642e-10, -1.1642e-10, -1.1642e-10, -1.1642e-10, -1.1642e-10, -1.1642e-10],\n",
       "          [-1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09],\n",
       "          [-1.6298e-09, -1.6298e-09, -1.6298e-09, -1.6298e-09, -1.6298e-09, -1.6298e-09, -1.6298e-09, -1.6298e-09],\n",
       "          [-6.9849e-10, -6.9849e-10, -6.9849e-10, -6.9849e-10, -6.9849e-10, -6.9849e-10, -6.9849e-10, -6.9849e-10],\n",
       "          [-1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09],\n",
       "          [-1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09, -1.1642e-09],\n",
       "          [-2.3283e-10, -2.3283e-10, -2.3283e-10, -2.3283e-10, -2.3283e-10, -2.3283e-10, -2.3283e-10, -2.3283e-10],\n",
       "          [-1.8626e-09, -1.8626e-09, -1.8626e-09, -1.8626e-09, -1.8626e-09, -1.8626e-09, -1.8626e-09, -1.8626e-09],\n",
       "          [-5.8208e-10, -5.8208e-10, -5.8208e-10, -5.8208e-10, -5.8208e-10, -5.8208e-10, -5.8208e-10, -5.8208e-10],\n",
       "          [-3.8184e-08, -3.8184e-08, -3.8184e-08, -3.8184e-08, -3.8184e-08, -3.8184e-08, -3.8184e-08, -3.8184e-08],\n",
       "          [-2.8871e-08, -2.8871e-08, -2.8871e-08, -2.8871e-08, -2.8871e-08, -2.8871e-08, -2.8871e-08, -2.8871e-08],\n",
       "          [-2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08],\n",
       "          [-2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08, -2.1420e-08],\n",
       "          [-1.5832e-08, -1.5832e-08, -1.5832e-08, -1.5832e-08, -1.5832e-08, -1.5832e-08, -1.5832e-08, -1.5832e-08]]]], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward match\n",
    "hs_pt - hs_cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_pt.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
       "           [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
       "           [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
       "           [0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
       "           [0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
       "           [0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
       "           [0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
       "           [0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
       "           [0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
       "           [0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
       "           [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
       "           [0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
       "           [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
       "           [0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
       "           [0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
       "           [0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008]]]], device='cuda:0'),\n",
       " tensor([[[[0.0174, 0.0195, 0.0216, 0.0237, 0.0258, 0.0279, 0.0300, 0.0321],\n",
       "           [0.0785, 0.0829, 0.0873, 0.0917, 0.0961, 0.1005, 0.1049, 0.1093],\n",
       "           [0.0097, 0.0101, 0.0105, 0.0108, 0.0112, 0.0116, 0.0120, 0.0124],\n",
       "           [0.3243, 0.3337, 0.3430, 0.3524, 0.3618, 0.3711, 0.3805, 0.3898],\n",
       "           [0.2358, 0.2417, 0.2475, 0.2533, 0.2592, 0.2650, 0.2708, 0.2767],\n",
       "           [0.1397, 0.1426, 0.1456, 0.1486, 0.1516, 0.1546, 0.1576, 0.1606],\n",
       "           [0.1373, 0.1398, 0.1423, 0.1448, 0.1474, 0.1499, 0.1524, 0.1549],\n",
       "           [0.3080, 0.3129, 0.3177, 0.3225, 0.3273, 0.3321, 0.3370, 0.3418],\n",
       "           [0.1329, 0.1348, 0.1367, 0.1386, 0.1405, 0.1424, 0.1443, 0.1462],\n",
       "           [0.2046, 0.2073, 0.2101, 0.2128, 0.2155, 0.2182, 0.2209, 0.2236],\n",
       "           [0.0640, 0.0647, 0.0655, 0.0662, 0.0670, 0.0677, 0.0685, 0.0692],\n",
       "           [2.6744, 2.7021, 2.7298, 2.7575, 2.7852, 2.8129, 2.8406, 2.8683],\n",
       "           [0.3938, 0.3977, 0.4015, 0.4054, 0.4093, 0.4131, 0.4170, 0.4209],\n",
       "           [0.2212, 0.2233, 0.2253, 0.2274, 0.2295, 0.2315, 0.2336, 0.2357],\n",
       "           [0.9315, 0.9396, 0.9478, 0.9559, 0.9641, 0.9722, 0.9804, 0.9885],\n",
       "           [0.5474, 0.5520, 0.5566, 0.5611, 0.5657, 0.5702, 0.5748, 0.5794]]]], device='cuda:0'),\n",
       " tensor([[[[0.0247, 0.0247, 0.0247, 0.0247, 0.0247, 0.0247, 0.0247, 0.0247],\n",
       "           [0.0939, 0.0939, 0.0939, 0.0939, 0.0939, 0.0939, 0.0939, 0.0939],\n",
       "           [0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110],\n",
       "           [0.3571, 0.3571, 0.3571, 0.3571, 0.3571, 0.3571, 0.3571, 0.3571],\n",
       "           [0.2562, 0.2562, 0.2562, 0.2562, 0.2562, 0.2562, 0.2562, 0.2562],\n",
       "           [0.1501, 0.1501, 0.1501, 0.1501, 0.1501, 0.1501, 0.1501, 0.1501],\n",
       "           [0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461],\n",
       "           [0.3249, 0.3249, 0.3249, 0.3249, 0.3249, 0.3249, 0.3249, 0.3249],\n",
       "           [0.1395, 0.1395, 0.1395, 0.1395, 0.1395, 0.1395, 0.1395, 0.1395],\n",
       "           [0.2141, 0.2141, 0.2141, 0.2141, 0.2141, 0.2141, 0.2141, 0.2141],\n",
       "           [0.0666, 0.0666, 0.0666, 0.0666, 0.0666, 0.0666, 0.0666, 0.0666],\n",
       "           [2.7713, 2.7713, 2.7713, 2.7713, 2.7713, 2.7713, 2.7713, 2.7713],\n",
       "           [0.4073, 0.4073, 0.4073, 0.4073, 0.4073, 0.4073, 0.4073, 0.4073],\n",
       "           [0.2284, 0.2284, 0.2284, 0.2284, 0.2284, 0.2284, 0.2284, 0.2284],\n",
       "           [0.9600, 0.9600, 0.9600, 0.9600, 0.9600, 0.9600, 0.9600, 0.9600],\n",
       "           [0.5634, 0.5634, 0.5634, 0.5634, 0.5634, 0.5634, 0.5634, 0.5634]]]], device='cuda:0'),\n",
       " tensor([[[[0.0020],\n",
       "           [0.0075],\n",
       "           [0.0009],\n",
       "           [0.0286],\n",
       "           [0.0205],\n",
       "           [0.0120],\n",
       "           [0.0117],\n",
       "           [0.0260],\n",
       "           [0.0112],\n",
       "           [0.0171],\n",
       "           [0.0053],\n",
       "           [0.2217],\n",
       "           [0.0326],\n",
       "           [0.0183],\n",
       "           [0.0768],\n",
       "           [0.0451]]]], device='cuda:0'),\n",
       " tensor([[[[0.0000],\n",
       "           [0.0011],\n",
       "           [0.0020],\n",
       "           [0.0035],\n",
       "           [0.0051],\n",
       "           [0.0088],\n",
       "           [0.0106],\n",
       "           [0.0115],\n",
       "           [0.0081],\n",
       "           [0.0058],\n",
       "           [0.0083],\n",
       "           [0.0033],\n",
       "           [0.0553],\n",
       "           [0.0396],\n",
       "           [0.0254],\n",
       "           [0.0186]]]], device='cuda:0'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs_pt.grad, ks_pt.grad, vs_pt.grad, igs_pt.grad, fgs_pt.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - float32!\n",
      "B: 1, NH: 1, S: 16, DH: 8\n",
      "blocksxy: 1-2, threadsxy: 4-4, shared_mem in bytes: 7136\n",
      "In BW-Kernel: gdim.x: 1, gdim.y: 2, gdim.z: 1, bdim.x: 4, bdim.y: 4\n",
      "In BW-Kernel: QtileDim: 8, KVtileDim: 8, TblockDim:4\n"
     ]
    }
   ],
   "source": [
    "hs_cu.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.]]]], device='cuda:0'),\n",
       " tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.]]]], device='cuda:0'),\n",
       " tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.]]]], device='cuda:0'),\n",
       " tensor([[[[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]]]], device='cuda:0'),\n",
       " tensor([[[[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]]]], device='cuda:0'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs_cu.grad, ks_cu.grad, vs_cu.grad, igs_cu.grad, fgs_cu.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstmpt21cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
