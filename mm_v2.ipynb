{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "sys.path.append(\".\")\n",
    "# os.environ[\"MAX_JOBS\"] = \"100\"\n",
    "\n",
    "import torch\n",
    "torch.set_printoptions(linewidth=200, threshold=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCLUDE: ['/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include', '/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/torch/csrc/api/include', '/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/TH', '/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/THC', '/home/max/miniconda3/envs/xlstmpt220cu121/include']\n",
      "/home/max/myrepos/vlstm_cuda/src\n",
      "/home/max/cpplibs/libtorch/lib:/usr/local/cuda-12.3/lib64:\n",
      "/home/max/miniconda3/envs/xlstmpt220cu121/lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/max/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/max/.cache/torch_extensions/py311_cu121/mm_v2/build.ninja...\n",
      "Building extension module mm_v2...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] /home/max/miniconda3/envs/xlstmpt220cu121/bin/nvcc --generate-dependencies-with-compile --dependency-output kernels.cuda.o.d -ccbin /home/max/miniconda3/envs/xlstmpt220cu121/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=mm_v2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include -isystem /home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/TH -isystem /home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/include/THC -isystem /home/max/miniconda3/envs/xlstmpt220cu121/include -isystem /home/max/miniconda3/envs/xlstmpt220cu121/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -gencode arch=compute_80,code=compute_80 -res-usage --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=13407807929942597099574024998205846127479365820592393377723561443721764030073546976801874298166903427690031858186486050853753882811946569946433649006084096 -std=c++17 -c /home/max/myrepos/vlstm_cuda/src/mm_v2/kernels.cu -o kernels.cuda.o \n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/kernels.cu:181: warning: \"__CUDA_ARCH__\" redefined\n",
      "  181 | #define __CUDA_ARCH__ 800\n",
      "      | \n",
      "<command-line>: note: this is the location of the previous definition\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops.cuh(264): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops.cuh:264 CUDART_VERSION: 12010, arch: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops.cuh\" \":\" \"264\" \" CUDART_VERSION: \" \"12010\" \", arch: \" \"890\")\n",
      "                                                                                                                                         ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops.cuh(269): remark #20200-D: #pragma message: \"INCLUDING FP16\"\n",
      "  #pragma message(\"INCLUDING FP16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                            ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_2fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_2fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_2fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                             ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops.cuh(282): remark #20200-D: #pragma message: \"INCLUDING BF16\"\n",
      "  #pragma message(\"INCLUDING BF16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_bf16.cuh:29 CUDART_VERSION with BF16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with BF16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                            ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_2bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_2bf16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 890\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_2bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"890\")\n",
      "                                                                                                                                                             ^\n",
      "\n",
      "ptxas info    : 75 bytes gmem\n",
      "ptxas info    : Compiling entry function '_ZN5vlstm8mmkernelEPK13__nv_bfloat16S2_PKfPfff' for 'sm_89'\n",
      "ptxas info    : Function properties for _ZN5vlstm8mmkernelEPK13__nv_bfloat16S2_PKfPfff\n",
      "    16 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 156 registers, 392 bytes cmem[0]\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops.cuh(264): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops.cuh:264 CUDART_VERSION: 12010, arch: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops.cuh\" \":\" \"264\" \" CUDART_VERSION: \" \"12010\" \", arch: \" \"800\")\n",
      "                                                                                                                                         ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops.cuh(269): remark #20200-D: #pragma message: \"INCLUDING FP16\"\n",
      "  #pragma message(\"INCLUDING FP16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                            ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_2fp16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_2fp16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_2fp16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                             ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops.cuh(282): remark #20200-D: #pragma message: \"INCLUDING BF16\"\n",
      "  #pragma message(\"INCLUDING BF16\")\n",
      "                                   ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_bf16.cuh:29 CUDART_VERSION with BF16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with BF16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                            ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_2bf16.cuh(29): remark #20200-D: #pragma message: \"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_2bf16.cuh:29 CUDART_VERSION with FP16: 12010, CUDA_ARCH: 800\"\n",
      "  #pragma message(\"/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops_2bf16.cuh\" \":\" \"29\" \" CUDART_VERSION with FP16: \" \"12010\" \", CUDA_ARCH: \" \"800\")\n",
      "                                                                                                                                                             ^\n",
      "\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops.cuh:264:130: note: '#pragma message: /home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops.cuh:264 CUDART_VERSION: 12010, arch: __CUDA_ARCH__'\n",
      "  264 | #pragma message(AT \" CUDART_VERSION: \" TOSTRING(                               \\\n",
      "      |                                                                                                                                  ^\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops.cuh:276:90: note: '#pragma message: SKIPPING FP16, because of CUDART_VERSION: 12010, arch: __CUDA_ARCH__'\n",
      "  276 | #pragma message(\"SKIPPING FP16, because of CUDART_VERSION: \" TOSTRING(         \\\n",
      "      |                                                                                          ^\n",
      "/home/max/myrepos/vlstm_cuda/src/mm_v2/../util/inline_ops.cuh:289:90: note: '#pragma message: SKIPPING BF16, because of CUDART_VERSION: 12010, arch: __CUDA_ARCH__'\n",
      "  289 | #pragma message(\"SKIPPING BF16, because of CUDART_VERSION: \" TOSTRING(         \\\n",
      "      |                                                                                          ^\n",
      "[2/2] /home/max/miniconda3/envs/xlstmpt220cu121/bin/x86_64-conda-linux-gnu-c++ interface.o kernels.cuda.o -shared -L/home/max/miniconda3/envs/xlstmpt220cu121/lib -lcublas -L/home/max/miniconda3/envs/xlstmpt220cu121/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/home/max/miniconda3/envs/xlstmpt220cu121/lib -lcudart -o mm_v2.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module mm_v2...\n"
     ]
    }
   ],
   "source": [
    "from src.mm_v2.interface import mmkernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "div = 1\n",
    "M = 8192 // div\n",
    "N = 8192 // div\n",
    "K = 8192 // div\n",
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float32\n",
    "dtype_AB = torch.bfloat16\n",
    "beta = 1.2\n",
    "alpha = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "mat_A = torch.randn((M, K)).to(device=device, dtype=dtype_AB)\n",
    "mat_B = torch.randn((K, N)).to(device=device, dtype=dtype_AB)\n",
    "mat_C = torch.zeros((M, N)).to(device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_D_pt = (beta / alpha) * mat_A @ mat_B + mat_C * alpha\n",
    "# mat_D2_pt = mat_A @ mat_B.transpose(0,1) + mat_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kernel dispatch - bfloat16!\n",
      "Required shared memory size: 73728 bytes = MAX(73728, 65536)\n",
      "Required shared memory size: 72 Kb\n",
      "#bytes: sizeof(int4)= 16, sizeof(float4)= 16, sizeof(__nv_bfloat16)= 2\n",
      "CHUNK_K=8, CHUNK_LINE_BYTES=256, WARP_COPY_BYTES=512, CHUNK_COPY_LINES_PER_WARP=2, CHUNK_COPY_LINE_LANES=16\n",
      "BLOCK_ROW_TILES=8, BLOCK_COL_TILES=8, BLOCK_ROW_WARPS=2, BLOCK_COL_WARPS=4\n",
      "WARP_ROW_TILES=4, WARP_COL_TILES=2, WARP_SIZE=32, WARPS_PER_BLOCK=8\n",
      "SHMEM_STRIDE=128, SHMEM_OFFSET=64\n",
      "BlockIdx: 0, ThreadIdx: 0, blockDim: 256, gridDim: 48\n"
     ]
    }
   ],
   "source": [
    "mat_D_cu = mmkernel(mat_A, mat_B.transpose(0,1), mat_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0.6027,   99.5545,    6.6524,  ...,   63.4920,   90.0374,  -79.6230],\n",
       "        [ -16.2407,  -70.2106,   74.2199,  ...,  108.1273, -200.2493,   93.9348],\n",
       "        [ -61.3344,   -7.3290, -164.8177,  ...,    3.9053,  -57.0123, -187.6240],\n",
       "        ...,\n",
       "        [  15.8896,   -9.2444,  164.1558,  ...,   21.6965,  -97.8878,  -23.0336],\n",
       "        [ 140.6422,   72.7060,   11.8973,  ..., -195.9341, -194.0892,  143.7378],\n",
       "        [ -21.1582, -170.4006,  -10.7048,  ...,  -35.7926,  104.6309, -104.6804]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_D_cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0.6367,   99.0000,    6.7500,  ...,   63.0000,   89.5000,  -79.0000],\n",
       "        [ -16.0000,  -69.5000,   73.5000,  ...,  107.0000, -199.0000,   93.0000],\n",
       "        [ -60.7500,   -7.0938, -163.0000,  ...,    4.0312,  -56.5000, -187.0000],\n",
       "        ...,\n",
       "        [  15.6875,   -9.4375,  163.0000,  ...,   21.5000,  -97.0000,  -22.8750],\n",
       "        [ 139.0000,   72.0000,   11.6875,  ..., -195.0000, -192.0000,  143.0000],\n",
       "        [ -21.0000, -169.0000,  -10.8750,  ...,  -35.5000,  104.0000, -103.5000]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_D_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstmpt220cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
